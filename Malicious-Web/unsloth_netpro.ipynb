{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eea4c6f",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10039837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fishmon/.conda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from jsonschema import validate, ValidationError\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056f1cb",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a3927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"json\", data_files=\"./dataset/netpro_chatml_thought.jsonl\")\n",
    "\n",
    "dataset = load_dataset(\"jordinia/netpro-finetune\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc5a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset[\"train\"]  # Access the 'train' split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ed36710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output Type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset Info:\n",
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 33262\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOutput Type:\", type(dataset))\n",
    "print(\"Dataset Info:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8651d1",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4834df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1 with CUDA 1201 (you have 2.6.0+cu124)\n",
      "    Python  3.11.10 (you have 3.11.11)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 1. Max memory: 23.551 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.3.19 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from typing import Dict, Any\n",
    "\n",
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "config = load_config('./config/config-0405.yml')\n",
    "\n",
    "# Model and Training Config\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=config.get('model_name', 'unsloth/Llama-3.2-3B-Instruct-bnb-4bit'),\n",
    "    max_seq_length=config.get('max_seq_length', 30000),\n",
    "    dtype=config.get('dtype', None),\n",
    "    load_in_4bit=config.get('load_in_4bit', True)\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=config.get('lora_r', 32),\n",
    "    target_modules=config.get('target_modules', [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]),\n",
    "    lora_alpha=config.get('lora_alpha', 32),\n",
    "    lora_dropout=config.get('lora_dropout', 0),\n",
    "    bias=config.get('bias', 'none'),\n",
    "    use_gradient_checkpointing=config.get('use_gradient_checkpointing', 'unsloth'),\n",
    "    random_state=config.get('random_state', 3407),\n",
    "    use_rslora=config.get('use_rslora', False),\n",
    "    loftq_config=config.get('loftq_config', None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eb134c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d2424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a875a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = dataset.train_test_split(test_size=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c5151d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'text'],\n",
       "        num_rows: 33128\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversations', 'text'],\n",
       "        num_rows: 134\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b1044",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87c48ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjordinia\u001b[0m (\u001b[33mjordinia-netpro\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title wandb init\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "434ca268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH=all\n",
      "env: WANDB_SILENT=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/fishmon/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_WATCH=all\n",
    "%env WANDB_SILENT=true\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3b1d11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fishmon/AJ/LLM-Finetuning/Malicious-Web/wandb/run-20250504_024412-8p2km39j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jordinia-netpro/netpro-finetune/runs/8p2km39j' target=\"_blank\">llama-3.2-3b-instruct-unsloth-sft-2025-05-04</a></strong> to <a href='https://wandb.ai/jordinia-netpro/netpro-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jordinia-netpro/netpro-finetune' target=\"_blank\">https://wandb.ai/jordinia-netpro/netpro-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jordinia-netpro/netpro-finetune/runs/8p2km39j' target=\"_blank\">https://wandb.ai/jordinia-netpro/netpro-finetune/runs/8p2km39j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to WANDB!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers.utils import logging\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Loads from .env file automatically\n",
    "\n",
    "# 2. Verify token loading\n",
    "if not os.getenv(\"WANDB_API_KEY\"):\n",
    "    raise ValueError(\"WANDB_API_KEY not found in .env file\")\n",
    "\n",
    "# 3. Initialize and upload\n",
    "os.environ[\"WANDB_API_KEY\"]=os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "logging.set_verbosity_info()\n",
    "project_name = \"netpro-finetune\" \n",
    "entity_name = \"jordinia-netpro\"\n",
    "run_name = \"llama-3.2-3b-instruct-unsloth-sft-2025-05-04\" # Set your desired run name\n",
    "\n",
    "# Initialize WANDB (FIXED ENTITY/PROJECT)\n",
    "try:\n",
    "    run = wandb.init(\n",
    "        entity=entity_name,\n",
    "        project=project_name,\n",
    "        name=run_name,\n",
    "        # id=\"j4vh49mi\",\n",
    "        # resume=\"allow\" # Uncomment to resume a previous run\n",
    "    )\n",
    "    print(\"Successfully connected to WANDB!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize WANDB: {str(e)}\")\n",
    "    # Consider exiting if WANDB is critical\n",
    "    # sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "052ed88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "PyTorch: setting up devices\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir                  =config.get('output_dir', 'outputs'),\n",
    "    per_device_train_batch_size =config.get('per_device_train_batch_size', 2),\n",
    "    per_device_eval_batch_size  =config.get('per_device_eval_batch_size', 1),\n",
    "    gradient_accumulation_steps =config.get('gradient_accumulation_steps', 4),\n",
    "    warmup_ratio                =config.get('warmup_ratio', 0.05),\n",
    "    max_steps                   =config.get('max_steps', 4125),\n",
    "    learning_rate               =2e-5,\n",
    "    fp16                        =not is_bfloat16_supported(),\n",
    "    bf16                        =is_bfloat16_supported(),\n",
    "    optim                       =config.get('optim', 'adamw_8bit'),\n",
    "    weight_decay                =config.get('weight_decay', 0.1),\n",
    "    lr_scheduler_type           =config.get('lr_scheduler_type', 'cosine'),\n",
    "    eval_strategy               =config.get('eval_strategy', 'steps'),\n",
    "    eval_steps                  =config.get('eval_steps', 50),\n",
    "    save_strategy               =config.get('save_strategy', 'steps'),\n",
    "    save_steps                  =config.get('save_steps', 100),\n",
    "    save_total_limit            =config.get('save_total_limit', 3),\n",
    "    logging_steps               =config.get('logging_steps', 10),\n",
    "    seed                        =config.get('seed', 3407),\n",
    "    report_to                   =config.get('report_to', 'wandb'),\n",
    "    load_best_model_at_end      =config.get('load_best_model_at_end', True),\n",
    "    metric_for_best_model       =config.get('metric_for_best_model', 'eval_loss'),\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model               =model,\n",
    "    tokenizer           =tokenizer,\n",
    "    train_dataset       =dataset_dict[\"train\"],\n",
    "    eval_dataset        =dataset_dict[\"test\"],\n",
    "    dataset_text_field  =config.get('dataset_text_field', 'text'),\n",
    "    max_seq_length      =config.get('max_seq_length', 30000),\n",
    "    data_collator       =DataCollatorForSeq2Seq(\n",
    "        tokenizer               =tokenizer,\n",
    "        padding                 =config.get('data_collator', {}).get('padding', True),\n",
    "        pad_to_multiple_of      =config.get('data_collator', {}).get('pad_to_multiple_of', 8),\n",
    "        max_length              =config.get('max_seq_length', 30000),\n",
    "    ),\n",
    "    dataset_num_proc    =config.get('dataset_num_proc', 2),\n",
    "    packing             =config.get('packing', False),\n",
    "    args                =args,\n",
    ")\n",
    "\n",
    "# # Calculate training length (33k samples)\n",
    "# total_samples = 33000  # Your balanced dataset size\n",
    "# batch_size = 2 * 4  # batch_size * gradient_accum\n",
    "# steps_per_epoch = total_samples // batch_size  # ~4125 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cc344",
   "metadata": {},
   "source": [
    "### Train on Completions Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07252b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_on_responses_only_custom(trainer):\n",
    "#     \"\"\"\n",
    "#     Custom version for website classification that:\n",
    "#     1. Preserves system prompt context\n",
    "#     2. Only trains on assistant responses (JSON + reasoning)\n",
    "#     3. Handles length consistency for batching\n",
    "#     \"\"\"\n",
    "#     tokenizer = trainer.tokenizer\n",
    "    \n",
    "#     # Manually define token sequences for your template\n",
    "#     SYSTEM_TOKENS = tokenizer.encode(\n",
    "#         \"<|start_header_id|>system<|end_header_id|>\\n\\n\", \n",
    "#         add_special_tokens=False\n",
    "#     )\n",
    "#     USER_TOKENS = tokenizer.encode(\n",
    "#         \"<|start_header_id|>user<|end_header_id|>\\n\\n\", \n",
    "#         add_special_tokens=False\n",
    "#     )\n",
    "#     ASSISTANT_TOKENS = tokenizer.encode(\n",
    "#         \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \n",
    "#         add_special_tokens=False\n",
    "#     )\n",
    "\n",
    "#     def custom_masking(examples):\n",
    "#         input_ids = examples[\"input_ids\"]\n",
    "        \n",
    "#         # Create labels if they don't exist\n",
    "#         labels = examples.get(\"labels\", [ids.copy() for ids in input_ids])\n",
    "#         new_labels = []\n",
    "        \n",
    "#         for seq_id, seq_labels in zip(input_ids, labels):\n",
    "#             n = len(seq_id)\n",
    "#             mask = [-100] * n\n",
    "#             i = 0\n",
    "            \n",
    "#             while i < n:\n",
    "#                 # Check for system prompt\n",
    "#                 if seq_id[i:i+len(SYSTEM_TOKENS)] == SYSTEM_TOKENS:\n",
    "#                     i += len(SYSTEM_TOKENS)\n",
    "#                     continue\n",
    "                    \n",
    "#                 # Check for user prompt\n",
    "#                 if seq_id[i:i+len(USER_TOKENS)] == USER_TOKENS:\n",
    "#                     i += len(USER_TOKENS)\n",
    "#                     continue\n",
    "                    \n",
    "#                 # Check for assistant prompt\n",
    "#                 if seq_id[i:i+len(ASSISTANT_TOKENS)] == ASSISTANT_TOKENS:\n",
    "#                     start = i\n",
    "#                     i += len(ASSISTANT_TOKENS)\n",
    "                    \n",
    "#                     # Find end of assistant response\n",
    "#                     while i < n:\n",
    "#                         if seq_id[i] == tokenizer.eos_token_id:\n",
    "#                             end = i\n",
    "#                             break\n",
    "#                         i += 1\n",
    "#                     else:\n",
    "#                         end = n\n",
    "                    \n",
    "#                     # Unmask assistant response\n",
    "#                     mask[start:end] = seq_labels[start:end]\n",
    "#                     break\n",
    "                    \n",
    "#                 i += 1\n",
    "            \n",
    "#             # Enforce length matching (critical fix)\n",
    "#             if len(mask) != len(seq_id):\n",
    "#                 mask = mask[:len(seq_id)] + [-100] * (len(seq_id) - len(mask))\n",
    "            \n",
    "#             new_labels.append(mask)\n",
    "        \n",
    "#         return {\"labels\": new_labels}\n",
    "\n",
    "#     # Apply to datasets with length verification\n",
    "#     def apply_masking(dataset):\n",
    "#         dataset = dataset.map(\n",
    "#             custom_masking,\n",
    "#             batched=True,\n",
    "#             batch_size=1000,\n",
    "#             num_proc=4,\n",
    "#         )\n",
    "#         # Verify lengths\n",
    "#         for i in range(min(3, len(dataset))):\n",
    "#             assert len(dataset[i][\"input_ids\"]) == len(dataset[i][\"labels\"]), \\\n",
    "#                 f\"Length mismatch in sample {i}\"\n",
    "#         return dataset\n",
    "    \n",
    "#     trainer.train_dataset = apply_masking(trainer.train_dataset)\n",
    "    \n",
    "#     if trainer.eval_dataset is not None:\n",
    "#         trainer.eval_dataset = apply_masking(trainer.eval_dataset)\n",
    "        \n",
    "#     return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9499b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = train_on_responses_only_custom(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96e2c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final verification (optional)\n",
    "# print(\"\\n=== Final Pre-Train Check ===\")\n",
    "# sample = trainer.train_dataset[0]\n",
    "# print(\"First sample labels preview:\")\n",
    "# print(\"Input IDs length:\", len(sample[\"input_ids\"])) \n",
    "# print(\"Labels length:\", len(sample[\"labels\"]))\n",
    "# print(\"Last 10 labels:\", sample[\"labels\"][-10:])  # Should show unmasked assistant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6841e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a sample from the training set\n",
    "# sample_index = 5  # Try different indices\n",
    "# sample = trainer.train_dataset[sample_index]\n",
    "\n",
    "# # Decode the full input context\n",
    "# print(\"==== Full Input Context ====\")\n",
    "# print(tokenizer.decode(sample[\"input_ids\"]))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Decode the labels with masking visualization\n",
    "# print(\"==== Training Targets (Masked) ====\")\n",
    "# masked_labels = []\n",
    "# for token_id, label_id in zip(sample[\"input_ids\"], sample[\"labels\"]):\n",
    "#     if label_id == -100:\n",
    "#         # Show masked tokens as blank spaces\n",
    "#         masked_labels.append(\" \")\n",
    "#     else:\n",
    "#         # Show actual token\n",
    "#         masked_labels.append(tokenizer.decode([token_id]))\n",
    "\n",
    "# print(\"\".join(masked_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "136a307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = trainer.train_dataset.select(range(2)).with_format(\"torch\")[0]\n",
    "# print(batch[\"input_ids\"])\n",
    "# print(batch[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221f500",
   "metadata": {},
   "source": [
    "### Memory Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7c95376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 33128\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b14b138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'You are an Expert Website Classifier tasked with categorizing websites (using provided Domain & Content) into distinct categories: **0 - Benign**, **1 - Gambling**, **2 - Pornography**, or **3 - Harmful**. Output strict JSON including classification, reason, and a point-based confidence score (0-100). This aids a sophisticated website prediction system for digital safety.\\n\\n### **Categories & Definitions:**\\n\\n-   **0 - Benign:** General info, news, entertainment, services, e-commerce, educational sites, blogs, informational pages, and general entertainment sites. No gambling/porn themes.\\n    -   **Includes:** Educational discussion of sensitive topics (drugs, gambling, adult, illegal), suggestive content (dating, lingerie) *without* explicit material, drugs for educational or news purposes (e.g., addiction recovery, medical cannabis research).\\n\\n-   **1 - Gambling:** Promotes/facilitates betting, casino, poker, lottery, wagering. Includes online betting platforms, casinos, poker sites, lottery sites, and any website that encourages or provides means for users to wager money on games of chance or skill.\\n    -   **Keywords:** betting, casino, poker, odds, jackpot, slots, roulette, judi, slot gacor, togel, similar gambling-related terminology within their domain name or content.\\n    \\n-   **2 - Pornography:** These are websites that contain explicit sexual content intended to cause arousal. Explicit sexual content (images, videos, text) for arousal. Immoral content (bestiality, child exploitation). or links to such materials. \\n    -   **Keywords:** porn, bokep, sex, xxx, adult, nude, erotic, explicit video/photo.\\n\\n- **3 - Harmful**: Websites that does not fall to Benign, Gambling, and Pornography category. These websites engage in or promote activities harmful to users or violating laws/regulations, including:\\n    - Malware Distribution: Hosting/downloading computer viruses, worms, ransomware, spyware, etc.\\n    - Cybercrime: Phishing kits, hacking tools, stolen data markets, carding forums.\\n    - Extremism & Terrorism: Content inciting violence, extremist ideologies, or terrorist recruitment.\\n    - Violations of Indonesian Law:\\n        - Insults, defamation, blackmail, or threats.\\n        - Hoaxes/misleading news, hate speech, or incitement of violence.\\n    - Copyright Infringement/Piracy: Illegal software/cracks, torrents, pirated media.\\n    - Drugs/Narcotics: Sale/promotion of illegal drugs (e.g., cocaine, heroin) or unregulated pharmaceuticals.\\n    - Weapons: Sale of illegal firearms, explosives, or weapons.\\n    - Other Illegal Activities: Counterfeit goods, money laundering, human trafficking.\\n    - **Examples:**  \\n        - Illegal: `darknet-drugs.com` (drug sales), `pirated-movies.id` (piracy).  \\n        - Harmful: `extremist-forum.net` (terrorism recruitment), `hackers-tools.org` (phishing kits).  \\n        - Scam/phishng: `hadiah-telkomsel7.blogspot.com` (non-genuine website).\\n    - **Keywords**:\\nmalware, ransomware, phishing, hack, terrorism, jual narkoba, senjata ilegal, konten SARA, berita bohong, ancaman, pembajakan, cracked software, carding, darknet.\\n    - **Exceptions:** \\n        - Licensed/unlicensed gambling â†’ **1 - Gambling**; scams â†’ **3 - Harmful**.\\n\\n\\n### **Input Data Context:**\\n\\nYou will be provided with data entries, each consisting of two primary fields:\\n\\n*   **Domain:** The domain name or URL of the website (e.g., `example.com`, `gamble-site.net`). This can provide hints about the website\\'s purpose.\\n*   **Content:**  The textual content scraped from the website. This content offers detailed information about the website\\'s topics, services, and themes.\\n\\n### **Labeling Instructions:**\\n\\nAnalyze both the **Domain** and the **Content** provided. Use keywords and contextual clues from both to determine the most appropriate category for the website.  Consider the primary purpose and content focus of the website when classifying.\\n\\n### **Confidence Assessment Guidelines: Point-Based System (Total Possible Points: 100)**\\n\\nTo determine the **confidence** level (0-100) for your classification, evaluate the following factors and sum up the points.  The total points will directly correspond to the confidence percentage (e.g., 95 points = 95% confidence).\\n\\n#### **I. Keyword Strength and Relevance (Maximum 40 Points)**\\n*   **(40 Points):  Exceptional Keyword Strength: Explicit and Overwhelming Keywords in BOTH Domain and Content:** Presence of extremely explicit and overwhelmingly strong keywords that are *unquestionably* indicative of a specific category in *both* the domain name AND the website content. These keywords leave absolutely no doubt about the website\\'s nature. (e.g., Domain: `casino-royal-betting.com`, Content:  \"Gamble now and win HUGE jackpots on slots, poker, roulette! Real money betting!\").  This represents the absolute strongest keyword signal possible.\\n*   **(35 Points): Clear and Strong Keywords in BOTH Domain and Content:**  Presence of highly explicit keywords strongly indicative of a specific category in both the domain name AND the website content. (e.g., Domain: `bet.com`, Content:  \"Bet on sports and casino games!\").\\n*   **(25 Points): Strong Keywords in EITHER Domain OR Content:** Presence of highly explicit keywords strongly indicative of a specific category in EITHER the domain name OR the website content, but not both.\\n*   **(15 Points): Some Relevant Keywords:** Presence of keywords related to a category, but they are less explicit, less frequent, or require more contextual interpretation in either domain or content.\\n*   **(0 Points): Weak or Generic Keywords:** Lack of clear category-specific keywords in both domain and content. Keywords are generic and do not strongly suggest any specific category.\\n\\n#### **II. Domain and Content Alignment (Maximum 30 Points)**\\n\\n*   **(30 Points): Strong Domain and Content Alignment:** Domain name strongly and unambiguously suggests a category, and the website content consistently and explicitly reinforces that category.  They tell the same clear story.\\n*   **(15 Points): Partial Domain and Content Alignment:** Domain name and content generally point towards the same category, but the alignment might be less direct, slightly ambiguous, or require some interpretation to connect them.\\n*   **(0 Points): Domain-Content Mismatch or No Alignment:** Domain name suggests one thing, but the content is unclear, suggests something different, or there\\'s no clear connection between the domain and the content\\'s apparent purpose.\\n\\n#### **III. Content Clarity and Unambiguity (Maximum 20 Points)**\\n\\n*   **(20 Points): Unambiguous and Explicit Content:** The website content is very clear, direct, and leaves virtually no room for interpretation. It unambiguously falls into one of the defined categories.\\n*   **(10 Points): Content Requires Some Interpretation:** The content generally points to a category, but requires some interpretation to confidently assign it.  There might be subtle hints, implied meanings, or a need to infer the primary purpose.\\n*   **(0 Points): Ambiguous or Conflicting Content:** The website content is vague, contradictory, or could be reasonably interpreted in multiple ways, making it difficult to confidently assign a category.\\n\\n#### **IV. Category Indicator Strength (Maximum 10 Points)**\\n\\n*   **(10 Points): Multiple Strong Category Indicators:** Presence of numerous strong and clear indicators for a specific category throughout the domain and content (e.g., for Pornography: explicit keywords, descriptions of sexual acts, calls to action to view adult content, age verification prompts).\\n*   **(5 Points): Some Category Indicators Present:** Presence of a few indicators for a category, but they are not overwhelmingly strong or numerous.\\n*   **(0 Points): Lack of Category Indicators:** Few or no clear indicators for any of the defined categories are present in the domain and content.\\n\\n#### **Calculation:**\\n\\n1.  For each of the four sections (I-IV), assess the website and select the point value that best describes the presence of the described factors.\\n2.  Sum up the points from all four sections.\\n3.  The total sum represents the confidence level in percentage (%).\\n\\n#### **Confidence Level Ranges (for reference - already implicitly defined by points):**\\n\\n*   **High Confidence (80-100 Points):**  Strong evidence across multiple factors pointing clearly to a category.\\n*   **Medium Confidence (50-79 Points):** Moderate evidence, some ambiguity or less directness in indicators.\\n*   **Low Confidence (0-49 Points):** Weak or conflicting evidence, high uncertainty about the correct category.\\n\\n#### **Example:**\\n\\nLet\\'s say you are classifying `lucky-slots-online.com` with content about slot games and bonuses.\\n\\n*   **I. Keyword Strength:** Strong keywords in both Domain and Content (e.g., \"slots,\" \"online,\" \"win,\" \"bonuses\") - **25 Points**\\n*   **II. Domain-Content Alignment:** Domain and content strongly align with Gambling - **30 Points**\\n*   **III. Content Clarity:** Content is very clear about gambling activities - **20 Points**\\n*   **IV. Category Indicators:** Multiple indicators of gambling (games, bonuses, calls to action) - **10 Points**\\n\\n**Total Points: 25 + 30 + 20 + 10 = 85 Points.  Confidence: 85%**\\n\\n#### **Using this Point System:**\\n\\nWhen generating the \"reason\" for your classification, you can now also briefly mention the points you assigned for each section to justify the final confidence score. For example:\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Strong keywords in domain and content (25 pts), strong domain-content alignment (30 pts), clear gambling content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n\\n### **Output Requirements: STRICTLY ADHERE TO JSON FORMAT**\\n\\nFor each website, you **MUST** provide your classification in the following **JSON format ONLY**.  **No other format is acceptable.**\\n\\n```json\\n{\\n  \"answer\": (integer: 0, 1, 2, or 3),\\n  \"classification\": (string: \"Benign\", \"Gambling\", \"Pornography\", or \"Harmful\"),\\n  \"reason\": (string: \"Brief explanation of the classification using keywords and hints from the domain and content.\"),\\n  \"confidence\": (integer: 0 to 100, \"Level of confidence in the classification\")\\n}\\n```\\n\\n**Example Input and Expected Output:**\\n\\n**1st Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: news-today.info\\nContent: Welcome to News Today! Get the latest breaking news, top stories, and in-depth analysis from around the world. Covering politics, business, technology, sports, and culture. Stay informed with News Today - your source for reliable journalism.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 0,\\n  \"classification\": \"Benign\",\\n  \"reason\": \"Domain \\'news-today.info\\' and content mention \\'breaking news,\\' \\'top stories,\\' \\'analysis,\\' \\'reliable journalism,\\' indicating a general information/news website. Strong keywords in domain and content (35 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), some category indicators (5 pts). Total 90 points.\",\\n  \"confidence\": 90\\n}\\n```\\n\\n**2nd Sample:**\\n\\n**Input Data:**\\n\\n```\\nDomain: lucky-slots-online.com\\nContent:  Spin to win big at Lucky Slots Online! Play hundreds of exciting slot games, claim your bonuses, and join the fun.  Licensed and regulated for your safety. 24/7 customer support available. Join now and get 100 free spins!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Domain \\'lucky-slots-online.com\\' and content include keywords like \\'slots,\\' \\'win,\\' \\'casino,\\' \\'bonuses,\\' \\'free spins,\\' indicating a gambling website.\",\\n  \"confidence\": 100\\n}\\n```\\n\\n**3rd Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: adult-pleasures.net\\nContent:  Experience the hottest adult entertainment online. Unlimited access to exclusive videos and photos. 18+ only. Join our community of pleasure seekers today!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 2,\\n  \"classification\": \"Pornography\",\\n  \"reason\": \"Domain \\'adult-pleasures.net\\' and content include explicit terms like \\'adult entertainment,\\' \\'exclusive videos,\\' \\'photos,\\' \\'18+ only,\\' and \\'pleasure seekers,\\' indicating a pornography website. Strong keywords in both domain and content (25 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n**4th Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: project420.com\\nContent:  Welcome to Project420.com an ongoing community marijuana, hemp search engine and your gateway to 420 products, services and information from all over the world. Marijuana seeds, super bongs, high tech growing marijuana books, contests, communities, cool people and tons of 420 fun.  Potent marijuana websites are added frequently.  Let\\'s help each other grow.  Use the table of contents located to the left and begin your search of 420 world wide websites.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 3,\\n  \"classification\": \"Harmful\",\\n  \"reason\": \"Domain \\'project420.com\\' and content include keywords like \\'marijuana seeds,\\' \\'growing marijuana,\\' and \\'420 products,\\' indicating promotion of illegal drugs under Indonesian law (UU ITE). Strong keywords in content (25 pts), Partial Domain and Content Alignment (15 pts), clear harmful intent (20 pts), multiple indicators (10 pts). Total 70 points.\",\\n  \"confidence\": 70\\n}\\n```\\n**Execution:**\\nProcess the provided website data (input fields: `Domain`, `Content`). Apply the classification categories, definitions, and confidence point system accurately. Generate the mandatory JSON output for each entry. Begin classification now.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n\\nDomain: adevotedslave.blogspot.com, Content: \"Delicious Debauchery If You arenâ€™t over the age 18 (21 in some areas) then please leave. If You are offended by sex, BDSM, phone sex or brutal honesty then this isn\\'t the place for You. FRIDAY, JUNE 08, 2007 Why Iâ€™ll never be a full lesbianâ€¦ You know, it doesnâ€™t seem to matter what I am using or what position I am in. Iâ€™ve tried it with dildos, vibes, my gear shift knob, my fingers, and miscellaneous household objects, with just about anything and every thing I can think of. Iâ€™ve done it on my hands and knees on my back, with one leg up. Iâ€™ve tried putting the base on the floor and bouncing up and down. Iâ€™ve put it up against a wall and pushed against it. It doesnâ€™t matter. Nothing that I do makes it so that I can fuck myself hard enough to make it feel just right. Nothing can take me to that place where I am balancing between the pleasure of being fucked and the pain of having a cock battering my insides. Of course, when Iâ€™m trying to simulate hard fucking it still feels good, but itâ€™s that kind of feeling you get when you are almost there, but not quite. You know where you want to go but you just canâ€™t get there. Itâ€™s frustrating as hell. Besides, I have never actually had a toy, or object Iâ€™ve converted into a toy, thatâ€™s felt like a real cock anyway. They all just feel like random objects that fall short of the glory that is a real, hard, throbbing, piece of flesh designed for fucking. Of course, if thereâ€™s some woman out there whoâ€™s willing to try to convert me into a full fledged lesbian anyway, Iâ€™m more than willing to submit to that kind of training. Who knows? A woman with a strap on may be able to fuck me just like I like. Of course, Iâ€™d not refuse returning the favor. It could be a mutually beneficial experiment. POSTED BY DEIDRE AT 11:13 AM 2 COMMENTS SATURDAY, APRIL 28, 2007 Bruised and Battered For some reason I think that the depth of my pussy fluctuates. Sometimes when Masterâ€™s cock slides inside of me it is the perfect fit. The head hits in just the... en melted apart. A puff of smoke flew up, and there was another familiar sensation... the smell of burned meat. Again the heat, with a dirty orange light where the flame burned away the bits of tissue stuck to the wire. Again the application, the shudder, the pop, the hiss, the smoke. Around the stencil he went, carefully watching the brand, lining up the strikes. As the skin opens along the brand\\'s outline, it shifts. About halfway around, he started almost freehanding, using the stencil as a reminder of what he wanted, not as a guide. Then with the outline complete, and even, he used its angles as a reference for the final strikes down the middle. There were ten strikes in all. Ten times the iron glowed, ten times it lowered to her helpless flesh. Ten times she moaned as her love for me was burned into her very body. As I continued my research I rediscovered Fakirâ€™s site and the process of using an electro-cautery pencil instead of using the striking method with heated ribbons of steel. I showed a few examples to Master, including a photo of hisdevotionâ€™s back. He asked how one goes about getting an electro-cautery pencil. Until that moment it never even occurred to me that a person outside of the medical profession could just go and buy one. Well, for less than a hundred bucks one can buy a battery operated one with various different tips. Something wonderful happened as I scrolled through all of the options for this tool. I began reading in the descriptions that they get up to 2200 degrees Fahrenheit. Instead of the fear I expected to feel at such a number I felt the nervous flutters of excitement. If Master decides to go this direction, and have His mark of ownership burned into my skin in this fashion, I will have 2200 degrees focused on my skin, burning me, marking me bit by bit. This makes me incredibly aroused. Perhaps I have the makings of a pain slut afterall. POSTED BY DEIDRE AT 1:43 PM 0 COMMENTS TUESDAY, MARCH 13, 2007 masks Iâ€™ve said before that my relationship with Master and the trust and honesty that is the foundation of that relationship has made me more fully myself than I ever have been before. I am more aware of the different aspects of myself and I am able to lie them all out on the table to be poked at and prodded by any stranger who happens by. On the other hand there are those for whom I must wear certain masks. My relationship with Master might have inspired an intense honesty with myself and with Him, as well as with those who can accept us this way and those whose judgement doesnâ€™t matter, but itâ€™s also forced me to be more dishonest than ever to those around me, my neighbors, friends and family. I have been asked by potential sisters, by friends and by callers about how the world sees me. What does Master let the neighbors see? His friends? My friends? My family? His family? Itâ€™s all kind of complicated. The neighbors/His friends: * My neighbors see a 28 year old girl who is living with a 53 year old man. They see the 2 of us happily interacting with each other. He is very active in our neighborhood community. I am not. I occasionally wave to the neighbors or have a conversation with them and most of the time they say something to the effect of, \"Itâ€™s been a while. How have you been?\" Most of them are nice people, but I have little desire to interact with them. They know that I am a phone sex operator and most of them are intrigued. They do not know that I am a slave but there are probably quite a few who have observed that I donâ€™t make any decisions of importance. My friends: * My friends all know just about everything there is to know. Friends that I consider to be mine (as opposed to those that I know because of Master) are basically those from the internet. This includes a few people from collarme and other personals sites but the vast majority is made up of people from LiveJournal. LiveJournal is where all of my meaningful social interaction takes place, as sad as that may seem. There just arenâ€™t a whole lot of people like me nearby and being friends with a girl whoâ€™s first priority is never friendships can be trying. His family: * Masterâ€™s brother, K. He is aware of the basics of the dynamics between Master and myself however itâ€™s never discussed with him and itâ€™s never thrown in his face. We donâ€™t play or fuck in front of K. Iâ€™ve never been loaned to K. Master and I have discussed it and if K is anything heâ€™s probably more submissive than dominant. K knows that I am a phone sex operator. I think he may have heard me on occasion when I am taking calls. * The rest of Masterâ€™s family. He has one of those families that has reunions every year. The patriarch of the family is in his 90s and still kicking. I havenâ€™t met everyone yet, but Iâ€™ve met the ones that are either closest or that Master likes the most. They have opened their arms and accepted me, making me an honorary â€“insert Masterâ€™s family name here- One of them told me over thanksgiving that she has never seen Master as happy as he is now and she thinks that I am to thank for that. What higher compliment can someone pay me than that? My Family: * My sister, E, and I used to be very close. Weâ€™re not anymore though. When I went to her house last summer I had just come from my fathers and wasnâ€™t wearing my collar. The first night I was getting ready to go out with her and my mother and put it back on. Her reaction was that my necklace didnâ€™t match my outfit and I should take the jewelry off. I refused explaining that Master (I used his first name) gave it to me and I hadnâ€™t really had it off since except for when I visited dad. I think she started to get the idea at that point. She knew a little before then, but I think the image of me collared finished off any of the blank parts in her imagination of how I was living my life. I donâ€™t think she approves. We havenâ€™t spoken since then. Iâ€™m not sure how much my lifestyle has to do with that though. * My mother knows the pieces and details that I think sh...  get enough. Sheâ€™s become my newest addiction. I lick the juice of her squirt from my arm and slide my fingers from her into my mouth. I love that taste. Outside the sun is beginning to rise. Weâ€™ve been lost in each other for the entire night. The magic begins to dissipate. We get dressed once again shy and nervous. The blush on my face doesnâ€™t stop me from telling her how much I enjoyed the night. It turns out that she did too and that we both want to do it again. Soon. POSTED BY DEIDRE AT 1:59 PM 0 COMMENTS Older Posts Home Subscribe to: Posts (Atom) ABOUT DEIDRE DEIDRE STRADDLING THE LINE BETWEEN FANTASY AND REALITY deidre is a 28 yo lifestyle slave, phone sex operator and fetish model. she is happily owned. VIEW MY COMPLETE PROFILE LINKS 101 Things about deidre NITEFLIRT delicious debauchery Join for Free What is NiteFlirt? MP3 RECORDINGS The Holy Fuck **fetish sample Gender Blender **hypnosis sample How to Get Out of a Ticket **fetish sample Gym Discipline **fetish sample French Maid **fetish sample Lesbian Strap-on BDSM **fetish sample Lesbian Tickle Bondage **fetish sample Isabella\\'s Sex Slave **hypnosis sample Gagged By Goddess **fetish sample Boarding School Humiliation **fetish sample RECORDINGS (BY THE MINUTE) rates and descriptions deidre\\'s daily prayer deidre gets face fucked equipment sampler deidre\\'s discipline barking cough tickle torture psalm 23 (deidre style) a steamy shower & my fav toy PHOTO SETS rates and descriptions teddy bears & coloring books deidre\\'s puppy training photo sampler bondage the golden set deidre\\'s naughty picnic deidre in distress foot bondage PODCASTS Session 2 Interview with Miss Isabella Session 1 WISHFUL THINKING amazon wishlist deidre\\'s corset fund LABELS questions (13) NiteFlirt (12) mp3 (11) calls (6) gifts (5) music (5) pics (2) podcast (2) meme (1) quotes (1) BLOG ARCHIVE â–¼ 2007 (41) â–¼ JUNE (1) â–¼ JUN 08 (1) WHY Iâ€™LL NEVER BE A FULL LESBIANâ€¦ â–º APRIL (2) â–º MARCH (6) â–º FEBRUARY (11) â–º JANUARY (21) â–º 2006 (117) â–º 1978 (2)\"',\n",
       "   'role': 'user'},\n",
       "  {'content': '<think>\\nOkay, I need to classify this website into one of the four categories: Benign, Gambling, Pornography, or Harmful. Let\\'s start by looking at the domain and content provided.  The domain is adevotedslave.blogspot. com. The word \"slavery\" in the domain could be a red flag because it might relate to harmful activities like human trafficking. But I should also consider if it\\'s used in a different context, such as BDSM or kink communities which are more about consensual adult content.  Looking at the content, there are several posts discussing BDSM practices, phone sex, and explicit sexual scenarios. For example, one post talks about trying various objects for self-pleasure and another describes a session involving masks and domination. There\\'s also mention of being a \"lifestyle slave\" and a \"phone sex operator,\" which suggests the site is focused on adult content.  The content includes terms like \"BDSM,\" \"fetish,\" \"bondage,\" and explicit descriptions of sexual acts. These keywords are strong indicators of Pornography because they relate to adult themes and explicit material intended for arousal.  However, I should check if it falls under Harmful instead. The website doesn\\'t seem to promote illegal activities like drug sales or cybercrime. It\\'s more about consensual adult content, which would typically fall under Pornography rather than Harmful unless there are elements of non-consent or exploitation, which aren\\'t clearly present here.  The domain and content align well with Pornography because the focus is on explicit sexual content and fetishes. There are no strong indicators pointing towards Gambling or Benign categories. The content is unambiguous in its adult themes, so it doesn\\'t seem to be a case of harmful activities outside the scope of Pornography.  In terms of confidence assessment: - Keyword Strength: Strong keywords like \"fetish,\" \"bondage,\" and explicit sexual descriptions are present both in the domain and content. - Domain-Content Alignment: The domain suggests a BDSM or slavery theme, which is reinforced by the content discussing explicit adult scenarios. - Content Clarity: The content is very clear about its focus on adult entertainment and fetishes without ambiguity. - Category Indicators: Multiple indicators such as explicit sexual descriptions, use of terms like \"phone sex operator,\" and detailed accounts of BDSM practices.  Based on these factors, I\\'m confident that this website falls under the Pornography category with a high confidence score.\\n</think>\\n```json\\n{\\n  \"answer\": 2,\\n  \"classification\": \"Pornography\",\\n  \"reason\": \"The domain \\'adevotedslave. blogspot.com\\' and content include explicit terms like \\'BDSM,\\' \\'fetish,\\' \\'bondage,\\' and detailed descriptions of sexual acts, indicating a focus on adult entertainment. Strong keywords in both domain and content (25 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```',\n",
       "   'role': 'assistant'}],\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are an Expert Website Classifier tasked with categorizing websites (using provided Domain & Content) into distinct categories: **0 - Benign**, **1 - Gambling**, **2 - Pornography**, or **3 - Harmful**. Output strict JSON including classification, reason, and a point-based confidence score (0-100). This aids a sophisticated website prediction system for digital safety.\\n\\n### **Categories & Definitions:**\\n\\n-   **0 - Benign:** General info, news, entertainment, services, e-commerce, educational sites, blogs, informational pages, and general entertainment sites. No gambling/porn themes.\\n    -   **Includes:** Educational discussion of sensitive topics (drugs, gambling, adult, illegal), suggestive content (dating, lingerie) *without* explicit material, drugs for educational or news purposes (e.g., addiction recovery, medical cannabis research).\\n\\n-   **1 - Gambling:** Promotes/facilitates betting, casino, poker, lottery, wagering. Includes online betting platforms, casinos, poker sites, lottery sites, and any website that encourages or provides means for users to wager money on games of chance or skill.\\n    -   **Keywords:** betting, casino, poker, odds, jackpot, slots, roulette, judi, slot gacor, togel, similar gambling-related terminology within their domain name or content.\\n    \\n-   **2 - Pornography:** These are websites that contain explicit sexual content intended to cause arousal. Explicit sexual content (images, videos, text) for arousal. Immoral content (bestiality, child exploitation). or links to such materials. \\n    -   **Keywords:** porn, bokep, sex, xxx, adult, nude, erotic, explicit video/photo.\\n\\n- **3 - Harmful**: Websites that does not fall to Benign, Gambling, and Pornography category. These websites engage in or promote activities harmful to users or violating laws/regulations, including:\\n    - Malware Distribution: Hosting/downloading computer viruses, worms, ransomware, spyware, etc.\\n    - Cybercrime: Phishing kits, hacking tools, stolen data markets, carding forums.\\n    - Extremism & Terrorism: Content inciting violence, extremist ideologies, or terrorist recruitment.\\n    - Violations of Indonesian Law:\\n        - Insults, defamation, blackmail, or threats.\\n        - Hoaxes/misleading news, hate speech, or incitement of violence.\\n    - Copyright Infringement/Piracy: Illegal software/cracks, torrents, pirated media.\\n    - Drugs/Narcotics: Sale/promotion of illegal drugs (e.g., cocaine, heroin) or unregulated pharmaceuticals.\\n    - Weapons: Sale of illegal firearms, explosives, or weapons.\\n    - Other Illegal Activities: Counterfeit goods, money laundering, human trafficking.\\n    - **Examples:**  \\n        - Illegal: `darknet-drugs.com` (drug sales), `pirated-movies.id` (piracy).  \\n        - Harmful: `extremist-forum.net` (terrorism recruitment), `hackers-tools.org` (phishing kits).  \\n        - Scam/phishng: `hadiah-telkomsel7.blogspot.com` (non-genuine website).\\n    - **Keywords**:\\nmalware, ransomware, phishing, hack, terrorism, jual narkoba, senjata ilegal, konten SARA, berita bohong, ancaman, pembajakan, cracked software, carding, darknet.\\n    - **Exceptions:** \\n        - Licensed/unlicensed gambling â†’ **1 - Gambling**; scams â†’ **3 - Harmful**.\\n\\n\\n### **Input Data Context:**\\n\\nYou will be provided with data entries, each consisting of two primary fields:\\n\\n*   **Domain:** The domain name or URL of the website (e.g., `example.com`, `gamble-site.net`). This can provide hints about the website\\'s purpose.\\n*   **Content:**  The textual content scraped from the website. This content offers detailed information about the website\\'s topics, services, and themes.\\n\\n### **Labeling Instructions:**\\n\\nAnalyze both the **Domain** and the **Content** provided. Use keywords and contextual clues from both to determine the most appropriate category for the website.  Consider the primary purpose and content focus of the website when classifying.\\n\\n### **Confidence Assessment Guidelines: Point-Based System (Total Possible Points: 100)**\\n\\nTo determine the **confidence** level (0-100) for your classification, evaluate the following factors and sum up the points.  The total points will directly correspond to the confidence percentage (e.g., 95 points = 95% confidence).\\n\\n#### **I. Keyword Strength and Relevance (Maximum 40 Points)**\\n*   **(40 Points):  Exceptional Keyword Strength: Explicit and Overwhelming Keywords in BOTH Domain and Content:** Presence of extremely explicit and overwhelmingly strong keywords that are *unquestionably* indicative of a specific category in *both* the domain name AND the website content. These keywords leave absolutely no doubt about the website\\'s nature. (e.g., Domain: `casino-royal-betting.com`, Content:  \"Gamble now and win HUGE jackpots on slots, poker, roulette! Real money betting!\").  This represents the absolute strongest keyword signal possible.\\n*   **(35 Points): Clear and Strong Keywords in BOTH Domain and Content:**  Presence of highly explicit keywords strongly indicative of a specific category in both the domain name AND the website content. (e.g., Domain: `bet.com`, Content:  \"Bet on sports and casino games!\").\\n*   **(25 Points): Strong Keywords in EITHER Domain OR Content:** Presence of highly explicit keywords strongly indicative of a specific category in EITHER the domain name OR the website content, but not both.\\n*   **(15 Points): Some Relevant Keywords:** Presence of keywords related to a category, but they are less explicit, less frequent, or require more contextual interpretation in either domain or content.\\n*   **(0 Points): Weak or Generic Keywords:** Lack of clear category-specific keywords in both domain and content. Keywords are generic and do not strongly suggest any specific category.\\n\\n#### **II. Domain and Content Alignment (Maximum 30 Points)**\\n\\n*   **(30 Points): Strong Domain and Content Alignment:** Domain name strongly and unambiguously suggests a category, and the website content consistently and explicitly reinforces that category.  They tell the same clear story.\\n*   **(15 Points): Partial Domain and Content Alignment:** Domain name and content generally point towards the same category, but the alignment might be less direct, slightly ambiguous, or require some interpretation to connect them.\\n*   **(0 Points): Domain-Content Mismatch or No Alignment:** Domain name suggests one thing, but the content is unclear, suggests something different, or there\\'s no clear connection between the domain and the content\\'s apparent purpose.\\n\\n#### **III. Content Clarity and Unambiguity (Maximum 20 Points)**\\n\\n*   **(20 Points): Unambiguous and Explicit Content:** The website content is very clear, direct, and leaves virtually no room for interpretation. It unambiguously falls into one of the defined categories.\\n*   **(10 Points): Content Requires Some Interpretation:** The content generally points to a category, but requires some interpretation to confidently assign it.  There might be subtle hints, implied meanings, or a need to infer the primary purpose.\\n*   **(0 Points): Ambiguous or Conflicting Content:** The website content is vague, contradictory, or could be reasonably interpreted in multiple ways, making it difficult to confidently assign a category.\\n\\n#### **IV. Category Indicator Strength (Maximum 10 Points)**\\n\\n*   **(10 Points): Multiple Strong Category Indicators:** Presence of numerous strong and clear indicators for a specific category throughout the domain and content (e.g., for Pornography: explicit keywords, descriptions of sexual acts, calls to action to view adult content, age verification prompts).\\n*   **(5 Points): Some Category Indicators Present:** Presence of a few indicators for a category, but they are not overwhelmingly strong or numerous.\\n*   **(0 Points): Lack of Category Indicators:** Few or no clear indicators for any of the defined categories are present in the domain and content.\\n\\n#### **Calculation:**\\n\\n1.  For each of the four sections (I-IV), assess the website and select the point value that best describes the presence of the described factors.\\n2.  Sum up the points from all four sections.\\n3.  The total sum represents the confidence level in percentage (%).\\n\\n#### **Confidence Level Ranges (for reference - already implicitly defined by points):**\\n\\n*   **High Confidence (80-100 Points):**  Strong evidence across multiple factors pointing clearly to a category.\\n*   **Medium Confidence (50-79 Points):** Moderate evidence, some ambiguity or less directness in indicators.\\n*   **Low Confidence (0-49 Points):** Weak or conflicting evidence, high uncertainty about the correct category.\\n\\n#### **Example:**\\n\\nLet\\'s say you are classifying `lucky-slots-online.com` with content about slot games and bonuses.\\n\\n*   **I. Keyword Strength:** Strong keywords in both Domain and Content (e.g., \"slots,\" \"online,\" \"win,\" \"bonuses\") - **25 Points**\\n*   **II. Domain-Content Alignment:** Domain and content strongly align with Gambling - **30 Points**\\n*   **III. Content Clarity:** Content is very clear about gambling activities - **20 Points**\\n*   **IV. Category Indicators:** Multiple indicators of gambling (games, bonuses, calls to action) - **10 Points**\\n\\n**Total Points: 25 + 30 + 20 + 10 = 85 Points.  Confidence: 85%**\\n\\n#### **Using this Point System:**\\n\\nWhen generating the \"reason\" for your classification, you can now also briefly mention the points you assigned for each section to justify the final confidence score. For example:\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Strong keywords in domain and content (25 pts), strong domain-content alignment (30 pts), clear gambling content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n\\n### **Output Requirements: STRICTLY ADHERE TO JSON FORMAT**\\n\\nFor each website, you **MUST** provide your classification in the following **JSON format ONLY**.  **No other format is acceptable.**\\n\\n```json\\n{\\n  \"answer\": (integer: 0, 1, 2, or 3),\\n  \"classification\": (string: \"Benign\", \"Gambling\", \"Pornography\", or \"Harmful\"),\\n  \"reason\": (string: \"Brief explanation of the classification using keywords and hints from the domain and content.\"),\\n  \"confidence\": (integer: 0 to 100, \"Level of confidence in the classification\")\\n}\\n```\\n\\n**Example Input and Expected Output:**\\n\\n**1st Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: news-today.info\\nContent: Welcome to News Today! Get the latest breaking news, top stories, and in-depth analysis from around the world. Covering politics, business, technology, sports, and culture. Stay informed with News Today - your source for reliable journalism.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 0,\\n  \"classification\": \"Benign\",\\n  \"reason\": \"Domain \\'news-today.info\\' and content mention \\'breaking news,\\' \\'top stories,\\' \\'analysis,\\' \\'reliable journalism,\\' indicating a general information/news website. Strong keywords in domain and content (35 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), some category indicators (5 pts). Total 90 points.\",\\n  \"confidence\": 90\\n}\\n```\\n\\n**2nd Sample:**\\n\\n**Input Data:**\\n\\n```\\nDomain: lucky-slots-online.com\\nContent:  Spin to win big at Lucky Slots Online! Play hundreds of exciting slot games, claim your bonuses, and join the fun.  Licensed and regulated for your safety. 24/7 customer support available. Join now and get 100 free spins!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Domain \\'lucky-slots-online.com\\' and content include keywords like \\'slots,\\' \\'win,\\' \\'casino,\\' \\'bonuses,\\' \\'free spins,\\' indicating a gambling website.\",\\n  \"confidence\": 100\\n}\\n```\\n\\n**3rd Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: adult-pleasures.net\\nContent:  Experience the hottest adult entertainment online. Unlimited access to exclusive videos and photos. 18+ only. Join our community of pleasure seekers today!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 2,\\n  \"classification\": \"Pornography\",\\n  \"reason\": \"Domain \\'adult-pleasures.net\\' and content include explicit terms like \\'adult entertainment,\\' \\'exclusive videos,\\' \\'photos,\\' \\'18+ only,\\' and \\'pleasure seekers,\\' indicating a pornography website. Strong keywords in both domain and content (25 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n**4th Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: project420.com\\nContent:  Welcome to Project420.com an ongoing community marijuana, hemp search engine and your gateway to 420 products, services and information from all over the world. Marijuana seeds, super bongs, high tech growing marijuana books, contests, communities, cool people and tons of 420 fun.  Potent marijuana websites are added frequently.  Let\\'s help each other grow.  Use the table of contents located to the left and begin your search of 420 world wide websites.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 3,\\n  \"classification\": \"Harmful\",\\n  \"reason\": \"Domain \\'project420.com\\' and content include keywords like \\'marijuana seeds,\\' \\'growing marijuana,\\' and \\'420 products,\\' indicating promotion of illegal drugs under Indonesian law (UU ITE). Strong keywords in content (25 pts), Partial Domain and Content Alignment (15 pts), clear harmful intent (20 pts), multiple indicators (10 pts). Total 70 points.\",\\n  \"confidence\": 70\\n}\\n```\\n**Execution:**\\nProcess the provided website data (input fields: `Domain`, `Content`). Apply the classification categories, definitions, and confidence point system accurately. Generate the mandatory JSON output for each entry. Begin classification now.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nClassify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n\\nDomain: adevotedslave.blogspot.com, Content: \"Delicious Debauchery If You arenâ€™t over the age 18 (21 in some areas) then please leave. If You are offended by sex, BDSM, phone sex or brutal honesty then this isn\\'t the place for You. FRIDAY, JUNE 08, 2007 Why Iâ€™ll never be a full lesbianâ€¦ You know, it doesnâ€™t seem to matter what I am using or what position I am in. Iâ€™ve tried it with dildos, vibes, my gear shift knob, my fingers, and miscellaneous household objects, with just about anything and every thing I can think of. Iâ€™ve done it on my hands and knees on my back, with one leg up. Iâ€™ve tried putting the base on the floor and bouncing up and down. Iâ€™ve put it up against a wall and pushed against it. It doesnâ€™t matter. Nothing that I do makes it so that I can fuck myself hard enough to make it feel just right. Nothing can take me to that place where I am balancing between the pleasure of being fucked and the pain of having a cock battering my insides. Of course, when Iâ€™m trying to simulate hard fucking it still feels good, but itâ€™s that kind of feeling you get when you are almost there, but not quite. You know where you want to go but you just canâ€™t get there. Itâ€™s frustrating as hell. Besides, I have never actually had a toy, or object Iâ€™ve converted into a toy, thatâ€™s felt like a real cock anyway. They all just feel like random objects that fall short of the glory that is a real, hard, throbbing, piece of flesh designed for fucking. Of course, if thereâ€™s some woman out there whoâ€™s willing to try to convert me into a full fledged lesbian anyway, Iâ€™m more than willing to submit to that kind of training. Who knows? A woman with a strap on may be able to fuck me just like I like. Of course, Iâ€™d not refuse returning the favor. It could be a mutually beneficial experiment. POSTED BY DEIDRE AT 11:13 AM 2 COMMENTS SATURDAY, APRIL 28, 2007 Bruised and Battered For some reason I think that the depth of my pussy fluctuates. Sometimes when Masterâ€™s cock slides inside of me it is the perfect fit. The head hits in just the... en melted apart. A puff of smoke flew up, and there was another familiar sensation... the smell of burned meat. Again the heat, with a dirty orange light where the flame burned away the bits of tissue stuck to the wire. Again the application, the shudder, the pop, the hiss, the smoke. Around the stencil he went, carefully watching the brand, lining up the strikes. As the skin opens along the brand\\'s outline, it shifts. About halfway around, he started almost freehanding, using the stencil as a reminder of what he wanted, not as a guide. Then with the outline complete, and even, he used its angles as a reference for the final strikes down the middle. There were ten strikes in all. Ten times the iron glowed, ten times it lowered to her helpless flesh. Ten times she moaned as her love for me was burned into her very body. As I continued my research I rediscovered Fakirâ€™s site and the process of using an electro-cautery pencil instead of using the striking method with heated ribbons of steel. I showed a few examples to Master, including a photo of hisdevotionâ€™s back. He asked how one goes about getting an electro-cautery pencil. Until that moment it never even occurred to me that a person outside of the medical profession could just go and buy one. Well, for less than a hundred bucks one can buy a battery operated one with various different tips. Something wonderful happened as I scrolled through all of the options for this tool. I began reading in the descriptions that they get up to 2200 degrees Fahrenheit. Instead of the fear I expected to feel at such a number I felt the nervous flutters of excitement. If Master decides to go this direction, and have His mark of ownership burned into my skin in this fashion, I will have 2200 degrees focused on my skin, burning me, marking me bit by bit. This makes me incredibly aroused. Perhaps I have the makings of a pain slut afterall. POSTED BY DEIDRE AT 1:43 PM 0 COMMENTS TUESDAY, MARCH 13, 2007 masks Iâ€™ve said before that my relationship with Master and the trust and honesty that is the foundation of that relationship has made me more fully myself than I ever have been before. I am more aware of the different aspects of myself and I am able to lie them all out on the table to be poked at and prodded by any stranger who happens by. On the other hand there are those for whom I must wear certain masks. My relationship with Master might have inspired an intense honesty with myself and with Him, as well as with those who can accept us this way and those whose judgement doesnâ€™t matter, but itâ€™s also forced me to be more dishonest than ever to those around me, my neighbors, friends and family. I have been asked by potential sisters, by friends and by callers about how the world sees me. What does Master let the neighbors see? His friends? My friends? My family? His family? Itâ€™s all kind of complicated. The neighbors/His friends: * My neighbors see a 28 year old girl who is living with a 53 year old man. They see the 2 of us happily interacting with each other. He is very active in our neighborhood community. I am not. I occasionally wave to the neighbors or have a conversation with them and most of the time they say something to the effect of, \"Itâ€™s been a while. How have you been?\" Most of them are nice people, but I have little desire to interact with them. They know that I am a phone sex operator and most of them are intrigued. They do not know that I am a slave but there are probably quite a few who have observed that I donâ€™t make any decisions of importance. My friends: * My friends all know just about everything there is to know. Friends that I consider to be mine (as opposed to those that I know because of Master) are basically those from the internet. This includes a few people from collarme and other personals sites but the vast majority is made up of people from LiveJournal. LiveJournal is where all of my meaningful social interaction takes place, as sad as that may seem. There just arenâ€™t a whole lot of people like me nearby and being friends with a girl whoâ€™s first priority is never friendships can be trying. His family: * Masterâ€™s brother, K. He is aware of the basics of the dynamics between Master and myself however itâ€™s never discussed with him and itâ€™s never thrown in his face. We donâ€™t play or fuck in front of K. Iâ€™ve never been loaned to K. Master and I have discussed it and if K is anything heâ€™s probably more submissive than dominant. K knows that I am a phone sex operator. I think he may have heard me on occasion when I am taking calls. * The rest of Masterâ€™s family. He has one of those families that has reunions every year. The patriarch of the family is in his 90s and still kicking. I havenâ€™t met everyone yet, but Iâ€™ve met the ones that are either closest or that Master likes the most. They have opened their arms and accepted me, making me an honorary â€“insert Masterâ€™s family name here- One of them told me over thanksgiving that she has never seen Master as happy as he is now and she thinks that I am to thank for that. What higher compliment can someone pay me than that? My Family: * My sister, E, and I used to be very close. Weâ€™re not anymore though. When I went to her house last summer I had just come from my fathers and wasnâ€™t wearing my collar. The first night I was getting ready to go out with her and my mother and put it back on. Her reaction was that my necklace didnâ€™t match my outfit and I should take the jewelry off. I refused explaining that Master (I used his first name) gave it to me and I hadnâ€™t really had it off since except for when I visited dad. I think she started to get the idea at that point. She knew a little before then, but I think the image of me collared finished off any of the blank parts in her imagination of how I was living my life. I donâ€™t think she approves. We havenâ€™t spoken since then. Iâ€™m not sure how much my lifestyle has to do with that though. * My mother knows the pieces and details that I think sh...  get enough. Sheâ€™s become my newest addiction. I lick the juice of her squirt from my arm and slide my fingers from her into my mouth. I love that taste. Outside the sun is beginning to rise. Weâ€™ve been lost in each other for the entire night. The magic begins to dissipate. We get dressed once again shy and nervous. The blush on my face doesnâ€™t stop me from telling her how much I enjoyed the night. It turns out that she did too and that we both want to do it again. Soon. POSTED BY DEIDRE AT 1:59 PM 0 COMMENTS Older Posts Home Subscribe to: Posts (Atom) ABOUT DEIDRE DEIDRE STRADDLING THE LINE BETWEEN FANTASY AND REALITY deidre is a 28 yo lifestyle slave, phone sex operator and fetish model. she is happily owned. VIEW MY COMPLETE PROFILE LINKS 101 Things about deidre NITEFLIRT delicious debauchery Join for Free What is NiteFlirt? MP3 RECORDINGS The Holy Fuck **fetish sample Gender Blender **hypnosis sample How to Get Out of a Ticket **fetish sample Gym Discipline **fetish sample French Maid **fetish sample Lesbian Strap-on BDSM **fetish sample Lesbian Tickle Bondage **fetish sample Isabella\\'s Sex Slave **hypnosis sample Gagged By Goddess **fetish sample Boarding School Humiliation **fetish sample RECORDINGS (BY THE MINUTE) rates and descriptions deidre\\'s daily prayer deidre gets face fucked equipment sampler deidre\\'s discipline barking cough tickle torture psalm 23 (deidre style) a steamy shower & my fav toy PHOTO SETS rates and descriptions teddy bears & coloring books deidre\\'s puppy training photo sampler bondage the golden set deidre\\'s naughty picnic deidre in distress foot bondage PODCASTS Session 2 Interview with Miss Isabella Session 1 WISHFUL THINKING amazon wishlist deidre\\'s corset fund LABELS questions (13) NiteFlirt (12) mp3 (11) calls (6) gifts (5) music (5) pics (2) podcast (2) meme (1) quotes (1) BLOG ARCHIVE â–¼ 2007 (41) â–¼ JUNE (1) â–¼ JUN 08 (1) WHY Iâ€™LL NEVER BE A FULL LESBIANâ€¦ â–º APRIL (2) â–º MARCH (6) â–º FEBRUARY (11) â–º JANUARY (21) â–º 2006 (117) â–º 1978 (2)\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<think>\\nOkay, I need to classify this website into one of the four categories: Benign, Gambling, Pornography, or Harmful. Let\\'s start by looking at the domain and content provided.  The domain is adevotedslave.blogspot. com. The word \"slavery\" in the domain could be a red flag because it might relate to harmful activities like human trafficking. But I should also consider if it\\'s used in a different context, such as BDSM or kink communities which are more about consensual adult content.  Looking at the content, there are several posts discussing BDSM practices, phone sex, and explicit sexual scenarios. For example, one post talks about trying various objects for self-pleasure and another describes a session involving masks and domination. There\\'s also mention of being a \"lifestyle slave\" and a \"phone sex operator,\" which suggests the site is focused on adult content.  The content includes terms like \"BDSM,\" \"fetish,\" \"bondage,\" and explicit descriptions of sexual acts. These keywords are strong indicators of Pornography because they relate to adult themes and explicit material intended for arousal.  However, I should check if it falls under Harmful instead. The website doesn\\'t seem to promote illegal activities like drug sales or cybercrime. It\\'s more about consensual adult content, which would typically fall under Pornography rather than Harmful unless there are elements of non-consent or exploitation, which aren\\'t clearly present here.  The domain and content align well with Pornography because the focus is on explicit sexual content and fetishes. There are no strong indicators pointing towards Gambling or Benign categories. The content is unambiguous in its adult themes, so it doesn\\'t seem to be a case of harmful activities outside the scope of Pornography.  In terms of confidence assessment: - Keyword Strength: Strong keywords like \"fetish,\" \"bondage,\" and explicit sexual descriptions are present both in the domain and content. - Domain-Content Alignment: The domain suggests a BDSM or slavery theme, which is reinforced by the content discussing explicit adult scenarios. - Content Clarity: The content is very clear about its focus on adult entertainment and fetishes without ambiguity. - Category Indicators: Multiple indicators such as explicit sexual descriptions, use of terms like \"phone sex operator,\" and detailed accounts of BDSM practices.  Based on these factors, I\\'m confident that this website falls under the Pornography category with a high confidence score.\\n</think>\\n```json\\n{\\n  \"answer\": 2,\\n  \"classification\": \"Pornography\",\\n  \"reason\": \"The domain \\'adevotedslave. blogspot.com\\' and content include explicit terms like \\'BDSM,\\' \\'fetish,\\' \\'bondage,\\' and detailed descriptions of sexual acts, indicating a focus on adult entertainment. Strong keywords in both domain and content (25 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```<|eot_id|>'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['test'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d4bd38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3090 Ti. Max memory = 23.551 GB.\n",
      "3.441 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f793d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 33,128 | Num Epochs = 2 | Total steps = 1,665\n",
      "O^O/ \\_/ \\    Batch size per device = 5 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (5 x 4 x 1) = 20\n",
      " \"-____-\"     Trainable parameters = 97,255,424/3,000,000,000 (3.24% trained)\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='401' max='1665' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 401/1665 9:38:42 < 30:33:18, 0.01 it/s, Epoch 0.24/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.312200</td>\n",
       "      <td>5.440914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.063000</td>\n",
       "      <td>4.987135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.062400</td>\n",
       "      <td>3.579475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.394600</td>\n",
       "      <td>1.360912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.742300</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.029804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.024851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.601300</td>\n",
       "      <td>0.019787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.016867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.014520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.013574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.013232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.011845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.012137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.012772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.012019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.011952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.011193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.011639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.012319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.011727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.010738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.010598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.011021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.010620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.009478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.009902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.011152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.010728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>0.009967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.008882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.010287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.009694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.008692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.011382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.012170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.010337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.010210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.009963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.009215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 15/134 00:10 < 01:28, 1.35 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to outputs/checkpoint-100\n",
      "loading configuration file config.json from cache at /home/fishmon/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/aae9d3e87a2c47cc465b6980017a05e1d1d61c8c/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": [\n",
      "      \"lm_head\",\n",
      "      \"multi_modal_projector\",\n",
      "      \"merger\",\n",
      "      \"modality_projection\"\n",
      "    ],\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to outputs/checkpoint-200\n",
      "loading configuration file config.json from cache at /home/fishmon/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/aae9d3e87a2c47cc465b6980017a05e1d1d61c8c/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": [\n",
      "      \"lm_head\",\n",
      "      \"multi_modal_projector\",\n",
      "      \"merger\",\n",
      "      \"modality_projection\"\n",
      "    ],\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to outputs/checkpoint-300\n",
      "loading configuration file config.json from cache at /home/fishmon/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/aae9d3e87a2c47cc465b6980017a05e1d1d61c8c/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": [\n",
      "      \"lm_head\",\n",
      "      \"multi_modal_projector\",\n",
      "      \"merger\",\n",
      "      \"modality_projection\"\n",
      "    ],\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to outputs/checkpoint-400\n",
      "loading configuration file config.json from cache at /home/fishmon/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/aae9d3e87a2c47cc465b6980017a05e1d1d61c8c/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": [\n",
      "      \"lm_head\",\n",
      "      \"multi_modal_projector\",\n",
      "      \"merger\",\n",
      "      \"modality_projection\"\n",
      "    ],\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# checkpoint_path = \"./outputs/checkpoint-700\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# if os.path.exists(checkpoint_path):\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     trainer.train(resume_from_checkpoint=checkpoint_path)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     print(f\"Warning: Checkpoint {checkpoint_path} not found. Starting from scratch.\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:314\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:77\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/accelerate/accelerator.py:2454\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2454\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n",
    "\n",
    "# checkpoint_path = \"./outputs/checkpoint-700\"\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     trainer.train(resume_from_checkpoint=checkpoint_path)\n",
    "# else:\n",
    "#     print(f\"Warning: Checkpoint {checkpoint_path} not found. Starting from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this before the automatic evaluation at step 500\n",
    "trainer.evaluate(eval_dataset=trainer.eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d15042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fc928",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a15012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 1. Max memory: 23.551 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are an Expert Website Classifier tasked with categorizing websites (using provided Domain & Content) into distinct categories: **0 - Benign**, **1 - Gambling**, **2 - Pornography**, or **3 - Harmful**. Output strict JSON including classification, reason, and a point-based confidence score (0-100). This aids a sophisticated website prediction system for digital safety.\\n\\n### **Categories & Definitions:**\\n\\n-   **0 - Benign:** General info, news, entertainment, services, e-commerce, educational sites, blogs, informational pages, and general entertainment sites. No gambling/porn themes.\\n    -   **Includes:** Educational discussion of sensitive topics (drugs, gambling, adult, illegal), suggestive content (dating, lingerie) *without* explicit material, drugs for educational or news purposes (e.g., addiction recovery, medical cannabis research).\\n\\n-   **1 - Gambling:** Promotes/facilitates betting, casino, poker, lottery, wagering. Includes online betting platforms, casinos, poker sites, lottery sites, and any website that encourages or provides means for users to wager money on games of chance or skill.\\n    -   **Keywords:** betting, casino, poker, odds, jackpot, slots, roulette, judi, slot gacor, togel, similar gambling-related terminology within their domain name or content.\\n    \\n-   **2 - Pornography:** These are websites that contain explicit sexual content intended to cause arousal. Explicit sexual content (images, videos, text) for arousal. Immoral content (bestiality, child exploitation). or links to such materials. \\n    -   **Keywords:** porn, bokep, sex, xxx, adult, nude, erotic, explicit video/photo.\\n\\n- **3 - Harmful**: Websites that does not fall to Benign, Gambling, and Pornography category. These websites engage in or promote activities harmful to users or violating laws/regulations, including:\\n    - Malware Distribution: Hosting/downloading computer viruses, worms, ransomware, spyware, etc.\\n    - Cybercrime: Phishing kits, hacking tools, stolen data markets, carding forums.\\n    - Extremism & Terrorism: Content inciting violence, extremist ideologies, or terrorist recruitment.\\n    - Violations of Indonesian Law:\\n        - Insults, defamation, blackmail, or threats.\\n        - Hoaxes/misleading news, hate speech, or incitement of violence.\\n    - Copyright Infringement/Piracy: Illegal software/cracks, torrents, pirated media.\\n    - Drugs/Narcotics: Sale/promotion of illegal drugs (e.g., cocaine, heroin) or unregulated pharmaceuticals.\\n    - Weapons: Sale of illegal firearms, explosives, or weapons.\\n    - Other Illegal Activities: Counterfeit goods, money laundering, human trafficking.\\n    - **Examples:**  \\n        - Illegal: `darknet-drugs.com` (drug sales), `pirated-movies.id` (piracy).  \\n        - Harmful: `extremist-forum.net` (terrorism recruitment), `hackers-tools.org` (phishing kits).  \\n        - Scam/phishng: `hadiah-telkomsel7.blogspot.com` (non-genuine website).\\n    - **Keywords**:\\nmalware, ransomware, phishing, hack, terrorism, jual narkoba, senjata ilegal, konten SARA, berita bohong, ancaman, pembajakan, cracked software, carding, darknet.\\n    - **Exceptions:** \\n        - Licensed/unlicensed gambling â†’ **1 - Gambling**; scams â†’ **3 - Harmful**.\\n\\n\\n### **Input Data Context:**\\n\\nYou will be provided with data entries, each consisting of two primary fields:\\n\\n*   **Domain:** The domain name or URL of the website (e.g., `example.com`, `gamble-site.net`). This can provide hints about the website\\'s purpose.\\n*   **Content:**  The textual content scraped from the website. This content offers detailed information about the website\\'s topics, services, and themes.\\n\\n### **Labeling Instructions:**\\n\\nAnalyze both the **Domain** and the **Content** provided. Use keywords and contextual clues from both to determine the most appropriate category for the website.  Consider the primary purpose and content focus of the website when classifying.\\n\\n### **Confidence Assessment Guidelines: Point-Based System (Total Possible Points: 100)**\\n\\nTo determine the **confidence** level (0-100) for your classification, evaluate the following factors and sum up the points.  The total points will directly correspond to the confidence percentage (e.g., 95 points = 95% confidence).\\n\\n#### **I. Keyword Strength and Relevance (Maximum 40 Points)**\\n*   **(40 Points):  Exceptional Keyword Strength: Explicit and Overwhelming Keywords in BOTH Domain and Content:** Presence of extremely explicit and overwhelmingly strong keywords that are *unquestionably* indicative of a specific category in *both* the domain name AND the website content. These keywords leave absolutely no doubt about the website\\'s nature. (e.g., Domain: `casino-royal-betting.com`, Content:  \"Gamble now and win HUGE jackpots on slots, poker, roulette! Real money betting!\").  This represents the absolute strongest keyword signal possible.\\n*   **(35 Points): Clear and Strong Keywords in BOTH Domain and Content:**  Presence of highly explicit keywords strongly indicative of a specific category in both the domain name AND the website content. (e.g., Domain: `bet.com`, Content:  \"Bet on sports and casino games!\").\\n*   **(25 Points): Strong Keywords in EITHER Domain OR Content:** Presence of highly explicit keywords strongly indicative of a specific category in EITHER the domain name OR the website content, but not both.\\n*   **(15 Points): Some Relevant Keywords:** Presence of keywords related to a category, but they are less explicit, less frequent, or require more contextual interpretation in either domain or content.\\n*   **(0 Points): Weak or Generic Keywords:** Lack of clear category-specific keywords in both domain and content. Keywords are generic and do not strongly suggest any specific category.\\n\\n#### **II. Domain and Content Alignment (Maximum 30 Points)**\\n\\n*   **(30 Points): Strong Domain and Content Alignment:** Domain name strongly and unambiguously suggests a category, and the website content consistently and explicitly reinforces that category.  They tell the same clear story.\\n*   **(15 Points): Partial Domain and Content Alignment:** Domain name and content generally point towards the same category, but the alignment might be less direct, slightly ambiguous, or require some interpretation to connect them.\\n*   **(0 Points): Domain-Content Mismatch or No Alignment:** Domain name suggests one thing, but the content is unclear, suggests something different, or there\\'s no clear connection between the domain and the content\\'s apparent purpose.\\n\\n#### **III. Content Clarity and Unambiguity (Maximum 20 Points)**\\n\\n*   **(20 Points): Unambiguous and Explicit Content:** The website content is very clear, direct, and leaves virtually no room for interpretation. It unambiguously falls into one of the defined categories.\\n*   **(10 Points): Content Requires Some Interpretation:** The content generally points to a category, but requires some interpretation to confidently assign it.  There might be subtle hints, implied meanings, or a need to infer the primary purpose.\\n*   **(0 Points): Ambiguous or Conflicting Content:** The website content is vague, contradictory, or could be reasonably interpreted in multiple ways, making it difficult to confidently assign a category.\\n\\n#### **IV. Category Indicator Strength (Maximum 10 Points)**\\n\\n*   **(10 Points): Multiple Strong Category Indicators:** Presence of numerous strong and clear indicators for a specific category throughout the domain and content (e.g., for Pornography: explicit keywords, descriptions of sexual acts, calls to action to view adult content, age verification prompts).\\n*   **(5 Points): Some Category Indicators Present:** Presence of a few indicators for a category, but they are not overwhelmingly strong or numerous.\\n*   **(0 Points): Lack of Category Indicators:** Few or no clear indicators for any of the defined categories are present in the domain and content.\\n\\n#### **Calculation:**\\n\\n1.  For each of the four sections (I-IV), assess the website and select the point value that best describes the presence of the described factors.\\n2.  Sum up the points from all four sections.\\n3.  The total sum represents the confidence level in percentage (%).\\n\\n#### **Confidence Level Ranges (for reference - already implicitly defined by points):**\\n\\n*   **High Confidence (80-100 Points):**  Strong evidence across multiple factors pointing clearly to a category.\\n*   **Medium Confidence (50-79 Points):** Moderate evidence, some ambiguity or less directness in indicators.\\n*   **Low Confidence (0-49 Points):** Weak or conflicting evidence, high uncertainty about the correct category.\\n\\n#### **Example:**\\n\\nLet\\'s say you are classifying `lucky-slots-online.com` with content about slot games and bonuses.\\n\\n*   **I. Keyword Strength:** Strong keywords in both Domain and Content (e.g., \"slots,\" \"online,\" \"win,\" \"bonuses\") - **25 Points**\\n*   **II. Domain-Content Alignment:** Domain and content strongly align with Gambling - **30 Points**\\n*   **III. Content Clarity:** Content is very clear about gambling activities - **20 Points**\\n*   **IV. Category Indicators:** Multiple indicators of gambling (games, bonuses, calls to action) - **10 Points**\\n\\n**Total Points: 25 + 30 + 20 + 10 = 85 Points.  Confidence: 85%**\\n\\n#### **Using this Point System:**\\n\\nWhen generating the \"reason\" for your classification, you can now also briefly mention the points you assigned for each section to justify the final confidence score. For example:\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Strong keywords in domain and content (25 pts), strong domain-content alignment (30 pts), clear gambling content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n\\n### **Output Requirements: STRICTLY ADHERE TO JSON FORMAT**\\n\\nFor each website, you **MUST** provide your classification in the following **JSON format ONLY**.  **No other format is acceptable.**\\n\\n```json\\n{\\n  \"answer\": (integer: 0, 1, 2, or 3),\\n  \"classification\": (string: \"Benign\", \"Gambling\", \"Pornography\", or \"Harmful\"),\\n  \"reason\": (string: \"Brief explanation of the classification using keywords and hints from the domain and content.\"),\\n  \"confidence\": (integer: 0 to 100, \"Level of confidence in the classification\")\\n}\\n```\\n\\n**Example Input and Expected Output:**\\n\\n**1st Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: news-today.info\\nContent: Welcome to News Today! Get the latest breaking news, top stories, and in-depth analysis from around the world. Covering politics, business, technology, sports, and culture. Stay informed with News Today - your source for reliable journalism.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 0,\\n  \"classification\": \"Benign\",\\n  \"reason\": \"Domain \\'news-today.info\\' and content mention \\'breaking news,\\' \\'top stories,\\' \\'analysis,\\'\\'reliable journalism,\\' indicating a general information/news website. Strong keywords in domain and content (35 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), some category indicators (5 pts). Total 90 points.\",\\n  \"confidence\": 90\\n}\\n```\\n\\n**2nd Sample:**\\n\\n**Input Data:**\\n\\n```\\nDomain: lucky-slots-online.com\\nContent:  Spin to win big at Lucky Slots Online! Play hundreds of exciting slot games, claim your bonuses, and join the fun.  Licensed and regulated for your safety. 24/7 customer support available. Join now and get 100 free spins!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Domain \\'lucky-slots-online.com\\' and content include keywords like\\'slots,\\' \\'win,\\' \\'casino,\\' \\'bonuses,\\' \\'free spins,\\' indicating a gambling website.\",\\n  \"confidence\": 100\\n}\\n```\\n\\n**3rd Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: adult-pleasures.net\\nContent:  Experience the hottest adult entertainment online. Unlimited access to exclusive videos and photos. 18+ only. Join our community of pleasure seekers today!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 2,\\n  \"classification\": \"Pornography\",\\n  \"reason\": \"Domain \\'adult-pleasures.net\\' and content include explicit terms like \\'adult entertainment,\\' \\'exclusive videos,\\' \\'photos,\\' \\'18+ only,\\' and \\'pleasure seekers,\\' indicating a pornography website. Strong keywords in both domain and content (25 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n**4th Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: project420.com\\nContent:  Welcome to Project420.com an ongoing community marijuana, hemp search engine and your gateway to 420 products, services and information from all over the world. Marijuana seeds, super bongs, high tech growing marijuana books, contests, communities, cool people and tons of 420 fun.  Potent marijuana websites are added frequently.  Let\\'s help each other grow.  Use the table of contents located to the left and begin your search of 420 world wide websites.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 3,\\n  \"classification\": \"Harmful\",\\n  \"reason\": \"Domain \\'project420.com\\' and content include keywords like\\'marijuana seeds,\\' \\'growing marijuana,\\' and \\'420 products,\\' indicating promotion of illegal drugs under Indonesian law (UU ITE). Strong keywords in content (25 pts), Partial Domain and Content Alignment (15 pts), clear harmful intent (20 pts), multiple indicators (10 pts). Total 70 points.\",\\n  \"confidence\": 70\\n}\\n```\\n**Execution:**\\nProcess the provided website data (input fields: `Domain`, `Content`). Apply the classification categories, definitions, and confidence point system accurately. Generate the mandatory JSON output for each entry. Begin classification now.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nClassify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n \\\\nDomain: jitutogel.net, Content: \\\\\"Whatsapp Telegram Livechat DAFTAR LOGIN BERANDA SLOT GAMES LIVE CASINO POKER ONLINE E-SPORTS ARCADE LOTTERY PROMOTION Jackpot Gaming Playstar TTG Slots Spadegaming RedTiger GMW CQ9 Gaming Live Gaming Evolution Gaming Sexy Gaming SAgaming HOgaming Gameplay OpusGaming Sports Gaming CMD368 SBOBET UBOBET TFGaming Ultraplay SabaEsports JituTogel: Serunya Game Online dengan Hadiah Uang Nyata! Penyedia Games Metode Pembayaran Bank Cimb Niaga BCA Danamon Permata Mandiri BNI BRI Panin Pulsa XL Axiata Tri Telkomsel Axis E-Money QRIS Dana OVO LinkAja Gopay Â©2024 jitutogel. All rights reserved | 18+ LIVECHAT\\\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIt seems you\\'ve entered a valid JavaScript code snippet!\\n\\nHere\\'s what happens next:\\n\\n1. Your input is parsed as JavaScript syntax highlighting mode enabled HTML/CSS/javascript snippet `<div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# 1. Load checkpoint\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"./outputs/checkpoint-400\",\n",
    "    max_seq_length = 30000,  # Match training config\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# 2. Prepare for inference\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",  # Must match training template\n",
    ")\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "# 3. Load prompts\n",
    "with open('./prompt/labelling_promptv4.txt', 'r') as f:\n",
    "    system_prompt = f.read()\n",
    "with open('./prompt/class_3_sample1.txt', 'r') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# 4. Format input\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n {sample_text}\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    max_length = 30000,  # Prevent OOM\n",
    "    truncation = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 5. Generate\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens = 2048,  \n",
    "    temperature = 0.7,  \n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.2,\n",
    "    eos_token_id        = tokenizer.eos_token_id,\n",
    "    pad_token_id        = tokenizer.pad_token_id,\n",
    "    use_cache = True,\n",
    ")\n",
    "tokenizer.batch_decode(outputs)\n",
    "# print(tokenizer.decode(outputs[0]))                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12225aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n\u001b[1;32m     34\u001b[0m text_streamer \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer, skip_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_streamer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                   \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/peft/peft_model.py:1875\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1874\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1875\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1877\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:1574\u001b[0m, in \u001b[0;36munsloth_fast_generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;66;03m# Mixed precision autocast\u001b[39;00m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode(), torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[0;32m-> 1574\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1577\u001b[0m \u001b[38;5;66;03m# Return accelerate back\u001b[39;00m\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;66;03m# if accelerate_new_send_to_device is not None:\u001b[39;00m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;66;03m#     accelerate.utils.operations.send_to_device = accelerate_old_send_to_device\u001b[39;00m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;66;03m# pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/transformers/generation/utils.py:3434\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3434\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3436\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3437\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3438\u001b[0m     outputs,\n\u001b[1;32m   3439\u001b[0m     model_kwargs,\n\u001b[1;32m   3440\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3441\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:1026\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_CausalLM_fast_forward\u001b[39m(\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1010\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1024\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1026\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1034\u001b[0m         causal_mask \u001b[38;5;241m=\u001b[39m xformers\u001b[38;5;241m.\u001b[39mattn_bias\u001b[38;5;241m.\u001b[39mLowerTriangularMask() \u001b[38;5;28;01mif\u001b[39;00m HAS_XFORMERS \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:980\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward_inference\u001b[0;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m    972\u001b[0m residual\u001b[38;5;241m.\u001b[39mcopy_(X) \u001b[38;5;66;03m# residual = X\u001b[39;00m\n\u001b[1;32m    973\u001b[0m X \u001b[38;5;241m=\u001b[39m fast_rms_layernorm_inference(\n\u001b[1;32m    974\u001b[0m     decoder_layer\u001b[38;5;241m.\u001b[39mpost_attention_layernorm,\n\u001b[1;32m    975\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    978\u001b[0m     variance \u001b[38;5;241m=\u001b[39m variance,\n\u001b[1;32m    979\u001b[0m )\n\u001b[0;32m--> 980\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mfast_swiglu_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp_gate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemp_gate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp_up\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemp_up\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[1;32m    988\u001b[0m next_decoder_cache\u001b[38;5;241m.\u001b[39mappend(present_key_value)\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:299\u001b[0m, in \u001b[0;36mfast_swiglu_inference\u001b[0;34m(self, X, temp_gate, temp_up)\u001b[0m\n\u001b[1;32m    295\u001b[0m bsz, _, hd \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# mlp_size = self.config.intermediate_size\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda:0\")\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m gate \u001b[38;5;241m=\u001b[39m \u001b[43mfast_linear_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemp_gate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m up   \u001b[38;5;241m=\u001b[39m fast_linear_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m  up_proj, X, out \u001b[38;5;241m=\u001b[39m temp_up)\n\u001b[1;32m    301\u001b[0m gate \u001b[38;5;241m=\u001b[39m torch_nn_functional_silu(gate, inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/kernels/utils.py:440\u001b[0m, in \u001b[0;36mfast_linear_forward\u001b[0;34m(proj, X, temp_lora, out)\u001b[0m\n\u001b[1;32m    438\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch_matmul(X, W\u001b[38;5;241m.\u001b[39mt(), out \u001b[38;5;241m=\u001b[39m out)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m bsz \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m q_len \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 440\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfast_gemv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     W \u001b[38;5;241m=\u001b[39m fast_dequantize(W\u001b[38;5;241m.\u001b[39mt(), W_quant, use_global_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/kernels/utils.py:346\u001b[0m, in \u001b[0;36mfast_gemv\u001b[0;34m(X, W, quant_state, out)\u001b[0m\n\u001b[1;32m    344\u001b[0m df \u001b[38;5;241m=\u001b[39m torch_empty(absmax\u001b[38;5;241m.\u001b[39mshape, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch_cuda_device(device):\n\u001b[0;32m--> 346\u001b[0m     \u001b[43mcdequantize_blockwise_fp32\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabsmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabsmax2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes_c_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocksize2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes_c_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCUDA_STREAM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     df \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m offset\n\u001b[1;32m    351\u001b[0m     absmax \u001b[38;5;241m=\u001b[39m df\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Load the system prompt from the file\n",
    "with open('./prompt/labelling_promptv4.txt', 'r') as system_file:\n",
    "    system_prompt = system_file.read()\n",
    "\n",
    "# Load the label from the file\n",
    "with open('./prompt/class_3_sample2.txt', 'r') as label_file:\n",
    "    label = label_file.read()\n",
    "\n",
    "# Define the messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n{label}\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(input_ids = inputs, max_new_tokens = 2048, use_cache = True,\n",
    "#                          temperature = 0.7, min_p = 0.1)\n",
    "# tokenizer.batch_decode(outputs)\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 2048,\n",
    "                   use_cache = True, temperature = 1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3facb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A question that gets to the heart of our planet's atmosphere!\n",
      "\n",
      "The sky appears blue because of a phenomenon called scattering, which occurs when sunlight interacts with atmospheric gases.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. Sunlight enters Earth's atmosphere and contains all colors of visible light, except blue-violet.\n",
      "2. When sunlight encounters atmospheric gases like nitrogen and oxygen, shorter wavelengths (like violet) are scattered in all directions.\n",
      "3. However, blue light has a longer wavelength and is scattered at lower angles (about 90 degrees) compared to violet.\n",
      "4. Because blue light is scattered at lower angles, our eyes perceive it as the dominant color\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Why is the sky is blue\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee7f367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're referring to the classic poem by Edward Taylor!\n",
      "\n",
      "\"The Sky Above the Sea, or Heaven\"\n",
      "\n",
      "\"In heaven's name there's no shame\n",
      "But for shame's sake there's no shame\n",
      "For heaven's sake there's no shame\n",
      "But for shame's sake there's no shame\n",
      "But heaven's sake there's no shame\n",
      "But for shame's sake there's no shame\n",
      "In heaven's name there's no shame\"\n",
      "\n",
      "While the poem itself doesn't explicitly say why the sky is blue, it does describe the sky in heaven, where there is no shame.\n",
      "\n",
      "However, if we're looking for a scientific explanation for why the sky appears blue, it's because of the way light behaves when it passes through water droplets or clouds in the Earth's atmosphere.\n",
      "\n",
      "Light comes in various colors, including blue. When light travels through a medium like air or water, shorter wavelengths like blue are scattered more than longer wavelengths like red. This phenomenon is called Rayleigh scattering.\n",
      "\n",
      "On Earth, our atmosphere is composed of gases, including nitrogen and oxygen, and particles like pollutants and dust. These particles scatter light in all directions, including downwards from clouds or water droplets.\n",
      "\n",
      "When sunlight enters Earth's atmosphere, it encounters these particles. As light travels through the atmosphere, shorter wavelengths like blue are scattered more than longer wavelengths like red.\n",
      "\n",
      "This scattered light appears blue because of its shorter wavelength, and our eyes perceive blue light. Blue appears to be everywhere because light with blue wavelengths penetrates all around.\n",
      "\n",
      "That's a science-based answer to why the sky appears blue!\n",
      "\n",
      "However, if you'd like an answer to the original question that asks why the sky appears blue? \n",
      "\n",
      "\"The sky is blue because light with blue wavelengths penetrates all around.\"\n",
      "\n",
      "(Note: Blue appears everywhere because light with blue wavelengths penetrates all around.)\n",
      "\n",
      "However, it is worth noting that blue does not appear everywhere. It appears everywhere because light with blue wavelengths penetrates all around.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Why is the sky is blue\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 2048,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a39afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Why is the sky is blue\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0197b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Load the system prompt from the file\n",
    "with open('./prompt/labelling_promptv4.txt', 'r') as system_file:\n",
    "    system_prompt = system_file.read()\n",
    "\n",
    "# Load the label from the file\n",
    "with open('./prompt/class_3_sample2.txt', 'r') as label_file:\n",
    "    label = label_file.read()\n",
    "\n",
    "# Define the messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n{label}\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 4096,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7fada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
