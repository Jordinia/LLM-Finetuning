{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eea4c6f",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10039837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordinia/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8651d1",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4834df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.8: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 24.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.6. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.8 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from typing import Dict, Any\n",
    "\n",
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "config = load_config('./config/config-0505-qwen3.yml')\n",
    "\n",
    "# Model and Training Config\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=config.get('model_name', 'unsloth/Llama-3.2-3B-Instruct-bnb-4bit'),\n",
    "    max_seq_length=config.get('max_seq_length', 30000),\n",
    "    dtype=config.get('dtype', None),\n",
    "    load_in_4bit=config.get('load_in_4bit', True)\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=config.get('lora_r', 32),\n",
    "    target_modules=config.get('target_modules', [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]),\n",
    "    lora_alpha=config.get('lora_alpha', 32),\n",
    "    lora_dropout=config.get('lora_dropout', 0),\n",
    "    bias=config.get('bias', 'none'),\n",
    "    use_gradient_checkpointing=config.get('use_gradient_checkpointing', 'unsloth'),\n",
    "    random_state=config.get('random_state', 3407),\n",
    "    use_rslora=config.get('use_rslora', False),\n",
    "    loftq_config=config.get('loftq_config', None),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222b187",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311f5a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens_in_conversation(conversation, tokenizer):\n",
    "    \"\"\"\n",
    "    Count tokens in a ChatML conversation EXACTLY as done during training.\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False,  # Match training\n",
    "        truncation=False,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return tokenized.shape[1]\n",
    "\n",
    "def analyze_dataset_tokens(dataset, tokenizer, max_seq_length):\n",
    "    \"\"\"\n",
    "    Analyze token usage statistics across a dataset with ChatML format.\n",
    "    \"\"\"\n",
    "    token_counts = []\n",
    "\n",
    "    for idx, item in enumerate(dataset):\n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Processing conversation {idx}/{len(dataset)}\")\n",
    "\n",
    "        tokens = count_tokens_in_conversation(item[\"conversations\"], tokenizer)\n",
    "        token_counts.append(tokens)\n",
    "\n",
    "    return {\n",
    "        \"total_conversations\": len(token_counts),\n",
    "        \"total_tokens\": sum(token_counts),\n",
    "        \"average_tokens\": sum(token_counts) / len(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"conversations_over_limit\": sum(1 for c in token_counts if c > max_seq_length),\n",
    "        \"percent_over_limit\": (sum(c > max_seq_length for c in token_counts) / len(token_counts)) * 100,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34902bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing conversation 0/7245\n",
      "Processing conversation 1000/7245\n",
      "Processing conversation 2000/7245\n",
      "Processing conversation 3000/7245\n",
      "Processing conversation 4000/7245\n",
      "Processing conversation 5000/7245\n",
      "Processing conversation 6000/7245\n",
      "Processing conversation 7000/7245\n",
      "Train split stats: {'total_conversations': 7245, 'total_tokens': 36559657, 'average_tokens': 5046.191442374051, 'max_tokens': 10186, 'min_tokens': 3495, 'conversations_over_limit': 0, 'percent_over_limit': 0.0}\n",
      "Processing conversation 0/60\n",
      "Validation split stats: {'total_conversations': 60, 'total_tokens': 303146, 'average_tokens': 5052.433333333333, 'max_tokens': 7312, 'min_tokens': 3712, 'conversations_over_limit': 0, 'percent_over_limit': 0.0}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"jordinia/netpro-finetune\", name=\"chatml_thought_7k\")\n",
    "max_seq_length = config.get('max_seq_length', 30000)\n",
    "\n",
    "train_stats = analyze_dataset_tokens(dataset[\"train\"], tokenizer, max_seq_length)\n",
    "print(\"Train split stats:\", train_stats)\n",
    "\n",
    "val_stats = analyze_dataset_tokens(dataset[\"validation\"], tokenizer, max_seq_length)\n",
    "print(\"Validation split stats:\", val_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f212220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_text(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"conversations\"],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False  # match training setup\n",
    "        )\n",
    "    }\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].map(convert_to_text, num_proc=4)\n",
    "dataset[\"validation\"] = dataset[\"validation\"].map(convert_to_text, num_proc=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f3edac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'text'],\n",
       "        num_rows: 7245\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['conversations', 'text'],\n",
       "        num_rows: 60\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b1044",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87c48ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH=all\n",
      "env: WANDB_SILENT=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title wandb init\n",
    "import wandb\n",
    "%env WANDB_WATCH=all\n",
    "%env WANDB_SILENT=true\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b1d11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to WANDB!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers.utils import logging\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Loads from .env file automatically\n",
    "\n",
    "# 2. Verify token loading\n",
    "if not os.getenv(\"WANDB_API_KEY\"):\n",
    "    raise ValueError(\"WANDB_API_KEY not found in .env file\")\n",
    "\n",
    "# 3. Initialize and upload\n",
    "os.environ[\"WANDB_API_KEY\"]=os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "logging.set_verbosity_info()\n",
    "project_name = \"netpro-finetune\" \n",
    "entity_name = \"jordinia-netpro\"\n",
    "run_name = \"unsloth/Qwen3-4B-unsloth-sft-2025-05-11-TESTING\" # Set your desired run name\n",
    "\n",
    "# Initialize WANDB (FIXED ENTITY/PROJECT)\n",
    "try:\n",
    "    run = wandb.init(\n",
    "        entity=entity_name,\n",
    "        project=project_name,\n",
    "        name=run_name,\n",
    "        # id=\"j4vh49mi\",\n",
    "        # resume=\"allow\" # Uncomment to resume a previous run\n",
    "    )\n",
    "    print(\"Successfully connected to WANDB!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize WANDB: {str(e)}\")\n",
    "    # Consider exiting if WANDB is critical\n",
    "    # sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "052ed88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "PyTorch: setting up devices\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir                  =config.get('output_dir', 'outputs'),\n",
    "    per_device_train_batch_size =config.get('per_device_train_batch_size', 2),\n",
    "    per_device_eval_batch_size  =config.get('per_device_eval_batch_size', 1),\n",
    "    gradient_accumulation_steps =config.get('gradient_accumulation_steps', 4),\n",
    "    warmup_ratio                =config.get('warmup_ratio', 0.05),\n",
    "    max_steps                   =config.get('max_steps', 4125),\n",
    "    learning_rate               =2e-5,\n",
    "    fp16                        =not is_bfloat16_supported(),\n",
    "    bf16                        =is_bfloat16_supported(),\n",
    "    optim                       =config.get('optim', 'adamw_8bit'),\n",
    "    weight_decay                =config.get('weight_decay', 0.1),\n",
    "    lr_scheduler_type           =config.get('lr_scheduler_type', 'cosine'),\n",
    "    eval_strategy               =config.get('eval_strategy', 'steps'),\n",
    "    eval_steps                  =config.get('eval_steps', 50),\n",
    "    save_strategy               =config.get('save_strategy', 'steps'),\n",
    "    save_steps                  =config.get('save_steps', 100),\n",
    "    save_total_limit            =config.get('save_total_limit', 3),\n",
    "    logging_steps               =config.get('logging_steps', 10),\n",
    "    seed                        =config.get('seed', 3407),\n",
    "    report_to                   =config.get('report_to', 'wandb'),\n",
    "    load_best_model_at_end      =config.get('load_best_model_at_end', True),\n",
    "    metric_for_best_model       =config.get('metric_for_best_model', 'eval_loss'),\n",
    "    # max_grad_norm               =config.get('max_grad_norm', 0.3),\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model               =model,\n",
    "    tokenizer           =tokenizer,\n",
    "    train_dataset       =dataset[\"train\"],\n",
    "    eval_dataset        =dataset[\"validation\"],\n",
    "    dataset_text_field  =config.get('dataset_text_field', 'text'),\n",
    "    max_seq_length      =config.get('max_seq_length', 30000),\n",
    "    data_collator       =DataCollatorForSeq2Seq(\n",
    "        tokenizer               =tokenizer,\n",
    "        padding                 =config.get('data_collator', {}).get('padding', True),\n",
    "        pad_to_multiple_of      =config.get('data_collator', {}).get('pad_to_multiple_of', 8),\n",
    "        max_length              =config.get('max_seq_length', 30000),\n",
    "    ),\n",
    "    dataset_num_proc    =config.get('dataset_num_proc', 2),\n",
    "    packing             =config.get('packing', False),\n",
    "    args                =args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cc344",
   "metadata": {},
   "source": [
    "### Train on Completions Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07252b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_on_responses_only_custom(trainer):\n",
    "#     \"\"\"\n",
    "#     Custom version for website classification that:\n",
    "#     1. Preserves system prompt context\n",
    "#     2. Only trains on assistant responses (JSON + reasoning)\n",
    "#     3. Handles length consistency for batching\n",
    "#     \"\"\"\n",
    "#     tokenizer = trainer.tokenizer\n",
    "    \n",
    "#     # Manually define token sequences for your template\n",
    "#     SYSTEM_TOKENS = tokenizer.encode(\n",
    "#         \"<|start_header_id|>system<|end_header_id|>\\n\\n\", \n",
    "#         add_special_tokens=False\n",
    "#     )\n",
    "#     USER_TOKENS = tokenizer.encode(\n",
    "#         \"<|start_header_id|>user<|end_header_id|>\\n\\n\", \n",
    "#         add_special_tokens=False\n",
    "#     )\n",
    "#     ASSISTANT_TOKENS = tokenizer.encode(\n",
    "#         \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \n",
    "#         add_special_tokens=False\n",
    "#     )\n",
    "\n",
    "#     def custom_masking(examples):\n",
    "#         input_ids = examples[\"input_ids\"]\n",
    "        \n",
    "#         # Create labels if they don't exist\n",
    "#         labels = examples.get(\"labels\", [ids.copy() for ids in input_ids])\n",
    "#         new_labels = []\n",
    "        \n",
    "#         for seq_id, seq_labels in zip(input_ids, labels):\n",
    "#             n = len(seq_id)\n",
    "#             mask = [-100] * n\n",
    "#             i = 0\n",
    "            \n",
    "#             while i < n:\n",
    "#                 # Check for system prompt\n",
    "#                 if seq_id[i:i+len(SYSTEM_TOKENS)] == SYSTEM_TOKENS:\n",
    "#                     i += len(SYSTEM_TOKENS)\n",
    "#                     continue\n",
    "                    \n",
    "#                 # Check for user prompt\n",
    "#                 if seq_id[i:i+len(USER_TOKENS)] == USER_TOKENS:\n",
    "#                     i += len(USER_TOKENS)\n",
    "#                     continue\n",
    "                    \n",
    "#                 # Check for assistant prompt\n",
    "#                 if seq_id[i:i+len(ASSISTANT_TOKENS)] == ASSISTANT_TOKENS:\n",
    "#                     start = i\n",
    "#                     i += len(ASSISTANT_TOKENS)\n",
    "                    \n",
    "#                     # Find end of assistant response\n",
    "#                     while i < n:\n",
    "#                         if seq_id[i] == tokenizer.eos_token_id:\n",
    "#                             end = i\n",
    "#                             break\n",
    "#                         i += 1\n",
    "#                     else:\n",
    "#                         end = n\n",
    "                    \n",
    "#                     # Unmask assistant response\n",
    "#                     mask[start:end] = seq_labels[start:end]\n",
    "#                     break\n",
    "                    \n",
    "#                 i += 1\n",
    "            \n",
    "#             # Enforce length matching (critical fix)\n",
    "#             if len(mask) != len(seq_id):\n",
    "#                 mask = mask[:len(seq_id)] + [-100] * (len(seq_id) - len(mask))\n",
    "            \n",
    "#             new_labels.append(mask)\n",
    "        \n",
    "#         return {\"labels\": new_labels}\n",
    "\n",
    "#     # Apply to datasets with length verification\n",
    "#     def apply_masking(dataset):\n",
    "#         dataset = dataset.map(\n",
    "#             custom_masking,\n",
    "#             batched=True,\n",
    "#             batch_size=1000,\n",
    "#             num_proc=4,\n",
    "#         )\n",
    "#         # Verify lengths\n",
    "#         for i in range(min(3, len(dataset))):\n",
    "#             assert len(dataset[i][\"input_ids\"]) == len(dataset[i][\"labels\"]), \\\n",
    "#                 f\"Length mismatch in sample {i}\"\n",
    "#         return dataset\n",
    "    \n",
    "#     trainer.train_dataset = apply_masking(trainer.train_dataset)\n",
    "    \n",
    "#     if trainer.eval_dataset is not None:\n",
    "#         trainer.eval_dataset = apply_masking(trainer.eval_dataset)\n",
    "        \n",
    "#     return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9499b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = train_on_responses_only_custom(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96e2c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final verification (optional)\n",
    "# print(\"\\n=== Final Pre-Train Check ===\")\n",
    "# sample = trainer.train_dataset[0]\n",
    "# print(\"First sample labels preview:\")\n",
    "# print(\"Input IDs length:\", len(sample[\"input_ids\"])) \n",
    "# print(\"Labels length:\", len(sample[\"labels\"]))\n",
    "# print(\"Last 10 labels:\", sample[\"labels\"][-10:])  # Should show unmasked assistant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6841e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a sample from the training set\n",
    "# sample_index = 5  # Try different indices\n",
    "# sample = trainer.train_dataset[sample_index]\n",
    "\n",
    "# # Decode the full input context\n",
    "# print(\"==== Full Input Context ====\")\n",
    "# print(tokenizer.decode(sample[\"input_ids\"]))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Decode the labels with masking visualization\n",
    "# print(\"==== Training Targets (Masked) ====\")\n",
    "# masked_labels = []\n",
    "# for token_id, label_id in zip(sample[\"input_ids\"], sample[\"labels\"]):\n",
    "#     if label_id == -100:\n",
    "#         # Show masked tokens as blank spaces\n",
    "#         masked_labels.append(\" \")\n",
    "#     else:\n",
    "#         # Show actual token\n",
    "#         masked_labels.append(tokenizer.decode([token_id]))\n",
    "\n",
    "# print(\"\".join(masked_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "136a307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = trainer.train_dataset.select(range(2)).with_format(\"torch\")[0]\n",
    "# print(batch[\"input_ids\"])\n",
    "# print(batch[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221f500",
   "metadata": {},
   "source": [
    "### Memory Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c95376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 7245\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b14b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"train\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d4bd38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3090. Max memory = 24.0 GB.\n",
      "4.207 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f793d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, text. If conversations, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 7,245 | Num Epochs = 2 | Total steps = 906\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 132,120,576/4,000,000,000 (3.30% trained)\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41' max='906' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 41/906 19:02 < 7:02:21, 0.03 it/s, Epoch 0.04/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.895900</td>\n",
       "      <td>1.875898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.757900</td>\n",
       "      <td>1.708042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.576400</td>\n",
       "      <td>1.539896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.478400</td>\n",
       "      <td>1.404150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, text. If conversations, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 1\n",
      "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, text. If conversations, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, text. If conversations, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, text. If conversations, text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer_stats = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# checkpoint_path = \"./outputs/checkpoint-700\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# if os.path.exists(checkpoint_path):\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#     trainer.train(resume_from_checkpoint=checkpoint_path)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#     print(f\"Warning: Checkpoint {checkpoint_path} not found. Starting from scratch.\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:314\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:31\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/LLM-Finetuning/Malicious-Web/unsloth_compiled_cache/UnslothSFTTrainer.py:748\u001b[39m, in \u001b[36m_UnslothSFTTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs = \u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/_utils.py:1039\u001b[39m, in \u001b[36m_unsloth_pre_compute_loss\u001b[39m\u001b[34m(self, model, inputs, *args, **kwargs)\u001b[39m\n\u001b[32m   1033\u001b[39m     logger.warning_once(\n\u001b[32m   1034\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1035\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1036\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1037\u001b[39m     )\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1039\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/trainer.py:3801\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3799\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3800\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3801\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3803\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1844\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1843\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1844\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1846\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1849\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1790\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1787\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1788\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1792\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1793\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1794\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1795\u001b[39m     ):\n\u001b[32m   1796\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/accelerate/utils/operations.py:814\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/accelerate/utils/operations.py:802\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m prior = _maybe_set_eval_frame(callback)\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    634\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:1207\u001b[39m, in \u001b[36mPeftModelForCausalLM_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1191\u001b[39m \u001b[38;5;129m@torch\u001b[39m._disable_dynamo\n\u001b[32m   1192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[32m   1193\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1205\u001b[39m     **kwargs,\n\u001b[32m   1206\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:193\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:1050\u001b[39m, in \u001b[36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[39m\n\u001b[32m   1048\u001b[39m     \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28mself\u001b[39m.model._has_no_labels = labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1050\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1061\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1063\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:859\u001b[39m, in \u001b[36mLlamaModel_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[39m\n\u001b[32m    857\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m custom_forward\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m     layer_outputs = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_custom_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_reentrant\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreserve_rng_state\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    868\u001b[39m     hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    870\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m prior = _maybe_set_eval_frame(callback)\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    634\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/gradient_checkpointing.py:747\u001b[39m, in \u001b[36munsloth_checkpoint\u001b[39m\u001b[34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[39m\n\u001b[32m    742\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    744\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    745\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muse_reentrant=False.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    746\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnslothCheckpointFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    749\u001b[39m     gen = _checkpoint_without_reentrant_generator(\n\u001b[32m    750\u001b[39m         function, preserve, context_fn, determinism_check, debug, *args, **kwargs\n\u001b[32m    751\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth_zoo/gradient_checkpointing.py:463\u001b[39m, in \u001b[36mUnslothCheckpointFunction.forward\u001b[39m\u001b[34m(ctx, run_function, preserve_rng_state, *args)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ctx._requires_gradient: ctx.save_for_backward(*tensor_inputs)\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     outputs = \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_gpu_buffer: MAIN_STREAM.wait_stream(EXTRA_STREAM)\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:856\u001b[39m, in \u001b[36mLlamaModel_fast_forward.<locals>.create_custom_forward.<locals>.custom_forward\u001b[39m\u001b[34m(*inputs)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcustom_forward\u001b[39m(*inputs):\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:544\u001b[39m, in \u001b[36mLlamaDecoderLayer_fast_forward\u001b[39m\u001b[34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[39m\n\u001b[32m    542\u001b[39m residual = hidden_states\n\u001b[32m    543\u001b[39m hidden_states = fast_rms_layernorm(\u001b[38;5;28mself\u001b[39m.input_layernorm, hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m hidden_states, self_attn_weights, present_key_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m           \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/qwen3.py:193\u001b[39m, in \u001b[36mQwen3Attention_fast_forward\u001b[39m\u001b[34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    192\u001b[39m attn_output = A.reshape(bsz, q_len, n_heads*head_dim)\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_o\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m attn_weights = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights, past_key_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/kernels/fast_lora.py:457\u001b[39m, in \u001b[36mapply_lora_o\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_lora_o\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    456\u001b[39m     OW, OW_quant, OA, OB, OS = get_lora_parameters(\u001b[38;5;28mself\u001b[39m.o_proj)\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m     O = \u001b[43mLoRA_W\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m O\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/torch/amp/autocast_mode.py:465\u001b[39m, in \u001b[36mcustom_fwd.<locals>.decorate_fwd\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    464\u001b[39m     args[\u001b[32m0\u001b[39m]._fwd_used_autocast = torch.is_autocast_enabled(device_type)\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    467\u001b[39m     autocast_context = torch.is_autocast_enabled(device_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/kernels/fast_lora.py:408\u001b[39m, in \u001b[36mLoRA_W.forward\u001b[39m\u001b[34m(ctx, X, W, W_quant, A, B, S)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;129m@torch_amp_custom_fwd\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(ctx, X : torch.Tensor,\n\u001b[32m    406\u001b[39m             W, W_quant, A, B, S):\n\u001b[32m    407\u001b[39m     dtype = X.dtype\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     XW = \u001b[43mmatmul_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m     ctx.custom_saved_tensors = (W, W_quant, S,)\n\u001b[32m    410\u001b[39m     ctx.save_for_backward(A, B, X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/kernels/utils.py:491\u001b[39m, in \u001b[36mmatmul_lora\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m A \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    489\u001b[39m     \u001b[38;5;66;03m# LoRA is enabled\u001b[39;00m\n\u001b[32m    490\u001b[39m     A, B = A.t(), B.t()\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     XA = \u001b[43mtorch_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m     out.addmm_(XA, B.to(dtype), alpha = s)\n\u001b[32m    493\u001b[39m     \u001b[38;5;66;03m# out += (X @ A.to(dtype)) @ (s * B.to(dtype))\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n",
    "\n",
    "# checkpoint_path = \"./outputs/checkpoint-700\"\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     trainer.train(resume_from_checkpoint=checkpoint_path)\n",
    "# else:\n",
    "#     print(f\"Warning: Checkpoint {checkpoint_path} not found. Starting from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this before the automatic evaluation at step 500\n",
    "trainer.evaluate(eval_dataset=trainer.eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d15042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78330dcd",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889541fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(config.get('lora_model', 'outputs'))\n",
    "tokenizer.save_pretrained(config.get('lora_model', 'outputs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db833ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.46s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jordinia/test-finetune/commit/2b75560795409c528fa06c24185ba3ef1c45a59a', commit_message='Initial commit of Qwen3-0505-LORA model', commit_description='', oid='2b75560795409c528fa06c24185ba3ef1c45a59a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/jordinia/test-finetune', endpoint='https://huggingface.co', repo_type='model', repo_id='jordinia/test-finetune'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "model_id = \"jordinia/test-finetune\"\n",
    "local_model_path = \"/home/fishmon/AJ/LLM-Finetuning/Malicious-Web/model/qwen3-0505-lora_model\"\n",
    "hf_token = \"\"\n",
    "\n",
    "# Authenticate\n",
    "HfFolder.save_token(hf_token)\n",
    "api = HfApi()\n",
    "\n",
    "# Upload the folder with a commit message\n",
    "api.upload_folder(\n",
    "    folder_path=local_model_path,\n",
    "    repo_id=model_id,\n",
    "    repo_type=\"model\",\n",
    "    use_auth_token=hf_token,\n",
    "    commit_message=\"Initial commit of Qwen3-0505-LORA model\",  # Add your commit message here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa7d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=\"/path/to/local/model\",\n",
    "    repo_id=\"jordinia/netpro-0505-lora\",\n",
    "    repo_type=\"model\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323768d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 12.5 out of 31.26 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:00<00:00, 407.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"qwen3-0505-fp16-vllm\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e190418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: qwen3-0505-fp16-vllm\n",
      "INFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {2560, 151936}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.16.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.16.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.17.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.17.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.18.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.18.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.19.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.19.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.20.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.20.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.21.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.22.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.22.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.23.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.23.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.24.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.24.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.25.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.25.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.26.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.26.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.27.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.27.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.28.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.28.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.29.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.29.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.30.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.30.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.31.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.31.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.32.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.32.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.33.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.weight,      torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_output.weight, torch.bfloat16 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.33.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.weight,      torch.bfloat16 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.weight,      torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.34.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.weight,      torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_output.weight, torch.bfloat16 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.34.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.weight,      torch.bfloat16 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.weight,      torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.35.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.35.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 40960\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2560\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 9728\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151645\n",
      "INFO:gguf.vocab:Setting special token type pad to 151654\n",
      "INFO:gguf.vocab:Setting add_bos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- messages[0].content + '\\n\\n' }}\n",
      "    {%- endif %}\n",
      "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
      "{%- for forward_message in messages %}\n",
      "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
      "    {%- set message = messages[index] %}\n",
      "    {%- set tool_start = '<tool_response>' %}\n",
      "    {%- set tool_start_length = tool_start|length %}\n",
      "    {%- set start_of_message = message.content[:tool_start_length] %}\n",
      "    {%- set tool_end = '</tool_response>' %}\n",
      "    {%- set tool_end_length = tool_end|length %}\n",
      "    {%- set start_pos = (message.content|length) - tool_end_length %}\n",
      "    {%- if start_pos < 0 %}\n",
      "        {%- set start_pos = 0 %}\n",
      "    {%- endif %}\n",
      "    {%- set end_of_message = message.content[start_pos:] %}\n",
      "    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\n",
      "        {%- set ns.multi_step_tool = false %}\n",
      "        {%- set ns.last_query_index = index %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set content = message.content %}\n",
      "        {%- set reasoning_content = '' %}\n",
      "        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n",
      "            {%- set reasoning_content = message.reasoning_content %}\n",
      "        {%- else %}\n",
      "            {%- if '</think>' in message.content %}\n",
      "                {%- set content = (message.content.split('</think>')|last).lstrip('\\n') %}\n",
      "                {%- set reasoning_content = (message.content.split('</think>')|first).rstrip('\\n') %}\n",
      "                {%- set reasoning_content = (reasoning_content.split('<think>')|last).lstrip('\\n') %}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "        {%- if loop.index0 > ns.last_query_index %}\n",
      "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
      "            {%- else %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "            {%- endif %}\n",
      "        {%- else %}\n",
      "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls %}\n",
      "            {%- for tool_call in message.tool_calls %}\n",
      "                {%- if (loop.first and content) or (not loop.first) %}\n",
      "                    {{- '\\n' }}\n",
      "                {%- endif %}\n",
      "                {%- if tool_call.function %}\n",
      "                    {%- set tool_call = tool_call.function %}\n",
      "                {%- endif %}\n",
      "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
      "                {{- tool_call.name }}\n",
      "                {{- '\", \"arguments\": ' }}\n",
      "                {%- if tool_call.arguments is string %}\n",
      "                    {{- tool_call.arguments }}\n",
      "                {%- else %}\n",
      "                    {{- tool_call.arguments | tojson }}\n",
      "                {%- endif %}\n",
      "                {{- '}\\n</tool_call>' }}\n",
      "            {%- endfor %}\n",
      "        {%- endif %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
      "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:qwen3-0505-fp16-vllm.gguf: n_tensors = 398, total_size = 8.0G\n",
      "Writing:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 1.98G/8.05G [00:05<00:23, 261Mbyte/s]/home/fishmon/AJ/llama.cpp/gguf-py/gguf/lazy.py:217: RuntimeWarning: overflow encountered in cast\n",
      "  return type(self)(meta=meta, args=full_args, kwargs=kwargs, func=(lambda a, *args, **kwargs: a.astype(*args, **kwargs)))\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.05G/8.05G [00:51<00:00, 158Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to qwen3-0505-fp16-vllm.gguf\n"
     ]
    }
   ],
   "source": [
    "!python /home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py qwen3-0505-fp16-vllm --outfile qwen3-0505-fp16-vllm.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7e8b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily skip merging for conversion\n",
    "model.save_pretrained(\"qwen3-0505-q4-vllm\", tokenizer, save_method=\"4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e74e2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: qwen3-0505-q4-vllm\n",
      "INFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:choosing --outtype bf16 from first tensor type (torch.bfloat16)\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {2560, 151936}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.uint8 --> BF16, shape = {1, 12451840}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 6097, in <module>\n",
      "    main()\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 6091, in main\n",
      "    model_instance.write()\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 401, in write\n",
      "    self.prepare_tensors()\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 277, in prepare_tensors\n",
      "    for new_name, data_torch in (self.modify_tensors(data_torch, name, bid)):\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 2609, in modify_tensors\n",
      "    yield from super().modify_tensors(data_torch, name, bid)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 245, in modify_tensors\n",
      "    return [(self.map_tensor_name(name), data_torch)]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 236, in map_tensor_name\n",
      "    raise ValueError(f\"Can not map tensor {name!r}\")\n",
      "ValueError: Can not map tensor 'model.layers.10.mlp.down_proj.weight.absmax'\n"
     ]
    }
   ],
   "source": [
    "!python /home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py qwen3-0505-q4-vllm --outfile qwen3-0505-q4-vllm.gguf --outtype auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python convert_hf_to_gguf.py qwen3-0505-fp16-vllm --outfile qwen3-0505--q4fp16-vllm.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fc928",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a39afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordinia/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.8: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 24.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.6. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "\n",
    "# 1. Load base model (EXACTLY matching your training config)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",  # Must match training\n",
    "    max_seq_length = 30000,  # Same as training\n",
    "    load_in_4bit = True,     # Must match training\n",
    "    device_map = \"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a2c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load your fine-tuned adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    model_id = \"/home/jordinia/Projects/LLM-Finetuning/Malicious-Web/model/netpro-qwen3-0505-lora\",\n",
    "    adapter_name = \"web_classifier\",\n",
    "    is_trainable = False,  # Crucial for inference\n",
    ")\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797161d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "\n",
    "# 1. Load base model (EXACTLY matching your training config)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",  # Must match training\n",
    "    max_seq_length = 30000,  # Same as training\n",
    "    load_in_4bit = True,     # Must match training\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# 2. Load your fine-tuned adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    model_id = \"model/netpro-qwen3-0505-lora\",\n",
    "    adapter_name = \"web_classifier\",\n",
    "    is_trainable = False,  # Crucial for inference\n",
    ")\n",
    "\n",
    "# 3. Merge and optimize (reduces VRAM by ~30%)\n",
    "model = model.merge_and_unload()\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "# 3. Load prompts\n",
    "with open('./prompt/labelling_promptv4.txt', 'r') as f:\n",
    "    system_prompt = f.read()\n",
    "with open('./prompt/class_0_sample1.txt', 'r') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# 4. Format input\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n {sample_text}\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    max_length = 30000,  # Prevent OOM\n",
    "    truncation = True,\n",
    "    enable_thinking = True, \n",
    ").to(\"cuda\")\n",
    "\n",
    "# 5. Generate\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 2048,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1, top_p = 0.9,)\n",
    "# outputs = model.generate(\n",
    "#     inputs,\n",
    "#     max_new_tokens = 2048,  \n",
    "#     temperature = 0.7,  \n",
    "#     top_p = 0.9,\n",
    "#     repetition_penalty = 1.2,\n",
    "#     eos_token_id        = tokenizer.eos_token_id,\n",
    "#     pad_token_id        = tokenizer.pad_token_id,\n",
    "#     use_cache = True,\n",
    "# )\n",
    "# tokenizer.batch_decode(outputs)\n",
    "# # print(tokenizer.decode(outputs[0]))                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c70be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Merge and optimize (reduces VRAM by ~30%)\n",
    "model = model.merge_and_unload()\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "# 3. Load prompts\n",
    "with open('./prompt/labelling_promptv4.txt', 'r') as f:\n",
    "    system_prompt = f.read()\n",
    "with open('./prompt/class_0_sample1.txt', 'r') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# 4. Format input\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n {sample_text}\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    max_length = 30000,  # Prevent OOM\n",
    "    truncation = True,\n",
    "    enable_thinking = True, \n",
    ").to(\"cuda\")\n",
    "\n",
    "# 5. Generate\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 2048,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1, top_p = 0.9,)\n",
    "# outputs = model.generate(\n",
    "#     inputs,\n",
    "#     max_new_tokens = 2048,  \n",
    "#     temperature = 0.7,  \n",
    "#     top_p = 0.9,\n",
    "#     repetition_penalty = 1.2,\n",
    "#     eos_token_id        = tokenizer.eos_token_id,\n",
    "#     pad_token_id        = tokenizer.pad_token_id,\n",
    "#     use_cache = True,\n",
    "# )\n",
    "# tokenizer.batch_decode(outputs)\n",
    "# # print(tokenizer.decode(outputs[0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af7fada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>system\\nYou are an Expert Website Classifier tasked with categorizing websites (using provided Domain & Content) into distinct categories: **0 - Benign**, **1 - Gambling**, **2 - Pornography**, or **3 - Harmful**. Output strict JSON including classification, reason, and a point-based confidence score (0-100). This aids a sophisticated website prediction system for digital safety.\\n\\n### **Categories & Definitions:**\\n\\n-   **0 - Benign:** General info, news, entertainment, services, e-commerce, educational sites, blogs, informational pages, and general entertainment sites. No gambling/porn themes.\\n    -   **Includes:** Educational discussion of sensitive topics (drugs, gambling, adult, illegal), suggestive content (dating, lingerie) *without* explicit material, drugs for educational or news purposes (e.g., addiction recovery, medical cannabis research).\\n\\n-   **1 - Gambling:** Promotes/facilitates betting, casino, poker, lottery, wagering. Includes online betting platforms, casinos, poker sites, lottery sites, and any website that encourages or provides means for users to wager money on games of chance or skill.\\n    -   **Keywords:** betting, casino, poker, odds, jackpot, slots, roulette, judi, slot gacor, togel, similar gambling-related terminology within their domain name or content.\\n    \\n-   **2 - Pornography:** These are websites that contain explicit sexual content intended to cause arousal. Explicit sexual content (images, videos, text) for arousal. Immoral content (bestiality, child exploitation). or links to such materials. \\n    -   **Keywords:** porn, bokep, sex, xxx, adult, nude, erotic, explicit video/photo.\\n\\n- **3 - Harmful**: Websites that does not fall to Benign, Gambling, and Pornography category. These websites engage in or promote activities harmful to users or violating laws/regulations, including:\\n    - Malware Distribution: Hosting/downloading computer viruses, worms, ransomware, spyware, etc.\\n    - Cybercrime: Phishing kits, hacking tools, stolen data markets, carding forums.\\n    - Extremism & Terrorism: Content inciting violence, extremist ideologies, or terrorist recruitment.\\n    - Violations of Indonesian Law:\\n        - Insults, defamation, blackmail, or threats.\\n        - Hoaxes/misleading news, hate speech, or incitement of violence.\\n    - Copyright Infringement/Piracy: Illegal software/cracks, torrents, pirated media.\\n    - Drugs/Narcotics: Sale/promotion of illegal drugs (e.g., cocaine, heroin) or unregulated pharmaceuticals.\\n    - Weapons: Sale of illegal firearms, explosives, or weapons.\\n    - Other Illegal Activities: Counterfeit goods, money laundering, human trafficking.\\n    - **Examples:**  \\n        - Illegal: `darknet-drugs.com` (drug sales), `pirated-movies.id` (piracy).  \\n        - Harmful: `extremist-forum.net` (terrorism recruitment), `hackers-tools.org` (phishing kits).  \\n        - Scam/phishng: `hadiah-telkomsel7.blogspot.com` (non-genuine website).\\n    - **Keywords**:\\nmalware, ransomware, phishing, hack, terrorism, jual narkoba, senjata ilegal, konten SARA, berita bohong, ancaman, pembajakan, cracked software, carding, darknet.\\n    - **Exceptions:** \\n        - Licensed/unlicensed gambling â†’ **1 - Gambling**; scams â†’ **3 - Harmful**.\\n\\n\\n### **Input Data Context:**\\n\\nYou will be provided with data entries, each consisting of two primary fields:\\n\\n*   **Domain:** The domain name or URL of the website (e.g., `example.com`, `gamble-site.net`). This can provide hints about the website\\'s purpose.\\n*   **Content:**  The textual content scraped from the website. This content offers detailed information about the website\\'s topics, services, and themes.\\n\\n### **Labeling Instructions:**\\n\\nAnalyze both the **Domain** and the **Content** provided. Use keywords and contextual clues from both to determine the most appropriate category for the website.  Consider the primary purpose and content focus of the website when classifying.\\n\\n### **Confidence Assessment Guidelines: Point-Based System (Total Possible Points: 100)**\\n\\nTo determine the **confidence** level (0-100) for your classification, evaluate the following factors and sum up the points.  The total points will directly correspond to the confidence percentage (e.g., 95 points = 95% confidence).\\n\\n#### **I. Keyword Strength and Relevance (Maximum 40 Points)**\\n*   **(40 Points):  Exceptional Keyword Strength: Explicit and Overwhelming Keywords in BOTH Domain and Content:** Presence of extremely explicit and overwhelmingly strong keywords that are *unquestionably* indicative of a specific category in *both* the domain name AND the website content. These keywords leave absolutely no doubt about the website\\'s nature. (e.g., Domain: `casino-royal-betting.com`, Content:  \"Gamble now and win HUGE jackpots on slots, poker, roulette! Real money betting!\").  This represents the absolute strongest keyword signal possible.\\n*   **(35 Points): Clear and Strong Keywords in BOTH Domain and Content:**  Presence of highly explicit keywords strongly indicative of a specific category in both the domain name AND the website content. (e.g., Domain: `bet.com`, Content:  \"Bet on sports and casino games!\").\\n*   **(25 Points): Strong Keywords in EITHER Domain OR Content:** Presence of highly explicit keywords strongly indicative of a specific category in EITHER the domain name OR the website content, but not both.\\n*   **(15 Points): Some Relevant Keywords:** Presence of keywords related to a category, but they are less explicit, less frequent, or require more contextual interpretation in either domain or content.\\n*   **(0 Points): Weak or Generic Keywords:** Lack of clear category-specific keywords in both domain and content. Keywords are generic and do not strongly suggest any specific category.\\n\\n#### **II. Domain and Content Alignment (Maximum 30 Points)**\\n\\n*   **(30 Points): Strong Domain and Content Alignment:** Domain name strongly and unambiguously suggests a category, and the website content consistently and explicitly reinforces that category.  They tell the same clear story.\\n*   **(15 Points): Partial Domain and Content Alignment:** Domain name and content generally point towards the same category, but the alignment might be less direct, slightly ambiguous, or require some interpretation to connect them.\\n*   **(0 Points): Domain-Content Mismatch or No Alignment:** Domain name suggests one thing, but the content is unclear, suggests something different, or there\\'s no clear connection between the domain and the content\\'s apparent purpose.\\n\\n#### **III. Content Clarity and Unambiguity (Maximum 20 Points)**\\n\\n*   **(20 Points): Unambiguous and Explicit Content:** The website content is very clear, direct, and leaves virtually no room for interpretation. It unambiguously falls into one of the defined categories.\\n*   **(10 Points): Content Requires Some Interpretation:** The content generally points to a category, but requires some interpretation to confidently assign it.  There might be subtle hints, implied meanings, or a need to infer the primary purpose.\\n*   **(0 Points): Ambiguous or Conflicting Content:** The website content is vague, contradictory, or could be reasonably interpreted in multiple ways, making it difficult to confidently assign a category.\\n\\n#### **IV. Category Indicator Strength (Maximum 10 Points)**\\n\\n*   **(10 Points): Multiple Strong Category Indicators:** Presence of numerous strong and clear indicators for a specific category throughout the domain and content (e.g., for Pornography: explicit keywords, descriptions of sexual acts, calls to action to view adult content, age verification prompts).\\n*   **(5 Points): Some Category Indicators Present:** Presence of a few indicators for a category, but they are not overwhelmingly strong or numerous.\\n*   **(0 Points): Lack of Category Indicators:** Few or no clear indicators for any of the defined categories are present in the domain and content.\\n\\n#### **Calculation:**\\n\\n1.  For each of the four sections (I-IV), assess the website and select the point value that best describes the presence of the described factors.\\n2.  Sum up the points from all four sections.\\n3.  The total sum represents the confidence level in percentage (%).\\n\\n#### **Confidence Level Ranges (for reference - already implicitly defined by points):**\\n\\n*   **High Confidence (80-100 Points):**  Strong evidence across multiple factors pointing clearly to a category.\\n*   **Medium Confidence (50-79 Points):** Moderate evidence, some ambiguity or less directness in indicators.\\n*   **Low Confidence (0-49 Points):** Weak or conflicting evidence, high uncertainty about the correct category.\\n\\n#### **Example:**\\n\\nLet\\'s say you are classifying `lucky-slots-online.com` with content about slot games and bonuses.\\n\\n*   **I. Keyword Strength:** Strong keywords in both Domain and Content (e.g., \"slots,\" \"online,\" \"win,\" \"bonuses\") - **25 Points**\\n*   **II. Domain-Content Alignment:** Domain and content strongly align with Gambling - **30 Points**\\n*   **III. Content Clarity:** Content is very clear about gambling activities - **20 Points**\\n*   **IV. Category Indicators:** Multiple indicators of gambling (games, bonuses, calls to action) - **10 Points**\\n\\n**Total Points: 25 + 30 + 20 + 10 = 85 Points.  Confidence: 85%**\\n\\n#### **Using this Point System:**\\n\\nWhen generating the \"reason\" for your classification, you can now also briefly mention the points you assigned for each section to justify the final confidence score. For example:\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Strong keywords in domain and content (25 pts), strong domain-content alignment (30 pts), clear gambling content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n\\n### **Output Requirements: STRICTLY ADHERE TO JSON FORMAT**\\n\\nFor each website, you **MUST** provide your classification in the following **JSON format ONLY**.  **No other format is acceptable.**\\n\\n```json\\n{\\n  \"answer\": (integer: 0, 1, 2, or 3),\\n  \"classification\": (string: \"Benign\", \"Gambling\", \"Pornography\", or \"Harmful\"),\\n  \"reason\": (string: \"Brief explanation of the classification using keywords and hints from the domain and content.\"),\\n  \"confidence\": (integer: 0 to 100, \"Level of confidence in the classification\")\\n}\\n```\\n\\n**Example Input and Expected Output:**\\n\\n**1st Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: news-today.info\\nContent: Welcome to News Today! Get the latest breaking news, top stories, and in-depth analysis from around the world. Covering politics, business, technology, sports, and culture. Stay informed with News Today - your source for reliable journalism.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 0,\\n  \"classification\": \"Benign\",\\n  \"reason\": \"Domain \\'news-today.info\\' and content mention \\'breaking news,\\' \\'top stories,\\' \\'analysis,\\' \\'reliable journalism,\\' indicating a general information/news website. Strong keywords in domain and content (35 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), some category indicators (5 pts). Total 90 points.\",\\n  \"confidence\": 90\\n}\\n```\\n\\n**2nd Sample:**\\n\\n**Input Data:**\\n\\n```\\nDomain: lucky-slots-online.com\\nContent:  Spin to win big at Lucky Slots Online! Play hundreds of exciting slot games, claim your bonuses, and join the fun.  Licensed and regulated for your safety. 24/7 customer support available. Join now and get 100 free spins!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Domain \\'lucky-slots-online.com\\' and content include keywords like \\'slots,\\' \\'win,\\' \\'casino,\\' \\'bonuses,\\' \\'free spins,\\' indicating a gambling website.\",\\n  \"confidence\": 100\\n}\\n```\\n\\n**3rd Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: adult-pleasures.net\\nContent:  Experience the hottest adult entertainment online. Unlimited access to exclusive videos and photos. 18+ only. Join our community of pleasure seekers today!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 2,\\n  \"classification\": \"Pornography\",\\n  \"reason\": \"Domain \\'adult-pleasures.net\\' and content include explicit terms like \\'adult entertainment,\\' \\'exclusive videos,\\' \\'photos,\\' \\'18+ only,\\' and \\'pleasure seekers,\\' indicating a pornography website. Strong keywords in both domain and content (25 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n**4th Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: project420.com\\nContent:  Welcome to Project420.com an ongoing community marijuana, hemp search engine and your gateway to 420 products, services and information from all over the world. Marijuana seeds, super bongs, high tech growing marijuana books, contests, communities, cool people and tons of 420 fun.  Potent marijuana websites are added frequently.  Let\\'s help each other grow.  Use the table of contents located to the left and begin your search of 420 world wide websites.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 3,\\n  \"classification\": \"Harmful\",\\n  \"reason\": \"Domain \\'project420.com\\' and content include keywords like \\'marijuana seeds,\\' \\'growing marijuana,\\' and \\'420 products,\\' indicating promotion of illegal drugs under Indonesian law (UU ITE). Strong keywords in content (25 pts), Partial Domain and Content Alignment (15 pts), clear harmful intent (20 pts), multiple indicators (10 pts). Total 70 points.\",\\n  \"confidence\": 70\\n}\\n```\\n**Execution:**\\nProcess the provided website data (input fields: `Domain`, `Content`). Apply the classification categories, definitions, and confidence point system accurately. Generate the mandatory JSON output for each entry. Begin classification now.<|im_end|>\\n<|im_start|>user\\nClassify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n \\\\nDomain: jitutogel.net, Content: \\\\\"Whatsapp Telegram Livechat DAFTAR LOGIN BERANDA SLOT GAMES LIVE CASINO POKER ONLINE E-SPORTS ARCADE LOTTERY PROMOTION Jackpot Gaming Playstar TTG Slots Spadegaming RedTiger GMW CQ9 Gaming Live Gaming Evolution Gaming Sexy Gaming SAgaming HOgaming Gameplay OpusGaming Sports Gaming CMD368 SBOBET UBOBET TFGaming Ultraplay SabaEsports JituTogel: Serunya Game Online dengan Hadiah Uang Nyata! Penyedia Games Metode Pembayaran Bank Cimb Niaga BCA Danamon Permata Mandiri BNI BRI Panin Pulsa XL Axiata Tri Telkomsel Axis E-Money QRIS Dana OVO LinkAja Gopay Â©2024 jitutogel. All rights reserved | 18+ LIVECHAT\\\\\"<|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay, let me start by looking at the domain and content provided. The domain is \"jitutogel.net\". The content mentions several things: WhatsApp, Telegram, live chat, login, and various gaming options like Slot Games, Live Casino, Poker Online, E-Sports, Arcade, Lottery Promotion, Jackpot Gaming, Playstar, TTG Slots, Spadegaming, RedTiger, GMW, CQ9 Gaming, Live Gaming, Evolution Gaming, Sexy Gaming, SA Gaming, HOGaming, Gameplay, OpusGaming, Sports Gaming, CMD368, SBOBET, UBOBET, TFGaming, Ultraplay, SABA Esports. Then there\\'s a note saying \"JituTogel: Serunya Game Online dengan Hadiah Uang Nyata!\" which translates to \"JituTogel: Enjoy Online Games with Real Money Rewards!\" followed by payment methods listed.\\n\\nFirst, I check if these keywords relate to known categories. Words like \"Slot Games\", \"Live Casino\", \"Poker Online\", \"E-Sports\", \"Jackpot Gaming\" immediately make me think of gambling. Also, phrases like \"Hadiah Uang Nyata\" mean real cash rewards, which is common in gambling contexts. Additionally, the domain has \"togel\" which refers to traditional lotteries in Indonesia, often associated with scamming due to its association with fraud in some regions. \\n\\nThe content includes details about deposit methods via banks and mobile providers, which is typical for gambling sites offering real money prizes. However, since the user hasn\\'t specified whether this site is legitimate or part of a scam, but based solely on the words used here, it seems to fit under Gambling because the main theme is providing online game opportunities with real money incentives, even though \"toto\" sometimes relates to fraudulent schemes. But according to guidelines, if it promotes/wagering, then it\\'s Gambling category.\\n\\nLooking further, the term \"JituTogel\" itself may refer to a type of lottery or bet, reinforcing the idea. Now checking against the definition for Gambling: promoting/betting on casino, poker, lottery, etc. So yes, the domain and content both indicate promotional aspects of gambling through online games and real-money rewards. Therefore, the answer should be 1 - Gambling.\\n\\nNow assessing the confidence levels. Looking at the key areas:\\n\\nSection I: Keywords strength. Terms like \"slot games\", \"live casino\", \"real money rewards\" are strong gamblind-indicative. Both domain (\"togel\" linked to lotto/scams, but also possibly genuine) and content have those elements. Since \"togel\" isn\\'t exactly a gambling term per se but here paired with \"game online dan hadiah uang nyata\" (cash reward), so likely Gambling. Maybe 35 points? Or maybe higher?\\n\\nWait, considering the domain and content together. The domain uses \"togel\" which is commonly associated with scammers in certain contexts. But in this case, combined with the list of games, it\\'s probably Gambling. So perhaps stronger keywords in both domain and content. That would give 35 points.\\n\\nSection II: Domain and content alignment. If domain says \"jitutogel.net\" and the content lists many types of online games along with real money promotions, that\\'s a solid match. So full 30 points.\\n\\nSection III: Content clarity. The content is pretty straightforwardly describing a platform that offers online games with real money prizes. Very clear indication of Gambling. Assign 20 points.\\n\\nSection IV: Category indicators. Several repeated terms: slot games, live casino, poker, real money, etc. Many signs. 10 points.\\n\\nSo adding up: 35 + 30 + 20 + 10 = 95 points. High confidence. Hence, answer 1, Classification Gambling, confidence 95.\\n</think>\\n\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Domain \\'jitutogel.net\\' and content emphasize online gaming (\\'slot games\\', \\'live casino\\', \\'poker online\\'), real money rewards (\\'Hadiah Uang Nyata\\'), and payment methods tied to financial transactions. Terms like \\'jackpot gaming\\', \\'lottery promotion\\', and \\'real cash prize\\' strongly associate with gambling activities. Domain contains \\'togel\\' (Indonesian lottery), while content highlights profit-driven offerings consistent with gambling operations.\",\\n  \"confidence\": 95\\n}<|im_end|>']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./prompt/labelling_promptv4.txt', 'r') as f:\n",
    "    system_prompt = f.read()\n",
    "with open('./prompt/class_3_sample1.txt', 'r') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# 4. Format input\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n {sample_text}\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    max_length = 30000,  # Prevent OOM\n",
    "    truncation = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 5. Generate\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens = 2048,  \n",
    "    temperature = 0.7,  \n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.2,\n",
    "    eos_token_id        = tokenizer.eos_token_id,\n",
    "    pad_token_id        = tokenizer.pad_token_id,\n",
    "    use_cache = True,\n",
    ")\n",
    "tokenizer.batch_decode(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
