{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eea4c6f",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10039837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fishmon/.conda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from jsonschema import validate, ValidationError\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056f1cb",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a3927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"json\", data_files=\"./dataset/netpro_chatml_thought.jsonl\")\n",
    "\n",
    "dataset = load_dataset(\"jordinia/netpro-finetune\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc5a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset[\"train\"]  # Access the 'train' split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ed36710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output Type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset Info:\n",
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 33262\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOutput Type:\", type(dataset))\n",
    "print(\"Dataset Info:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8651d1",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4834df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1 with CUDA 1201 (you have 2.6.0+cu124)\n",
      "    Python  3.11.10 (you have 3.11.11)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 1. Max memory: 23.551 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.3.19 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from typing import Dict, Any\n",
    "\n",
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "config = load_config('./config/config-0405.yml')\n",
    "\n",
    "# Model and Training Config\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=config.get('model_name', 'unsloth/Llama-3.2-3B-Instruct-bnb-4bit'),\n",
    "    max_seq_length=config.get('max_seq_length', 30000),\n",
    "    dtype=config.get('dtype', None),\n",
    "    load_in_4bit=config.get('load_in_4bit', True)\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=config.get('lora_r', 32),\n",
    "    target_modules=config.get('target_modules', [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]),\n",
    "    lora_alpha=config.get('lora_alpha', 32),\n",
    "    lora_dropout=config.get('lora_dropout', 0),\n",
    "    bias=config.get('bias', 'none'),\n",
    "    use_gradient_checkpointing=config.get('use_gradient_checkpointing', 'unsloth'),\n",
    "    random_state=config.get('random_state', 3407),\n",
    "    use_rslora=config.get('use_rslora', False),\n",
    "    loftq_config=config.get('loftq_config', None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eb134c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d2424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a875a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = dataset.train_test_split(test_size=0.004)\n",
    "dataset_dict = dataset.train_test_split(test_size=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c5151d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'text'],\n",
       "        num_rows: 33128\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversations', 'text'],\n",
       "        num_rows: 134\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b1044",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87c48ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjordinia\u001b[0m (\u001b[33mjordinia-netpro\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title wandb init\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "434ca268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH=all\n",
      "env: WANDB_SILENT=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/fishmon/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_WATCH=all\n",
    "%env WANDB_SILENT=true\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3b1d11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fishmon/AJ/LLM-Finetuning/Malicious-Web/wandb/run-20250504_024412-8p2km39j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jordinia-netpro/netpro-finetune/runs/8p2km39j' target=\"_blank\">llama-3.2-3b-instruct-unsloth-sft-2025-05-04</a></strong> to <a href='https://wandb.ai/jordinia-netpro/netpro-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jordinia-netpro/netpro-finetune' target=\"_blank\">https://wandb.ai/jordinia-netpro/netpro-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jordinia-netpro/netpro-finetune/runs/8p2km39j' target=\"_blank\">https://wandb.ai/jordinia-netpro/netpro-finetune/runs/8p2km39j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to WANDB!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers.utils import logging\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Loads from .env file automatically\n",
    "\n",
    "# 2. Verify token loading\n",
    "if not os.getenv(\"WANDB_API_KEY\"):\n",
    "    raise ValueError(\"WANDB_API_KEY not found in .env file\")\n",
    "\n",
    "# 3. Initialize and upload\n",
    "os.environ[\"WANDB_API_KEY\"]=os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "logging.set_verbosity_info()\n",
    "project_name = \"netpro-finetune\" \n",
    "entity_name = \"jordinia-netpro\"\n",
    "run_name = \"llama-3.2-3b-instruct-unsloth-sft-2025-05-04\" # Set your desired run name\n",
    "\n",
    "# Initialize WANDB (FIXED ENTITY/PROJECT)\n",
    "try:\n",
    "    run = wandb.init(\n",
    "        entity=entity_name,\n",
    "        project=project_name,\n",
    "        name=run_name,\n",
    "        # id=\"j4vh49mi\",\n",
    "        # resume=\"allow\" # Uncomment to resume a previous run\n",
    "    )\n",
    "    print(\"Successfully connected to WANDB!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize WANDB: {str(e)}\")\n",
    "    # Consider exiting if WANDB is critical\n",
    "    # sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "052ed88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "PyTorch: setting up devices\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir                  =config.get('output_dir', 'outputs'),\n",
    "    per_device_train_batch_size =config.get('per_device_train_batch_size', 2),\n",
    "    per_device_eval_batch_size  =config.get('per_device_eval_batch_size', 1),\n",
    "    gradient_accumulation_steps =config.get('gradient_accumulation_steps', 4),\n",
    "    warmup_ratio                =config.get('warmup_ratio', 0.05),\n",
    "    max_steps                   =config.get('max_steps', 4125),\n",
    "    learning_rate               =2e-5,\n",
    "    fp16                        =not is_bfloat16_supported(),\n",
    "    bf16                        =is_bfloat16_supported(),\n",
    "    optim                       =config.get('optim', 'adamw_8bit'),\n",
    "    weight_decay                =config.get('weight_decay', 0.1),\n",
    "    lr_scheduler_type           =config.get('lr_scheduler_type', 'cosine'),\n",
    "    eval_strategy               =config.get('eval_strategy', 'steps'),\n",
    "    eval_steps                  =config.get('eval_steps', 50),\n",
    "    save_strategy               =config.get('save_strategy', 'steps'),\n",
    "    save_steps                  =config.get('save_steps', 100),\n",
    "    save_total_limit            =config.get('save_total_limit', 3),\n",
    "    logging_steps               =config.get('logging_steps', 10),\n",
    "    seed                        =config.get('seed', 3407),\n",
    "    report_to                   =config.get('report_to', 'wandb'),\n",
    "    load_best_model_at_end      =config.get('load_best_model_at_end', True),\n",
    "    metric_for_best_model       =config.get('metric_for_best_model', 'eval_loss'),\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model               =model,\n",
    "    tokenizer           =tokenizer,\n",
    "    train_dataset       =dataset_dict[\"train\"],\n",
    "    eval_dataset        =dataset_dict[\"test\"],\n",
    "    dataset_text_field  =config.get('dataset_text_field', 'text'),\n",
    "    max_seq_length      =config.get('max_seq_length', 30000),\n",
    "    data_collator       =DataCollatorForSeq2Seq(\n",
    "        tokenizer               =tokenizer,\n",
    "        padding                 =config.get('data_collator', {}).get('padding', True),\n",
    "        pad_to_multiple_of      =config.get('data_collator', {}).get('pad_to_multiple_of', 8),\n",
    "        max_length              =config.get('max_seq_length', 30000),\n",
    "    ),\n",
    "    dataset_num_proc    =config.get('dataset_num_proc', 2),\n",
    "    packing             =config.get('packing', False),\n",
    "    args                =args,\n",
    ")\n",
    "\n",
    "# # Calculate training length (33k samples)\n",
    "# total_samples = 33000  # Your balanced dataset size\n",
    "# batch_size = 2 * 4  # batch_size * gradient_accum\n",
    "# steps_per_epoch = total_samples // batch_size  # ~4125 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cc344",
   "metadata": {},
   "source": [
    "### Train on Completions Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07252b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_on_responses_only_custom(trainer):\n",
    "#     \"\"\"\n",
    "#     Custom version for website classification that:\n",
    "#     1. Preserves system prompt context\n",
    "#     2. Only trains on assistant responses (JSON + reasoning)\n",
    "#     3. Handles length consistency for batching\n",
    "#     \"\"\"\n",
    "#     tokenizer = trainer.tokenizer\n",
    "    \n",
    "#     # Manually define token sequences for your template\n",
    "#     SYSTEM_TOKENS = tokenizer.encode(\n",
    "#         \"<|start_header_id|>system<|end_header_id|>\\n\\n\", \n",
    "#         add_special_tokens=False\n",
    "#     )\n",
    "#     USER_TOKENS = tokenizer.encode(\n",
    "#         \"<|start_header_id|>user<|end_header_id|>\\n\\n\", \n",
    "#         add_special_tokens=False\n",
    "#     )\n",
    "#     ASSISTANT_TOKENS = tokenizer.encode(\n",
    "#         \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \n",
    "#         add_special_tokens=False\n",
    "#     )\n",
    "\n",
    "#     def custom_masking(examples):\n",
    "#         input_ids = examples[\"input_ids\"]\n",
    "        \n",
    "#         # Create labels if they don't exist\n",
    "#         labels = examples.get(\"labels\", [ids.copy() for ids in input_ids])\n",
    "#         new_labels = []\n",
    "        \n",
    "#         for seq_id, seq_labels in zip(input_ids, labels):\n",
    "#             n = len(seq_id)\n",
    "#             mask = [-100] * n\n",
    "#             i = 0\n",
    "            \n",
    "#             while i < n:\n",
    "#                 # Check for system prompt\n",
    "#                 if seq_id[i:i+len(SYSTEM_TOKENS)] == SYSTEM_TOKENS:\n",
    "#                     i += len(SYSTEM_TOKENS)\n",
    "#                     continue\n",
    "                    \n",
    "#                 # Check for user prompt\n",
    "#                 if seq_id[i:i+len(USER_TOKENS)] == USER_TOKENS:\n",
    "#                     i += len(USER_TOKENS)\n",
    "#                     continue\n",
    "                    \n",
    "#                 # Check for assistant prompt\n",
    "#                 if seq_id[i:i+len(ASSISTANT_TOKENS)] == ASSISTANT_TOKENS:\n",
    "#                     start = i\n",
    "#                     i += len(ASSISTANT_TOKENS)\n",
    "                    \n",
    "#                     # Find end of assistant response\n",
    "#                     while i < n:\n",
    "#                         if seq_id[i] == tokenizer.eos_token_id:\n",
    "#                             end = i\n",
    "#                             break\n",
    "#                         i += 1\n",
    "#                     else:\n",
    "#                         end = n\n",
    "                    \n",
    "#                     # Unmask assistant response\n",
    "#                     mask[start:end] = seq_labels[start:end]\n",
    "#                     break\n",
    "                    \n",
    "#                 i += 1\n",
    "            \n",
    "#             # Enforce length matching (critical fix)\n",
    "#             if len(mask) != len(seq_id):\n",
    "#                 mask = mask[:len(seq_id)] + [-100] * (len(seq_id) - len(mask))\n",
    "            \n",
    "#             new_labels.append(mask)\n",
    "        \n",
    "#         return {\"labels\": new_labels}\n",
    "\n",
    "#     # Apply to datasets with length verification\n",
    "#     def apply_masking(dataset):\n",
    "#         dataset = dataset.map(\n",
    "#             custom_masking,\n",
    "#             batched=True,\n",
    "#             batch_size=1000,\n",
    "#             num_proc=4,\n",
    "#         )\n",
    "#         # Verify lengths\n",
    "#         for i in range(min(3, len(dataset))):\n",
    "#             assert len(dataset[i][\"input_ids\"]) == len(dataset[i][\"labels\"]), \\\n",
    "#                 f\"Length mismatch in sample {i}\"\n",
    "#         return dataset\n",
    "    \n",
    "#     trainer.train_dataset = apply_masking(trainer.train_dataset)\n",
    "    \n",
    "#     if trainer.eval_dataset is not None:\n",
    "#         trainer.eval_dataset = apply_masking(trainer.eval_dataset)\n",
    "        \n",
    "#     return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9499b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = train_on_responses_only_custom(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96e2c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final verification (optional)\n",
    "# print(\"\\n=== Final Pre-Train Check ===\")\n",
    "# sample = trainer.train_dataset[0]\n",
    "# print(\"First sample labels preview:\")\n",
    "# print(\"Input IDs length:\", len(sample[\"input_ids\"])) \n",
    "# print(\"Labels length:\", len(sample[\"labels\"]))\n",
    "# print(\"Last 10 labels:\", sample[\"labels\"][-10:])  # Should show unmasked assistant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6841e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a sample from the training set\n",
    "# sample_index = 5  # Try different indices\n",
    "# sample = trainer.train_dataset[sample_index]\n",
    "\n",
    "# # Decode the full input context\n",
    "# print(\"==== Full Input Context ====\")\n",
    "# print(tokenizer.decode(sample[\"input_ids\"]))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Decode the labels with masking visualization\n",
    "# print(\"==== Training Targets (Masked) ====\")\n",
    "# masked_labels = []\n",
    "# for token_id, label_id in zip(sample[\"input_ids\"], sample[\"labels\"]):\n",
    "#     if label_id == -100:\n",
    "#         # Show masked tokens as blank spaces\n",
    "#         masked_labels.append(\" \")\n",
    "#     else:\n",
    "#         # Show actual token\n",
    "#         masked_labels.append(tokenizer.decode([token_id]))\n",
    "\n",
    "# print(\"\".join(masked_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "136a307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = trainer.train_dataset.select(range(2)).with_format(\"torch\")[0]\n",
    "# print(batch[\"input_ids\"])\n",
    "# print(batch[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221f500",
   "metadata": {},
   "source": [
    "### Memory Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7c95376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 33128\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b14b138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'You are an Expert Website Classifier tasked with categorizing websites (using provided Domain & Content) into distinct categories: **0 - Benign**, **1 - Gambling**, **2 - Pornography**, or **3 - Harmful**. Output strict JSON including classification, reason, and a point-based confidence score (0-100). This aids a sophisticated website prediction system for digital safety.\\n\\n### **Categories & Definitions:**\\n\\n-   **0 - Benign:** General info, news, entertainment, services, e-commerce, educational sites, blogs, informational pages, and general entertainment sites. No gambling/porn themes.\\n    -   **Includes:** Educational discussion of sensitive topics (drugs, gambling, adult, illegal), suggestive content (dating, lingerie) *without* explicit material, drugs for educational or news purposes (e.g., addiction recovery, medical cannabis research).\\n\\n-   **1 - Gambling:** Promotes/facilitates betting, casino, poker, lottery, wagering. Includes online betting platforms, casinos, poker sites, lottery sites, and any website that encourages or provides means for users to wager money on games of chance or skill.\\n    -   **Keywords:** betting, casino, poker, odds, jackpot, slots, roulette, judi, slot gacor, togel, similar gambling-related terminology within their domain name or content.\\n    \\n-   **2 - Pornography:** These are websites that contain explicit sexual content intended to cause arousal. Explicit sexual content (images, videos, text) for arousal. Immoral content (bestiality, child exploitation). or links to such materials. \\n    -   **Keywords:** porn, bokep, sex, xxx, adult, nude, erotic, explicit video/photo.\\n\\n- **3 - Harmful**: Websites that does not fall to Benign, Gambling, and Pornography category. These websites engage in or promote activities harmful to users or violating laws/regulations, including:\\n    - Malware Distribution: Hosting/downloading computer viruses, worms, ransomware, spyware, etc.\\n    - Cybercrime: Phishing kits, hacking tools, stolen data markets, carding forums.\\n    - Extremism & Terrorism: Content inciting violence, extremist ideologies, or terrorist recruitment.\\n    - Violations of Indonesian Law:\\n        - Insults, defamation, blackmail, or threats.\\n        - Hoaxes/misleading news, hate speech, or incitement of violence.\\n    - Copyright Infringement/Piracy: Illegal software/cracks, torrents, pirated media.\\n    - Drugs/Narcotics: Sale/promotion of illegal drugs (e.g., cocaine, heroin) or unregulated pharmaceuticals.\\n    - Weapons: Sale of illegal firearms, explosives, or weapons.\\n    - Other Illegal Activities: Counterfeit goods, money laundering, human trafficking.\\n    - **Examples:**  \\n        - Illegal: `darknet-drugs.com` (drug sales), `pirated-movies.id` (piracy).  \\n        - Harmful: `extremist-forum.net` (terrorism recruitment), `hackers-tools.org` (phishing kits).  \\n        - Scam/phishng: `hadiah-telkomsel7.blogspot.com` (non-genuine website).\\n    - **Keywords**:\\nmalware, ransomware, phishing, hack, terrorism, jual narkoba, senjata ilegal, konten SARA, berita bohong, ancaman, pembajakan, cracked software, carding, darknet.\\n    - **Exceptions:** \\n        - Licensed/unlicensed gambling â†’ **1 - Gambling**; scams â†’ **3 - Harmful**.\\n\\n\\n### **Input Data Context:**\\n\\nYou will be provided with data entries, each consisting of two primary fields:\\n\\n*   **Domain:** The domain name or URL of the website (e.g., `example.com`, `gamble-site.net`). This can provide hints about the website\\'s purpose.\\n*   **Content:**  The textual content scraped from the website. This content offers detailed information about the website\\'s topics, services, and themes.\\n\\n### **Labeling Instructions:**\\n\\nAnalyze both the **Domain** and the **Content** provided. Use keywords and contextual clues from both to determine the most appropriate category for the website.  Consider the primary purpose and content focus of the website when classifying.\\n\\n### **Confidence Assessment Guidelines: Point-Based System (Total Possible Points: 100)**\\n\\nTo determine the **confidence** level (0-100) for your classification, evaluate the following factors and sum up the points.  The total points will directly correspond to the confidence percentage (e.g., 95 points = 95% confidence).\\n\\n#### **I. Keyword Strength and Relevance (Maximum 40 Points)**\\n*   **(40 Points):  Exceptional Keyword Strength: Explicit and Overwhelming Keywords in BOTH Domain and Content:** Presence of extremely explicit and overwhelmingly strong keywords that are *unquestionably* indicative of a specific category in *both* the domain name AND the website content. These keywords leave absolutely no doubt about the website\\'s nature. (e.g., Domain: `casino-royal-betting.com`, Content:  \"Gamble now and win HUGE jackpots on slots, poker, roulette! Real money betting!\").  This represents the absolute strongest keyword signal possible.\\n*   **(35 Points): Clear and Strong Keywords in BOTH Domain and Content:**  Presence of highly explicit keywords strongly indicative of a specific category in both the domain name AND the website content. (e.g., Domain: `bet.com`, Content:  \"Bet on sports and casino games!\").\\n*   **(25 Points): Strong Keywords in EITHER Domain OR Content:** Presence of highly explicit keywords strongly indicative of a specific category in EITHER the domain name OR the website content, but not both.\\n*   **(15 Points): Some Relevant Keywords:** Presence of keywords related to a category, but they are less explicit, less frequent, or require more contextual interpretation in either domain or content.\\n*   **(0 Points): Weak or Generic Keywords:** Lack of clear category-specific keywords in both domain and content. Keywords are generic and do not strongly suggest any specific category.\\n\\n#### **II. Domain and Content Alignment (Maximum 30 Points)**\\n\\n*   **(30 Points): Strong Domain and Content Alignment:** Domain name strongly and unambiguously suggests a category, and the website content consistently and explicitly reinforces that category.  They tell the same clear story.\\n*   **(15 Points): Partial Domain and Content Alignment:** Domain name and content generally point towards the same category, but the alignment might be less direct, slightly ambiguous, or require some interpretation to connect them.\\n*   **(0 Points): Domain-Content Mismatch or No Alignment:** Domain name suggests one thing, but the content is unclear, suggests something different, or there\\'s no clear connection between the domain and the content\\'s apparent purpose.\\n\\n#### **III. Content Clarity and Unambiguity (Maximum 20 Points)**\\n\\n*   **(20 Points): Unambiguous and Explicit Content:** The website content is very clear, direct, and leaves virtually no room for interpretation. It unambiguously falls into one of the defined categories.\\n*   **(10 Points): Content Requires Some Interpretation:** The content generally points to a category, but requires some interpretation to confidently assign it.  There might be subtle hints, implied meanings, or a need to infer the primary purpose.\\n*   **(0 Points): Ambiguous or Conflicting Content:** The website content is vague, contradictory, or could be reasonably interpreted in multiple ways, making it difficult to confidently assign a category.\\n\\n#### **IV. Category Indicator Strength (Maximum 10 Points)**\\n\\n*   **(10 Points): Multiple Strong Category Indicators:** Presence of numerous strong and clear indicators for a specific category throughout the domain and content (e.g., for Pornography: explicit keywords, descriptions of sexual acts, calls to action to view adult content, age verification prompts).\\n*   **(5 Points): Some Category Indicators Present:** Presence of a few indicators for a category, but they are not overwhelmingly strong or numerous.\\n*   **(0 Points): Lack of Category Indicators:** Few or no clear indicators for any of the defined categories are present in the domain and content.\\n\\n#### **Calculation:**\\n\\n1.  For each of the four sections (I-IV), assess the website and select the point value that best describes the presence of the described factors.\\n2.  Sum up the points from all four sections.\\n3.  The total sum represents the confidence level in percentage (%).\\n\\n#### **Confidence Level Ranges (for reference - already implicitly defined by points):**\\n\\n*   **High Confidence (80-100 Points):**  Strong evidence across multiple factors pointing clearly to a category.\\n*   **Medium Confidence (50-79 Points):** Moderate evidence, some ambiguity or less directness in indicators.\\n*   **Low Confidence (0-49 Points):** Weak or conflicting evidence, high uncertainty about the correct category.\\n\\n#### **Example:**\\n\\nLet\\'s say you are classifying `lucky-slots-online.com` with content about slot games and bonuses.\\n\\n*   **I. Keyword Strength:** Strong keywords in both Domain and Content (e.g., \"slots,\" \"online,\" \"win,\" \"bonuses\") - **25 Points**\\n*   **II. Domain-Content Alignment:** Domain and content strongly align with Gambling - **30 Points**\\n*   **III. Content Clarity:** Content is very clear about gambling activities - **20 Points**\\n*   **IV. Category Indicators:** Multiple indicators of gambling (games, bonuses, calls to action) - **10 Points**\\n\\n**Total Points: 25 + 30 + 20 + 10 = 85 Points.  Confidence: 85%**\\n\\n#### **Using this Point System:**\\n\\nWhen generating the \"reason\" for your classification, you can now also briefly mention the points you assigned for each section to justify the final confidence score. For example:\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Strong keywords in domain and content (25 pts), strong domain-content alignment (30 pts), clear gambling content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n\\n### **Output Requirements: STRICTLY ADHERE TO JSON FORMAT**\\n\\nFor each website, you **MUST** provide your classification in the following **JSON format ONLY**.  **No other format is acceptable.**\\n\\n```json\\n{\\n  \"answer\": (integer: 0, 1, 2, or 3),\\n  \"classification\": (string: \"Benign\", \"Gambling\", \"Pornography\", or \"Harmful\"),\\n  \"reason\": (string: \"Brief explanation of the classification using keywords and hints from the domain and content.\"),\\n  \"confidence\": (integer: 0 to 100, \"Level of confidence in the classification\")\\n}\\n```\\n\\n**Example Input and Expected Output:**\\n\\n**1st Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: news-today.info\\nContent: Welcome to News Today! Get the latest breaking news, top stories, and in-depth analysis from around the world. Covering politics, business, technology, sports, and culture. Stay informed with News Today - your source for reliable journalism.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 0,\\n  \"classification\": \"Benign\",\\n  \"reason\": \"Domain \\'news-today.info\\' and content mention \\'breaking news,\\' \\'top stories,\\' \\'analysis,\\' \\'reliable journalism,\\' indicating a general information/news website. Strong keywords in domain and content (35 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), some category indicators (5 pts). Total 90 points.\",\\n  \"confidence\": 90\\n}\\n```\\n\\n**2nd Sample:**\\n\\n**Input Data:**\\n\\n```\\nDomain: lucky-slots-online.com\\nContent:  Spin to win big at Lucky Slots Online! Play hundreds of exciting slot games, claim your bonuses, and join the fun.  Licensed and regulated for your safety. 24/7 customer support available. Join now and get 100 free spins!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Domain \\'lucky-slots-online.com\\' and content include keywords like \\'slots,\\' \\'win,\\' \\'casino,\\' \\'bonuses,\\' \\'free spins,\\' indicating a gambling website.\",\\n  \"confidence\": 100\\n}\\n```\\n\\n**3rd Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: adult-pleasures.net\\nContent:  Experience the hottest adult entertainment online. Unlimited access to exclusive videos and photos. 18+ only. Join our community of pleasure seekers today!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 2,\\n  \"classification\": \"Pornography\",\\n  \"reason\": \"Domain \\'adult-pleasures.net\\' and content include explicit terms like \\'adult entertainment,\\' \\'exclusive videos,\\' \\'photos,\\' \\'18+ only,\\' and \\'pleasure seekers,\\' indicating a pornography website. Strong keywords in both domain and content (25 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n**4th Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: project420.com\\nContent:  Welcome to Project420.com an ongoing community marijuana, hemp search engine and your gateway to 420 products, services and information from all over the world. Marijuana seeds, super bongs, high tech growing marijuana books, contests, communities, cool people and tons of 420 fun.  Potent marijuana websites are added frequently.  Let\\'s help each other grow.  Use the table of contents located to the left and begin your search of 420 world wide websites.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 3,\\n  \"classification\": \"Harmful\",\\n  \"reason\": \"Domain \\'project420.com\\' and content include keywords like \\'marijuana seeds,\\' \\'growing marijuana,\\' and \\'420 products,\\' indicating promotion of illegal drugs under Indonesian law (UU ITE). Strong keywords in content (25 pts), Partial Domain and Content Alignment (15 pts), clear harmful intent (20 pts), multiple indicators (10 pts). Total 70 points.\",\\n  \"confidence\": 70\\n}\\n```\\n**Execution:**\\nProcess the provided website data (input fields: `Domain`, `Content`). Apply the classification categories, definitions, and confidence point system accurately. Generate the mandatory JSON output for each entry. Begin classification now.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n\\nDomain: adevotedslave.blogspot.com, Content: \"Delicious Debauchery If You arenâ€™t over the age 18 (21 in some areas) then please leave. If You are offended by sex, BDSM, phone sex or brutal honesty then this isn\\'t the place for You. FRIDAY, JUNE 08, 2007 Why Iâ€™ll never be a full lesbianâ€¦ You know, it doesnâ€™t seem to matter what I am using or what position I am in. Iâ€™ve tried it with dildos, vibes, my gear shift knob, my fingers, and miscellaneous household objects, with just about anything and every thing I can think of. Iâ€™ve done it on my hands and knees on my back, with one leg up. Iâ€™ve tried putting the base on the floor and bouncing up and down. Iâ€™ve put it up against a wall and pushed against it. It doesnâ€™t matter. Nothing that I do makes it so that I can fuck myself hard enough to make it feel just right. Nothing can take me to that place where I am balancing between the pleasure of being fucked and the pain of having a cock battering my insides. Of course, when Iâ€™m trying to simulate hard fucking it still feels good, but itâ€™s that kind of feeling you get when you are almost there, but not quite. You know where you want to go but you just canâ€™t get there. Itâ€™s frustrating as hell. Besides, I have never actually had a toy, or object Iâ€™ve converted into a toy, thatâ€™s felt like a real cock anyway. They all just feel like random objects that fall short of the glory that is a real, hard, throbbing, piece of flesh designed for fucking. Of course, if thereâ€™s some woman out there whoâ€™s willing to try to convert me into a full fledged lesbian anyway, Iâ€™m more than willing to submit to that kind of training. Who knows? A woman with a strap on may be able to fuck me just like I like. Of course, Iâ€™d not refuse returning the favor. It could be a mutually beneficial experiment. POSTED BY DEIDRE AT 11:13 AM 2 COMMENTS SATURDAY, APRIL 28, 2007 Bruised and Battered For some reason I think that the depth of my pussy fluctuates. Sometimes when Masterâ€™s cock slides inside of me it is the perfect fit. The head hits in just the... en melted apart. A puff of smoke flew up, and there was another familiar sensation... the smell of burned meat. Again the heat, with a dirty orange light where the flame burned away the bits of tissue stuck to the wire. Again the application, the shudder, the pop, the hiss, the smoke. Around the stencil he went, carefully watching the brand, lining up the strikes. As the skin opens along the brand\\'s outline, it shifts. About halfway around, he started almost freehanding, using the stencil as a reminder of what he wanted, not as a guide. Then with the outline complete, and even, he used its angles as a reference for the final strikes down the middle. There were ten strikes in all. Ten times the iron glowed, ten times it lowered to her helpless flesh. Ten times she moaned as her love for me was burned into her very body. As I continued my research I rediscovered Fakirâ€™s site and the process of using an electro-cautery pencil instead of using the striking method with heated ribbons of steel. I showed a few examples to Master, including a photo of hisdevotionâ€™s back. He asked how one goes about getting an electro-cautery pencil. Until that moment it never even occurred to me that a person outside of the medical profession could just go and buy one. Well, for less than a hundred bucks one can buy a battery operated one with various different tips. Something wonderful happened as I scrolled through all of the options for this tool. I began reading in the descriptions that they get up to 2200 degrees Fahrenheit. Instead of the fear I expected to feel at such a number I felt the nervous flutters of excitement. If Master decides to go this direction, and have His mark of ownership burned into my skin in this fashion, I will have 2200 degrees focused on my skin, burning me, marking me bit by bit. This makes me incredibly aroused. Perhaps I have the makings of a pain slut afterall. POSTED BY DEIDRE AT 1:43 PM 0 COMMENTS TUESDAY, MARCH 13, 2007 masks Iâ€™ve said before that my relationship with Master and the trust and honesty that is the foundation of that relationship has made me more fully myself than I ever have been before. I am more aware of the different aspects of myself and I am able to lie them all out on the table to be poked at and prodded by any stranger who happens by. On the other hand there are those for whom I must wear certain masks. My relationship with Master might have inspired an intense honesty with myself and with Him, as well as with those who can accept us this way and those whose judgement doesnâ€™t matter, but itâ€™s also forced me to be more dishonest than ever to those around me, my neighbors, friends and family. I have been asked by potential sisters, by friends and by callers about how the world sees me. What does Master let the neighbors see? His friends? My friends? My family? His family? Itâ€™s all kind of complicated. The neighbors/His friends: * My neighbors see a 28 year old girl who is living with a 53 year old man. They see the 2 of us happily interacting with each other. He is very active in our neighborhood community. I am not. I occasionally wave to the neighbors or have a conversation with them and most of the time they say something to the effect of, \"Itâ€™s been a while. How have you been?\" Most of them are nice people, but I have little desire to interact with them. They know that I am a phone sex operator and most of them are intrigued. They do not know that I am a slave but there are probably quite a few who have observed that I donâ€™t make any decisions of importance. My friends: * My friends all know just about everything there is to know. Friends that I consider to be mine (as opposed to those that I know because of Master) are basically those from the internet. This includes a few people from collarme and other personals sites but the vast majority is made up of people from LiveJournal. LiveJournal is where all of my meaningful social interaction takes place, as sad as that may seem. There just arenâ€™t a whole lot of people like me nearby and being friends with a girl whoâ€™s first priority is never friendships can be trying. His family: * Masterâ€™s brother, K. He is aware of the basics of the dynamics between Master and myself however itâ€™s never discussed with him and itâ€™s never thrown in his face. We donâ€™t play or fuck in front of K. Iâ€™ve never been loaned to K. Master and I have discussed it and if K is anything heâ€™s probably more submissive than dominant. K knows that I am a phone sex operator. I think he may have heard me on occasion when I am taking calls. * The rest of Masterâ€™s family. He has one of those families that has reunions every year. The patriarch of the family is in his 90s and still kicking. I havenâ€™t met everyone yet, but Iâ€™ve met the ones that are either closest or that Master likes the most. They have opened their arms and accepted me, making me an honorary â€“insert Masterâ€™s family name here- One of them told me over thanksgiving that she has never seen Master as happy as he is now and she thinks that I am to thank for that. What higher compliment can someone pay me than that? My Family: * My sister, E, and I used to be very close. Weâ€™re not anymore though. When I went to her house last summer I had just come from my fathers and wasnâ€™t wearing my collar. The first night I was getting ready to go out with her and my mother and put it back on. Her reaction was that my necklace didnâ€™t match my outfit and I should take the jewelry off. I refused explaining that Master (I used his first name) gave it to me and I hadnâ€™t really had it off since except for when I visited dad. I think she started to get the idea at that point. She knew a little before then, but I think the image of me collared finished off any of the blank parts in her imagination of how I was living my life. I donâ€™t think she approves. We havenâ€™t spoken since then. Iâ€™m not sure how much my lifestyle has to do with that though. * My mother knows the pieces and details that I think sh...  get enough. Sheâ€™s become my newest addiction. I lick the juice of her squirt from my arm and slide my fingers from her into my mouth. I love that taste. Outside the sun is beginning to rise. Weâ€™ve been lost in each other for the entire night. The magic begins to dissipate. We get dressed once again shy and nervous. The blush on my face doesnâ€™t stop me from telling her how much I enjoyed the night. It turns out that she did too and that we both want to do it again. Soon. POSTED BY DEIDRE AT 1:59 PM 0 COMMENTS Older Posts Home Subscribe to: Posts (Atom) ABOUT DEIDRE DEIDRE STRADDLING THE LINE BETWEEN FANTASY AND REALITY deidre is a 28 yo lifestyle slave, phone sex operator and fetish model. she is happily owned. VIEW MY COMPLETE PROFILE LINKS 101 Things about deidre NITEFLIRT delicious debauchery Join for Free What is NiteFlirt? MP3 RECORDINGS The Holy Fuck **fetish sample Gender Blender **hypnosis sample How to Get Out of a Ticket **fetish sample Gym Discipline **fetish sample French Maid **fetish sample Lesbian Strap-on BDSM **fetish sample Lesbian Tickle Bondage **fetish sample Isabella\\'s Sex Slave **hypnosis sample Gagged By Goddess **fetish sample Boarding School Humiliation **fetish sample RECORDINGS (BY THE MINUTE) rates and descriptions deidre\\'s daily prayer deidre gets face fucked equipment sampler deidre\\'s discipline barking cough tickle torture psalm 23 (deidre style) a steamy shower & my fav toy PHOTO SETS rates and descriptions teddy bears & coloring books deidre\\'s puppy training photo sampler bondage the golden set deidre\\'s naughty picnic deidre in distress foot bondage PODCASTS Session 2 Interview with Miss Isabella Session 1 WISHFUL THINKING amazon wishlist deidre\\'s corset fund LABELS questions (13) NiteFlirt (12) mp3 (11) calls (6) gifts (5) music (5) pics (2) podcast (2) meme (1) quotes (1) BLOG ARCHIVE â–¼ 2007 (41) â–¼ JUNE (1) â–¼ JUN 08 (1) WHY Iâ€™LL NEVER BE A FULL LESBIANâ€¦ â–º APRIL (2) â–º MARCH (6) â–º FEBRUARY (11) â–º JANUARY (21) â–º 2006 (117) â–º 1978 (2)\"',\n",
       "   'role': 'user'},\n",
       "  {'content': '<think>\\nOkay, I need to classify this website into one of the four categories: Benign, Gambling, Pornography, or Harmful. Let\\'s start by looking at the domain and content provided.  The domain is adevotedslave.blogspot. com. The word \"slavery\" in the domain could be a red flag because it might relate to harmful activities like human trafficking. But I should also consider if it\\'s used in a different context, such as BDSM or kink communities which are more about consensual adult content.  Looking at the content, there are several posts discussing BDSM practices, phone sex, and explicit sexual scenarios. For example, one post talks about trying various objects for self-pleasure and another describes a session involving masks and domination. There\\'s also mention of being a \"lifestyle slave\" and a \"phone sex operator,\" which suggests the site is focused on adult content.  The content includes terms like \"BDSM,\" \"fetish,\" \"bondage,\" and explicit descriptions of sexual acts. These keywords are strong indicators of Pornography because they relate to adult themes and explicit material intended for arousal.  However, I should check if it falls under Harmful instead. The website doesn\\'t seem to promote illegal activities like drug sales or cybercrime. It\\'s more about consensual adult content, which would typically fall under Pornography rather than Harmful unless there are elements of non-consent or exploitation, which aren\\'t clearly present here.  The domain and content align well with Pornography because the focus is on explicit sexual content and fetishes. There are no strong indicators pointing towards Gambling or Benign categories. The content is unambiguous in its adult themes, so it doesn\\'t seem to be a case of harmful activities outside the scope of Pornography.  In terms of confidence assessment: - Keyword Strength: Strong keywords like \"fetish,\" \"bondage,\" and explicit sexual descriptions are present both in the domain and content. - Domain-Content Alignment: The domain suggests a BDSM or slavery theme, which is reinforced by the content discussing explicit adult scenarios. - Content Clarity: The content is very clear about its focus on adult entertainment and fetishes without ambiguity. - Category Indicators: Multiple indicators such as explicit sexual descriptions, use of terms like \"phone sex operator,\" and detailed accounts of BDSM practices.  Based on these factors, I\\'m confident that this website falls under the Pornography category with a high confidence score.\\n</think>\\n```json\\n{\\n  \"answer\": 2,\\n  \"classification\": \"Pornography\",\\n  \"reason\": \"The domain \\'adevotedslave. blogspot.com\\' and content include explicit terms like \\'BDSM,\\' \\'fetish,\\' \\'bondage,\\' and detailed descriptions of sexual acts, indicating a focus on adult entertainment. Strong keywords in both domain and content (25 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```',\n",
       "   'role': 'assistant'}],\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are an Expert Website Classifier tasked with categorizing websites (using provided Domain & Content) into distinct categories: **0 - Benign**, **1 - Gambling**, **2 - Pornography**, or **3 - Harmful**. Output strict JSON including classification, reason, and a point-based confidence score (0-100). This aids a sophisticated website prediction system for digital safety.\\n\\n### **Categories & Definitions:**\\n\\n-   **0 - Benign:** General info, news, entertainment, services, e-commerce, educational sites, blogs, informational pages, and general entertainment sites. No gambling/porn themes.\\n    -   **Includes:** Educational discussion of sensitive topics (drugs, gambling, adult, illegal), suggestive content (dating, lingerie) *without* explicit material, drugs for educational or news purposes (e.g., addiction recovery, medical cannabis research).\\n\\n-   **1 - Gambling:** Promotes/facilitates betting, casino, poker, lottery, wagering. Includes online betting platforms, casinos, poker sites, lottery sites, and any website that encourages or provides means for users to wager money on games of chance or skill.\\n    -   **Keywords:** betting, casino, poker, odds, jackpot, slots, roulette, judi, slot gacor, togel, similar gambling-related terminology within their domain name or content.\\n    \\n-   **2 - Pornography:** These are websites that contain explicit sexual content intended to cause arousal. Explicit sexual content (images, videos, text) for arousal. Immoral content (bestiality, child exploitation). or links to such materials. \\n    -   **Keywords:** porn, bokep, sex, xxx, adult, nude, erotic, explicit video/photo.\\n\\n- **3 - Harmful**: Websites that does not fall to Benign, Gambling, and Pornography category. These websites engage in or promote activities harmful to users or violating laws/regulations, including:\\n    - Malware Distribution: Hosting/downloading computer viruses, worms, ransomware, spyware, etc.\\n    - Cybercrime: Phishing kits, hacking tools, stolen data markets, carding forums.\\n    - Extremism & Terrorism: Content inciting violence, extremist ideologies, or terrorist recruitment.\\n    - Violations of Indonesian Law:\\n        - Insults, defamation, blackmail, or threats.\\n        - Hoaxes/misleading news, hate speech, or incitement of violence.\\n    - Copyright Infringement/Piracy: Illegal software/cracks, torrents, pirated media.\\n    - Drugs/Narcotics: Sale/promotion of illegal drugs (e.g., cocaine, heroin) or unregulated pharmaceuticals.\\n    - Weapons: Sale of illegal firearms, explosives, or weapons.\\n    - Other Illegal Activities: Counterfeit goods, money laundering, human trafficking.\\n    - **Examples:**  \\n        - Illegal: `darknet-drugs.com` (drug sales), `pirated-movies.id` (piracy).  \\n        - Harmful: `extremist-forum.net` (terrorism recruitment), `hackers-tools.org` (phishing kits).  \\n        - Scam/phishng: `hadiah-telkomsel7.blogspot.com` (non-genuine website).\\n    - **Keywords**:\\nmalware, ransomware, phishing, hack, terrorism, jual narkoba, senjata ilegal, konten SARA, berita bohong, ancaman, pembajakan, cracked software, carding, darknet.\\n    - **Exceptions:** \\n        - Licensed/unlicensed gambling â†’ **1 - Gambling**; scams â†’ **3 - Harmful**.\\n\\n\\n### **Input Data Context:**\\n\\nYou will be provided with data entries, each consisting of two primary fields:\\n\\n*   **Domain:** The domain name or URL of the website (e.g., `example.com`, `gamble-site.net`). This can provide hints about the website\\'s purpose.\\n*   **Content:**  The textual content scraped from the website. This content offers detailed information about the website\\'s topics, services, and themes.\\n\\n### **Labeling Instructions:**\\n\\nAnalyze both the **Domain** and the **Content** provided. Use keywords and contextual clues from both to determine the most appropriate category for the website.  Consider the primary purpose and content focus of the website when classifying.\\n\\n### **Confidence Assessment Guidelines: Point-Based System (Total Possible Points: 100)**\\n\\nTo determine the **confidence** level (0-100) for your classification, evaluate the following factors and sum up the points.  The total points will directly correspond to the confidence percentage (e.g., 95 points = 95% confidence).\\n\\n#### **I. Keyword Strength and Relevance (Maximum 40 Points)**\\n*   **(40 Points):  Exceptional Keyword Strength: Explicit and Overwhelming Keywords in BOTH Domain and Content:** Presence of extremely explicit and overwhelmingly strong keywords that are *unquestionably* indicative of a specific category in *both* the domain name AND the website content. These keywords leave absolutely no doubt about the website\\'s nature. (e.g., Domain: `casino-royal-betting.com`, Content:  \"Gamble now and win HUGE jackpots on slots, poker, roulette! Real money betting!\").  This represents the absolute strongest keyword signal possible.\\n*   **(35 Points): Clear and Strong Keywords in BOTH Domain and Content:**  Presence of highly explicit keywords strongly indicative of a specific category in both the domain name AND the website content. (e.g., Domain: `bet.com`, Content:  \"Bet on sports and casino games!\").\\n*   **(25 Points): Strong Keywords in EITHER Domain OR Content:** Presence of highly explicit keywords strongly indicative of a specific category in EITHER the domain name OR the website content, but not both.\\n*   **(15 Points): Some Relevant Keywords:** Presence of keywords related to a category, but they are less explicit, less frequent, or require more contextual interpretation in either domain or content.\\n*   **(0 Points): Weak or Generic Keywords:** Lack of clear category-specific keywords in both domain and content. Keywords are generic and do not strongly suggest any specific category.\\n\\n#### **II. Domain and Content Alignment (Maximum 30 Points)**\\n\\n*   **(30 Points): Strong Domain and Content Alignment:** Domain name strongly and unambiguously suggests a category, and the website content consistently and explicitly reinforces that category.  They tell the same clear story.\\n*   **(15 Points): Partial Domain and Content Alignment:** Domain name and content generally point towards the same category, but the alignment might be less direct, slightly ambiguous, or require some interpretation to connect them.\\n*   **(0 Points): Domain-Content Mismatch or No Alignment:** Domain name suggests one thing, but the content is unclear, suggests something different, or there\\'s no clear connection between the domain and the content\\'s apparent purpose.\\n\\n#### **III. Content Clarity and Unambiguity (Maximum 20 Points)**\\n\\n*   **(20 Points): Unambiguous and Explicit Content:** The website content is very clear, direct, and leaves virtually no room for interpretation. It unambiguously falls into one of the defined categories.\\n*   **(10 Points): Content Requires Some Interpretation:** The content generally points to a category, but requires some interpretation to confidently assign it.  There might be subtle hints, implied meanings, or a need to infer the primary purpose.\\n*   **(0 Points): Ambiguous or Conflicting Content:** The website content is vague, contradictory, or could be reasonably interpreted in multiple ways, making it difficult to confidently assign a category.\\n\\n#### **IV. Category Indicator Strength (Maximum 10 Points)**\\n\\n*   **(10 Points): Multiple Strong Category Indicators:** Presence of numerous strong and clear indicators for a specific category throughout the domain and content (e.g., for Pornography: explicit keywords, descriptions of sexual acts, calls to action to view adult content, age verification prompts).\\n*   **(5 Points): Some Category Indicators Present:** Presence of a few indicators for a category, but they are not overwhelmingly strong or numerous.\\n*   **(0 Points): Lack of Category Indicators:** Few or no clear indicators for any of the defined categories are present in the domain and content.\\n\\n#### **Calculation:**\\n\\n1.  For each of the four sections (I-IV), assess the website and select the point value that best describes the presence of the described factors.\\n2.  Sum up the points from all four sections.\\n3.  The total sum represents the confidence level in percentage (%).\\n\\n#### **Confidence Level Ranges (for reference - already implicitly defined by points):**\\n\\n*   **High Confidence (80-100 Points):**  Strong evidence across multiple factors pointing clearly to a category.\\n*   **Medium Confidence (50-79 Points):** Moderate evidence, some ambiguity or less directness in indicators.\\n*   **Low Confidence (0-49 Points):** Weak or conflicting evidence, high uncertainty about the correct category.\\n\\n#### **Example:**\\n\\nLet\\'s say you are classifying `lucky-slots-online.com` with content about slot games and bonuses.\\n\\n*   **I. Keyword Strength:** Strong keywords in both Domain and Content (e.g., \"slots,\" \"online,\" \"win,\" \"bonuses\") - **25 Points**\\n*   **II. Domain-Content Alignment:** Domain and content strongly align with Gambling - **30 Points**\\n*   **III. Content Clarity:** Content is very clear about gambling activities - **20 Points**\\n*   **IV. Category Indicators:** Multiple indicators of gambling (games, bonuses, calls to action) - **10 Points**\\n\\n**Total Points: 25 + 30 + 20 + 10 = 85 Points.  Confidence: 85%**\\n\\n#### **Using this Point System:**\\n\\nWhen generating the \"reason\" for your classification, you can now also briefly mention the points you assigned for each section to justify the final confidence score. For example:\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Strong keywords in domain and content (25 pts), strong domain-content alignment (30 pts), clear gambling content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n\\n### **Output Requirements: STRICTLY ADHERE TO JSON FORMAT**\\n\\nFor each website, you **MUST** provide your classification in the following **JSON format ONLY**.  **No other format is acceptable.**\\n\\n```json\\n{\\n  \"answer\": (integer: 0, 1, 2, or 3),\\n  \"classification\": (string: \"Benign\", \"Gambling\", \"Pornography\", or \"Harmful\"),\\n  \"reason\": (string: \"Brief explanation of the classification using keywords and hints from the domain and content.\"),\\n  \"confidence\": (integer: 0 to 100, \"Level of confidence in the classification\")\\n}\\n```\\n\\n**Example Input and Expected Output:**\\n\\n**1st Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: news-today.info\\nContent: Welcome to News Today! Get the latest breaking news, top stories, and in-depth analysis from around the world. Covering politics, business, technology, sports, and culture. Stay informed with News Today - your source for reliable journalism.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 0,\\n  \"classification\": \"Benign\",\\n  \"reason\": \"Domain \\'news-today.info\\' and content mention \\'breaking news,\\' \\'top stories,\\' \\'analysis,\\' \\'reliable journalism,\\' indicating a general information/news website. Strong keywords in domain and content (35 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), some category indicators (5 pts). Total 90 points.\",\\n  \"confidence\": 90\\n}\\n```\\n\\n**2nd Sample:**\\n\\n**Input Data:**\\n\\n```\\nDomain: lucky-slots-online.com\\nContent:  Spin to win big at Lucky Slots Online! Play hundreds of exciting slot games, claim your bonuses, and join the fun.  Licensed and regulated for your safety. 24/7 customer support available. Join now and get 100 free spins!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Domain \\'lucky-slots-online.com\\' and content include keywords like \\'slots,\\' \\'win,\\' \\'casino,\\' \\'bonuses,\\' \\'free spins,\\' indicating a gambling website.\",\\n  \"confidence\": 100\\n}\\n```\\n\\n**3rd Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: adult-pleasures.net\\nContent:  Experience the hottest adult entertainment online. Unlimited access to exclusive videos and photos. 18+ only. Join our community of pleasure seekers today!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 2,\\n  \"classification\": \"Pornography\",\\n  \"reason\": \"Domain \\'adult-pleasures.net\\' and content include explicit terms like \\'adult entertainment,\\' \\'exclusive videos,\\' \\'photos,\\' \\'18+ only,\\' and \\'pleasure seekers,\\' indicating a pornography website. Strong keywords in both domain and content (25 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n**4th Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: project420.com\\nContent:  Welcome to Project420.com an ongoing community marijuana, hemp search engine and your gateway to 420 products, services and information from all over the world. Marijuana seeds, super bongs, high tech growing marijuana books, contests, communities, cool people and tons of 420 fun.  Potent marijuana websites are added frequently.  Let\\'s help each other grow.  Use the table of contents located to the left and begin your search of 420 world wide websites.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 3,\\n  \"classification\": \"Harmful\",\\n  \"reason\": \"Domain \\'project420.com\\' and content include keywords like \\'marijuana seeds,\\' \\'growing marijuana,\\' and \\'420 products,\\' indicating promotion of illegal drugs under Indonesian law (UU ITE). Strong keywords in content (25 pts), Partial Domain and Content Alignment (15 pts), clear harmful intent (20 pts), multiple indicators (10 pts). Total 70 points.\",\\n  \"confidence\": 70\\n}\\n```\\n**Execution:**\\nProcess the provided website data (input fields: `Domain`, `Content`). Apply the classification categories, definitions, and confidence point system accurately. Generate the mandatory JSON output for each entry. Begin classification now.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nClassify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n\\nDomain: adevotedslave.blogspot.com, Content: \"Delicious Debauchery If You arenâ€™t over the age 18 (21 in some areas) then please leave. If You are offended by sex, BDSM, phone sex or brutal honesty then this isn\\'t the place for You. FRIDAY, JUNE 08, 2007 Why Iâ€™ll never be a full lesbianâ€¦ You know, it doesnâ€™t seem to matter what I am using or what position I am in. Iâ€™ve tried it with dildos, vibes, my gear shift knob, my fingers, and miscellaneous household objects, with just about anything and every thing I can think of. Iâ€™ve done it on my hands and knees on my back, with one leg up. Iâ€™ve tried putting the base on the floor and bouncing up and down. Iâ€™ve put it up against a wall and pushed against it. It doesnâ€™t matter. Nothing that I do makes it so that I can fuck myself hard enough to make it feel just right. Nothing can take me to that place where I am balancing between the pleasure of being fucked and the pain of having a cock battering my insides. Of course, when Iâ€™m trying to simulate hard fucking it still feels good, but itâ€™s that kind of feeling you get when you are almost there, but not quite. You know where you want to go but you just canâ€™t get there. Itâ€™s frustrating as hell. Besides, I have never actually had a toy, or object Iâ€™ve converted into a toy, thatâ€™s felt like a real cock anyway. They all just feel like random objects that fall short of the glory that is a real, hard, throbbing, piece of flesh designed for fucking. Of course, if thereâ€™s some woman out there whoâ€™s willing to try to convert me into a full fledged lesbian anyway, Iâ€™m more than willing to submit to that kind of training. Who knows? A woman with a strap on may be able to fuck me just like I like. Of course, Iâ€™d not refuse returning the favor. It could be a mutually beneficial experiment. POSTED BY DEIDRE AT 11:13 AM 2 COMMENTS SATURDAY, APRIL 28, 2007 Bruised and Battered For some reason I think that the depth of my pussy fluctuates. Sometimes when Masterâ€™s cock slides inside of me it is the perfect fit. The head hits in just the... en melted apart. A puff of smoke flew up, and there was another familiar sensation... the smell of burned meat. Again the heat, with a dirty orange light where the flame burned away the bits of tissue stuck to the wire. Again the application, the shudder, the pop, the hiss, the smoke. Around the stencil he went, carefully watching the brand, lining up the strikes. As the skin opens along the brand\\'s outline, it shifts. About halfway around, he started almost freehanding, using the stencil as a reminder of what he wanted, not as a guide. Then with the outline complete, and even, he used its angles as a reference for the final strikes down the middle. There were ten strikes in all. Ten times the iron glowed, ten times it lowered to her helpless flesh. Ten times she moaned as her love for me was burned into her very body. As I continued my research I rediscovered Fakirâ€™s site and the process of using an electro-cautery pencil instead of using the striking method with heated ribbons of steel. I showed a few examples to Master, including a photo of hisdevotionâ€™s back. He asked how one goes about getting an electro-cautery pencil. Until that moment it never even occurred to me that a person outside of the medical profession could just go and buy one. Well, for less than a hundred bucks one can buy a battery operated one with various different tips. Something wonderful happened as I scrolled through all of the options for this tool. I began reading in the descriptions that they get up to 2200 degrees Fahrenheit. Instead of the fear I expected to feel at such a number I felt the nervous flutters of excitement. If Master decides to go this direction, and have His mark of ownership burned into my skin in this fashion, I will have 2200 degrees focused on my skin, burning me, marking me bit by bit. This makes me incredibly aroused. Perhaps I have the makings of a pain slut afterall. POSTED BY DEIDRE AT 1:43 PM 0 COMMENTS TUESDAY, MARCH 13, 2007 masks Iâ€™ve said before that my relationship with Master and the trust and honesty that is the foundation of that relationship has made me more fully myself than I ever have been before. I am more aware of the different aspects of myself and I am able to lie them all out on the table to be poked at and prodded by any stranger who happens by. On the other hand there are those for whom I must wear certain masks. My relationship with Master might have inspired an intense honesty with myself and with Him, as well as with those who can accept us this way and those whose judgement doesnâ€™t matter, but itâ€™s also forced me to be more dishonest than ever to those around me, my neighbors, friends and family. I have been asked by potential sisters, by friends and by callers about how the world sees me. What does Master let the neighbors see? His friends? My friends? My family? His family? Itâ€™s all kind of complicated. The neighbors/His friends: * My neighbors see a 28 year old girl who is living with a 53 year old man. They see the 2 of us happily interacting with each other. He is very active in our neighborhood community. I am not. I occasionally wave to the neighbors or have a conversation with them and most of the time they say something to the effect of, \"Itâ€™s been a while. How have you been?\" Most of them are nice people, but I have little desire to interact with them. They know that I am a phone sex operator and most of them are intrigued. They do not know that I am a slave but there are probably quite a few who have observed that I donâ€™t make any decisions of importance. My friends: * My friends all know just about everything there is to know. Friends that I consider to be mine (as opposed to those that I know because of Master) are basically those from the internet. This includes a few people from collarme and other personals sites but the vast majority is made up of people from LiveJournal. LiveJournal is where all of my meaningful social interaction takes place, as sad as that may seem. There just arenâ€™t a whole lot of people like me nearby and being friends with a girl whoâ€™s first priority is never friendships can be trying. His family: * Masterâ€™s brother, K. He is aware of the basics of the dynamics between Master and myself however itâ€™s never discussed with him and itâ€™s never thrown in his face. We donâ€™t play or fuck in front of K. Iâ€™ve never been loaned to K. Master and I have discussed it and if K is anything heâ€™s probably more submissive than dominant. K knows that I am a phone sex operator. I think he may have heard me on occasion when I am taking calls. * The rest of Masterâ€™s family. He has one of those families that has reunions every year. The patriarch of the family is in his 90s and still kicking. I havenâ€™t met everyone yet, but Iâ€™ve met the ones that are either closest or that Master likes the most. They have opened their arms and accepted me, making me an honorary â€“insert Masterâ€™s family name here- One of them told me over thanksgiving that she has never seen Master as happy as he is now and she thinks that I am to thank for that. What higher compliment can someone pay me than that? My Family: * My sister, E, and I used to be very close. Weâ€™re not anymore though. When I went to her house last summer I had just come from my fathers and wasnâ€™t wearing my collar. The first night I was getting ready to go out with her and my mother and put it back on. Her reaction was that my necklace didnâ€™t match my outfit and I should take the jewelry off. I refused explaining that Master (I used his first name) gave it to me and I hadnâ€™t really had it off since except for when I visited dad. I think she started to get the idea at that point. She knew a little before then, but I think the image of me collared finished off any of the blank parts in her imagination of how I was living my life. I donâ€™t think she approves. We havenâ€™t spoken since then. Iâ€™m not sure how much my lifestyle has to do with that though. * My mother knows the pieces and details that I think sh...  get enough. Sheâ€™s become my newest addiction. I lick the juice of her squirt from my arm and slide my fingers from her into my mouth. I love that taste. Outside the sun is beginning to rise. Weâ€™ve been lost in each other for the entire night. The magic begins to dissipate. We get dressed once again shy and nervous. The blush on my face doesnâ€™t stop me from telling her how much I enjoyed the night. It turns out that she did too and that we both want to do it again. Soon. POSTED BY DEIDRE AT 1:59 PM 0 COMMENTS Older Posts Home Subscribe to: Posts (Atom) ABOUT DEIDRE DEIDRE STRADDLING THE LINE BETWEEN FANTASY AND REALITY deidre is a 28 yo lifestyle slave, phone sex operator and fetish model. she is happily owned. VIEW MY COMPLETE PROFILE LINKS 101 Things about deidre NITEFLIRT delicious debauchery Join for Free What is NiteFlirt? MP3 RECORDINGS The Holy Fuck **fetish sample Gender Blender **hypnosis sample How to Get Out of a Ticket **fetish sample Gym Discipline **fetish sample French Maid **fetish sample Lesbian Strap-on BDSM **fetish sample Lesbian Tickle Bondage **fetish sample Isabella\\'s Sex Slave **hypnosis sample Gagged By Goddess **fetish sample Boarding School Humiliation **fetish sample RECORDINGS (BY THE MINUTE) rates and descriptions deidre\\'s daily prayer deidre gets face fucked equipment sampler deidre\\'s discipline barking cough tickle torture psalm 23 (deidre style) a steamy shower & my fav toy PHOTO SETS rates and descriptions teddy bears & coloring books deidre\\'s puppy training photo sampler bondage the golden set deidre\\'s naughty picnic deidre in distress foot bondage PODCASTS Session 2 Interview with Miss Isabella Session 1 WISHFUL THINKING amazon wishlist deidre\\'s corset fund LABELS questions (13) NiteFlirt (12) mp3 (11) calls (6) gifts (5) music (5) pics (2) podcast (2) meme (1) quotes (1) BLOG ARCHIVE â–¼ 2007 (41) â–¼ JUNE (1) â–¼ JUN 08 (1) WHY Iâ€™LL NEVER BE A FULL LESBIANâ€¦ â–º APRIL (2) â–º MARCH (6) â–º FEBRUARY (11) â–º JANUARY (21) â–º 2006 (117) â–º 1978 (2)\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<think>\\nOkay, I need to classify this website into one of the four categories: Benign, Gambling, Pornography, or Harmful. Let\\'s start by looking at the domain and content provided.  The domain is adevotedslave.blogspot. com. The word \"slavery\" in the domain could be a red flag because it might relate to harmful activities like human trafficking. But I should also consider if it\\'s used in a different context, such as BDSM or kink communities which are more about consensual adult content.  Looking at the content, there are several posts discussing BDSM practices, phone sex, and explicit sexual scenarios. For example, one post talks about trying various objects for self-pleasure and another describes a session involving masks and domination. There\\'s also mention of being a \"lifestyle slave\" and a \"phone sex operator,\" which suggests the site is focused on adult content.  The content includes terms like \"BDSM,\" \"fetish,\" \"bondage,\" and explicit descriptions of sexual acts. These keywords are strong indicators of Pornography because they relate to adult themes and explicit material intended for arousal.  However, I should check if it falls under Harmful instead. The website doesn\\'t seem to promote illegal activities like drug sales or cybercrime. It\\'s more about consensual adult content, which would typically fall under Pornography rather than Harmful unless there are elements of non-consent or exploitation, which aren\\'t clearly present here.  The domain and content align well with Pornography because the focus is on explicit sexual content and fetishes. There are no strong indicators pointing towards Gambling or Benign categories. The content is unambiguous in its adult themes, so it doesn\\'t seem to be a case of harmful activities outside the scope of Pornography.  In terms of confidence assessment: - Keyword Strength: Strong keywords like \"fetish,\" \"bondage,\" and explicit sexual descriptions are present both in the domain and content. - Domain-Content Alignment: The domain suggests a BDSM or slavery theme, which is reinforced by the content discussing explicit adult scenarios. - Content Clarity: The content is very clear about its focus on adult entertainment and fetishes without ambiguity. - Category Indicators: Multiple indicators such as explicit sexual descriptions, use of terms like \"phone sex operator,\" and detailed accounts of BDSM practices.  Based on these factors, I\\'m confident that this website falls under the Pornography category with a high confidence score.\\n</think>\\n```json\\n{\\n  \"answer\": 2,\\n  \"classification\": \"Pornography\",\\n  \"reason\": \"The domain \\'adevotedslave. blogspot.com\\' and content include explicit terms like \\'BDSM,\\' \\'fetish,\\' \\'bondage,\\' and detailed descriptions of sexual acts, indicating a focus on adult entertainment. Strong keywords in both domain and content (25 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```<|eot_id|>'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['test'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d4bd38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3090 Ti. Max memory = 23.551 GB.\n",
      "3.441 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f793d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 33,128 | Num Epochs = 2 | Total steps = 1,665\n",
      "O^O/ \\_/ \\    Batch size per device = 5 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (5 x 4 x 1) = 20\n",
      " \"-____-\"     Trainable parameters = 97,255,424/3,000,000,000 (3.24% trained)\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='401' max='1665' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 401/1665 9:38:42 < 30:33:18, 0.01 it/s, Epoch 0.24/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.312200</td>\n",
       "      <td>5.440914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.063000</td>\n",
       "      <td>4.987135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.062400</td>\n",
       "      <td>3.579475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.394600</td>\n",
       "      <td>1.360912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.742300</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.029804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.024851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.601300</td>\n",
       "      <td>0.019787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.016867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.014520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.013574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.013232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.011845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.012137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.012772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.012019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.011952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.011193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.011639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.012319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.011727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.010738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.010598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.011021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.010620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.009478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.009902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.011152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.010728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>0.009967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.008882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.010287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.009694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.008692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.011382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.012170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.010337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.010210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.009963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.009215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 15/134 00:10 < 01:28, 1.35 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to outputs/checkpoint-100\n",
      "loading configuration file config.json from cache at /home/fishmon/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/aae9d3e87a2c47cc465b6980017a05e1d1d61c8c/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": [\n",
      "      \"lm_head\",\n",
      "      \"multi_modal_projector\",\n",
      "      \"merger\",\n",
      "      \"modality_projection\"\n",
      "    ],\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to outputs/checkpoint-200\n",
      "loading configuration file config.json from cache at /home/fishmon/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/aae9d3e87a2c47cc465b6980017a05e1d1d61c8c/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": [\n",
      "      \"lm_head\",\n",
      "      \"multi_modal_projector\",\n",
      "      \"merger\",\n",
      "      \"modality_projection\"\n",
      "    ],\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to outputs/checkpoint-300\n",
      "loading configuration file config.json from cache at /home/fishmon/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/aae9d3e87a2c47cc465b6980017a05e1d1d61c8c/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": [\n",
      "      \"lm_head\",\n",
      "      \"multi_modal_projector\",\n",
      "      \"merger\",\n",
      "      \"modality_projection\"\n",
      "    ],\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text, conversations. If text, conversations are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 134\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to outputs/checkpoint-400\n",
      "loading configuration file config.json from cache at /home/fishmon/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/aae9d3e87a2c47cc465b6980017a05e1d1d61c8c/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": [\n",
      "      \"lm_head\",\n",
      "      \"multi_modal_projector\",\n",
      "      \"merger\",\n",
      "      \"modality_projection\"\n",
      "    ],\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# checkpoint_path = \"./outputs/checkpoint-700\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# if os.path.exists(checkpoint_path):\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     trainer.train(resume_from_checkpoint=checkpoint_path)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     print(f\"Warning: Checkpoint {checkpoint_path} not found. Starting from scratch.\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:314\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:77\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/accelerate/accelerator.py:2454\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2454\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n",
    "\n",
    "# checkpoint_path = \"./outputs/checkpoint-700\"\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     trainer.train(resume_from_checkpoint=checkpoint_path)\n",
    "# else:\n",
    "#     print(f\"Warning: Checkpoint {checkpoint_path} not found. Starting from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this before the automatic evaluation at step 500\n",
    "trainer.evaluate(eval_dataset=trainer.eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d15042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78330dcd",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889541fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(config.get('output_dir', 'outputs'))\n",
    "tokenizer.save_pretrained(config.get('output_dir', 'outputs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db833ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.46s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jordinia/test-finetune/commit/2b75560795409c528fa06c24185ba3ef1c45a59a', commit_message='Initial commit of Qwen3-0505-LORA model', commit_description='', oid='2b75560795409c528fa06c24185ba3ef1c45a59a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/jordinia/test-finetune', endpoint='https://huggingface.co', repo_type='model', repo_id='jordinia/test-finetune'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "model_id = \"jordinia/test-finetune\"\n",
    "local_model_path = \"/home/fishmon/AJ/LLM-Finetuning/Malicious-Web/model/qwen3-0505-lora_model\"\n",
    "hf_token = \"\"\n",
    "\n",
    "# Authenticate\n",
    "HfFolder.save_token(hf_token)\n",
    "api = HfApi()\n",
    "\n",
    "# Upload the folder with a commit message\n",
    "api.upload_folder(\n",
    "    folder_path=local_model_path,\n",
    "    repo_id=model_id,\n",
    "    repo_type=\"model\",\n",
    "    use_auth_token=hf_token,\n",
    "    commit_message=\"Initial commit of Qwen3-0505-LORA model\",  # Add your commit message here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa7d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=\"/path/to/local/model\",\n",
    "    repo_id=\"jordinia/netpro-0505-lora\",\n",
    "    repo_type=\"model\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323768d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 12.5 out of 31.26 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:00<00:00, 407.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"qwen3-0505-fp16-vllm\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e190418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: qwen3-0505-fp16-vllm\n",
      "INFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {2560, 151936}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.16.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.16.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.17.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.17.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.18.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.18.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.19.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.19.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.20.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.20.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.21.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.22.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.22.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.23.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.23.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.24.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.24.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.25.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.25.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.26.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.26.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.27.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.27.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.28.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.28.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.29.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.29.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.30.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.30.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.31.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.31.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.32.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.32.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.33.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.weight,      torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_output.weight, torch.bfloat16 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.33.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.weight,      torch.bfloat16 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.weight,      torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.bfloat16 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.bfloat16 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.34.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.weight,      torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_output.weight, torch.bfloat16 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.34.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.weight,      torch.bfloat16 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.weight,      torch.bfloat16 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.float32 --> F16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.float32 --> F16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.35.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_output.weight, torch.float32 --> F16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.35.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.weight,      torch.float32 --> F16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.weight,      torch.float32 --> F16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 40960\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2560\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 9728\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151645\n",
      "INFO:gguf.vocab:Setting special token type pad to 151654\n",
      "INFO:gguf.vocab:Setting add_bos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- messages[0].content + '\\n\\n' }}\n",
      "    {%- endif %}\n",
      "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
      "{%- for forward_message in messages %}\n",
      "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
      "    {%- set message = messages[index] %}\n",
      "    {%- set tool_start = '<tool_response>' %}\n",
      "    {%- set tool_start_length = tool_start|length %}\n",
      "    {%- set start_of_message = message.content[:tool_start_length] %}\n",
      "    {%- set tool_end = '</tool_response>' %}\n",
      "    {%- set tool_end_length = tool_end|length %}\n",
      "    {%- set start_pos = (message.content|length) - tool_end_length %}\n",
      "    {%- if start_pos < 0 %}\n",
      "        {%- set start_pos = 0 %}\n",
      "    {%- endif %}\n",
      "    {%- set end_of_message = message.content[start_pos:] %}\n",
      "    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\n",
      "        {%- set ns.multi_step_tool = false %}\n",
      "        {%- set ns.last_query_index = index %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set content = message.content %}\n",
      "        {%- set reasoning_content = '' %}\n",
      "        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n",
      "            {%- set reasoning_content = message.reasoning_content %}\n",
      "        {%- else %}\n",
      "            {%- if '</think>' in message.content %}\n",
      "                {%- set content = (message.content.split('</think>')|last).lstrip('\\n') %}\n",
      "                {%- set reasoning_content = (message.content.split('</think>')|first).rstrip('\\n') %}\n",
      "                {%- set reasoning_content = (reasoning_content.split('<think>')|last).lstrip('\\n') %}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "        {%- if loop.index0 > ns.last_query_index %}\n",
      "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
      "            {%- else %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "            {%- endif %}\n",
      "        {%- else %}\n",
      "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls %}\n",
      "            {%- for tool_call in message.tool_calls %}\n",
      "                {%- if (loop.first and content) or (not loop.first) %}\n",
      "                    {{- '\\n' }}\n",
      "                {%- endif %}\n",
      "                {%- if tool_call.function %}\n",
      "                    {%- set tool_call = tool_call.function %}\n",
      "                {%- endif %}\n",
      "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
      "                {{- tool_call.name }}\n",
      "                {{- '\", \"arguments\": ' }}\n",
      "                {%- if tool_call.arguments is string %}\n",
      "                    {{- tool_call.arguments }}\n",
      "                {%- else %}\n",
      "                    {{- tool_call.arguments | tojson }}\n",
      "                {%- endif %}\n",
      "                {{- '}\\n</tool_call>' }}\n",
      "            {%- endfor %}\n",
      "        {%- endif %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
      "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:qwen3-0505-fp16-vllm.gguf: n_tensors = 398, total_size = 8.0G\n",
      "Writing:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 1.98G/8.05G [00:05<00:23, 261Mbyte/s]/home/fishmon/AJ/llama.cpp/gguf-py/gguf/lazy.py:217: RuntimeWarning: overflow encountered in cast\n",
      "  return type(self)(meta=meta, args=full_args, kwargs=kwargs, func=(lambda a, *args, **kwargs: a.astype(*args, **kwargs)))\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.05G/8.05G [00:51<00:00, 158Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to qwen3-0505-fp16-vllm.gguf\n"
     ]
    }
   ],
   "source": [
    "!python /home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py qwen3-0505-fp16-vllm --outfile qwen3-0505-fp16-vllm.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7e8b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily skip merging for conversion\n",
    "model.save_pretrained(\"qwen3-0505-q4-vllm\", tokenizer, save_method=\"4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e74e2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: qwen3-0505-q4-vllm\n",
      "INFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:choosing --outtype bf16 from first tensor type (torch.bfloat16)\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {2560, 151936}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.uint8 --> BF16, shape = {1, 12451840}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 6097, in <module>\n",
      "    main()\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 6091, in main\n",
      "    model_instance.write()\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 401, in write\n",
      "    self.prepare_tensors()\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 277, in prepare_tensors\n",
      "    for new_name, data_torch in (self.modify_tensors(data_torch, name, bid)):\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 2609, in modify_tensors\n",
      "    yield from super().modify_tensors(data_torch, name, bid)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 245, in modify_tensors\n",
      "    return [(self.map_tensor_name(name), data_torch)]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py\", line 236, in map_tensor_name\n",
      "    raise ValueError(f\"Can not map tensor {name!r}\")\n",
      "ValueError: Can not map tensor 'model.layers.10.mlp.down_proj.weight.absmax'\n"
     ]
    }
   ],
   "source": [
    "!python /home/fishmon/AJ/llama.cpp/convert_hf_to_gguf.py qwen3-0505-q4-vllm --outfile qwen3-0505-q4-vllm.gguf --outtype auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python convert_hf_to_gguf.py qwen3-0505-fp16-vllm --outfile qwen3-0505--q4fp16-vllm.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fc928",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a39afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fishmon/.conda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.8: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 1. Max memory: 23.551 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.6. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fishmon/.conda/envs/unsloth_env/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's tackle this classification. The domain is https://www.forbes.com/, and the content is a mix of news headlines and some subscription prompts. \n",
      "\n",
      "First, I need to check the categories. The domain is Forbes, which is a well-known news and business publication. The content includes breaking news about hurricanes, politics, and business, along with articles on billionaires and tech. There's also a subscription section and some ads.\n",
      "\n",
      "Looking at the categories: Benign (0) would be general info, news, etc. Gambling (1) is about betting, casinos, etc. Pornography (2) is explicit adult content. Harmful (3) is for illegal activities, malware, etc.\n",
      "\n",
      "The content here is all news and business articles. There's no mention of gambling, pornography, or harmful activities. The domain is a news site, and the content is about current events, business, and finance. The subscription part is for accessing premium content, which is common for news sites. \n",
      "\n",
      "I don't see any keywords related to gambling, pornography, or illegal activities. The content is focused on business, politics, and current events, which are all under the Benign category. \n",
      "\n",
      "Confidence points: The domain is clearly a news site, and the content is all news and business. The keywords are \"breaking news,\" \"billionaires,\" \"tax,\" \"hurricane,\" which are all benign. There's no mention of anything harmful or explicit. So, I would classify this as Benign (0). \n",
      "\n",
      "The confidence should be high. The domain and content align with Benign. No harmful or explicit content. So, the answer is 0.\n",
      "</think>\n",
      "\n",
      "{\n",
      "  \"answer\": 0,\n",
      "  \"classification\": \"Benign\",\n",
      "  \"reason\": \"Domain 'forbes.com' is a reputable news and business publication. Content includes breaking news, business analysis, and economic updates, with no explicit references to gambling, pornography, or harmful activities. Strong keywords in domain and content (35 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), some category indicators (5 pts). Total 90 points.\",\n",
      "  \"confidence\": 90\n",
      "}<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "\n",
    "# 1. Load base model (EXACTLY matching your training config)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",  # Must match training\n",
    "    max_seq_length = 30000,  # Same as training\n",
    "    load_in_4bit = True,     # Must match training\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# 2. Load your fine-tuned adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    model_id = \"model/qwen3-0505-lora_model\",\n",
    "    adapter_name = \"web_classifier\",\n",
    "    is_trainable = False,  # Crucial for inference\n",
    ")\n",
    "\n",
    "# 3. Merge and optimize (reduces VRAM by ~30%)\n",
    "model = model.merge_and_unload()\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "# 3. Load prompts\n",
    "with open('./prompt/labelling_promptv4.txt', 'r') as f:\n",
    "    system_prompt = f.read()\n",
    "with open('./prompt/class_0_sample1.txt', 'r') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# 4. Format input\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n {sample_text}\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    max_length = 30000,  # Prevent OOM\n",
    "    truncation = True,\n",
    "    enable_thinking = True, \n",
    ").to(\"cuda\")\n",
    "\n",
    "# 5. Generate\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 2048,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1, top_p = 0.9,)\n",
    "# outputs = model.generate(\n",
    "#     inputs,\n",
    "#     max_new_tokens = 2048,  \n",
    "#     temperature = 0.7,  \n",
    "#     top_p = 0.9,\n",
    "#     repetition_penalty = 1.2,\n",
    "#     eos_token_id        = tokenizer.eos_token_id,\n",
    "#     pad_token_id        = tokenizer.pad_token_id,\n",
    "#     use_cache = True,\n",
    "# )\n",
    "# tokenizer.batch_decode(outputs)\n",
    "# # print(tokenizer.decode(outputs[0]))                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af7fada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>system\\nYou are an Expert Website Classifier tasked with categorizing websites (using provided Domain & Content) into distinct categories: **0 - Benign**, **1 - Gambling**, **2 - Pornography**, or **3 - Harmful**. Output strict JSON including classification, reason, and a point-based confidence score (0-100). This aids a sophisticated website prediction system for digital safety.\\n\\n### **Categories & Definitions:**\\n\\n-   **0 - Benign:** General info, news, entertainment, services, e-commerce, educational sites, blogs, informational pages, and general entertainment sites. No gambling/porn themes.\\n    -   **Includes:** Educational discussion of sensitive topics (drugs, gambling, adult, illegal), suggestive content (dating, lingerie) *without* explicit material, drugs for educational or news purposes (e.g., addiction recovery, medical cannabis research).\\n\\n-   **1 - Gambling:** Promotes/facilitates betting, casino, poker, lottery, wagering. Includes online betting platforms, casinos, poker sites, lottery sites, and any website that encourages or provides means for users to wager money on games of chance or skill.\\n    -   **Keywords:** betting, casino, poker, odds, jackpot, slots, roulette, judi, slot gacor, togel, similar gambling-related terminology within their domain name or content.\\n    \\n-   **2 - Pornography:** These are websites that contain explicit sexual content intended to cause arousal. Explicit sexual content (images, videos, text) for arousal. Immoral content (bestiality, child exploitation). or links to such materials. \\n    -   **Keywords:** porn, bokep, sex, xxx, adult, nude, erotic, explicit video/photo.\\n\\n- **3 - Harmful**: Websites that does not fall to Benign, Gambling, and Pornography category. These websites engage in or promote activities harmful to users or violating laws/regulations, including:\\n    - Malware Distribution: Hosting/downloading computer viruses, worms, ransomware, spyware, etc.\\n    - Cybercrime: Phishing kits, hacking tools, stolen data markets, carding forums.\\n    - Extremism & Terrorism: Content inciting violence, extremist ideologies, or terrorist recruitment.\\n    - Violations of Indonesian Law:\\n        - Insults, defamation, blackmail, or threats.\\n        - Hoaxes/misleading news, hate speech, or incitement of violence.\\n    - Copyright Infringement/Piracy: Illegal software/cracks, torrents, pirated media.\\n    - Drugs/Narcotics: Sale/promotion of illegal drugs (e.g., cocaine, heroin) or unregulated pharmaceuticals.\\n    - Weapons: Sale of illegal firearms, explosives, or weapons.\\n    - Other Illegal Activities: Counterfeit goods, money laundering, human trafficking.\\n    - **Examples:**  \\n        - Illegal: `darknet-drugs.com` (drug sales), `pirated-movies.id` (piracy).  \\n        - Harmful: `extremist-forum.net` (terrorism recruitment), `hackers-tools.org` (phishing kits).  \\n        - Scam/phishng: `hadiah-telkomsel7.blogspot.com` (non-genuine website).\\n    - **Keywords**:\\nmalware, ransomware, phishing, hack, terrorism, jual narkoba, senjata ilegal, konten SARA, berita bohong, ancaman, pembajakan, cracked software, carding, darknet.\\n    - **Exceptions:** \\n        - Licensed/unlicensed gambling â†’ **1 - Gambling**; scams â†’ **3 - Harmful**.\\n\\n\\n### **Input Data Context:**\\n\\nYou will be provided with data entries, each consisting of two primary fields:\\n\\n*   **Domain:** The domain name or URL of the website (e.g., `example.com`, `gamble-site.net`). This can provide hints about the website\\'s purpose.\\n*   **Content:**  The textual content scraped from the website. This content offers detailed information about the website\\'s topics, services, and themes.\\n\\n### **Labeling Instructions:**\\n\\nAnalyze both the **Domain** and the **Content** provided. Use keywords and contextual clues from both to determine the most appropriate category for the website.  Consider the primary purpose and content focus of the website when classifying.\\n\\n### **Confidence Assessment Guidelines: Point-Based System (Total Possible Points: 100)**\\n\\nTo determine the **confidence** level (0-100) for your classification, evaluate the following factors and sum up the points.  The total points will directly correspond to the confidence percentage (e.g., 95 points = 95% confidence).\\n\\n#### **I. Keyword Strength and Relevance (Maximum 40 Points)**\\n*   **(40 Points):  Exceptional Keyword Strength: Explicit and Overwhelming Keywords in BOTH Domain and Content:** Presence of extremely explicit and overwhelmingly strong keywords that are *unquestionably* indicative of a specific category in *both* the domain name AND the website content. These keywords leave absolutely no doubt about the website\\'s nature. (e.g., Domain: `casino-royal-betting.com`, Content:  \"Gamble now and win HUGE jackpots on slots, poker, roulette! Real money betting!\").  This represents the absolute strongest keyword signal possible.\\n*   **(35 Points): Clear and Strong Keywords in BOTH Domain and Content:**  Presence of highly explicit keywords strongly indicative of a specific category in both the domain name AND the website content. (e.g., Domain: `bet.com`, Content:  \"Bet on sports and casino games!\").\\n*   **(25 Points): Strong Keywords in EITHER Domain OR Content:** Presence of highly explicit keywords strongly indicative of a specific category in EITHER the domain name OR the website content, but not both.\\n*   **(15 Points): Some Relevant Keywords:** Presence of keywords related to a category, but they are less explicit, less frequent, or require more contextual interpretation in either domain or content.\\n*   **(0 Points): Weak or Generic Keywords:** Lack of clear category-specific keywords in both domain and content. Keywords are generic and do not strongly suggest any specific category.\\n\\n#### **II. Domain and Content Alignment (Maximum 30 Points)**\\n\\n*   **(30 Points): Strong Domain and Content Alignment:** Domain name strongly and unambiguously suggests a category, and the website content consistently and explicitly reinforces that category.  They tell the same clear story.\\n*   **(15 Points): Partial Domain and Content Alignment:** Domain name and content generally point towards the same category, but the alignment might be less direct, slightly ambiguous, or require some interpretation to connect them.\\n*   **(0 Points): Domain-Content Mismatch or No Alignment:** Domain name suggests one thing, but the content is unclear, suggests something different, or there\\'s no clear connection between the domain and the content\\'s apparent purpose.\\n\\n#### **III. Content Clarity and Unambiguity (Maximum 20 Points)**\\n\\n*   **(20 Points): Unambiguous and Explicit Content:** The website content is very clear, direct, and leaves virtually no room for interpretation. It unambiguously falls into one of the defined categories.\\n*   **(10 Points): Content Requires Some Interpretation:** The content generally points to a category, but requires some interpretation to confidently assign it.  There might be subtle hints, implied meanings, or a need to infer the primary purpose.\\n*   **(0 Points): Ambiguous or Conflicting Content:** The website content is vague, contradictory, or could be reasonably interpreted in multiple ways, making it difficult to confidently assign a category.\\n\\n#### **IV. Category Indicator Strength (Maximum 10 Points)**\\n\\n*   **(10 Points): Multiple Strong Category Indicators:** Presence of numerous strong and clear indicators for a specific category throughout the domain and content (e.g., for Pornography: explicit keywords, descriptions of sexual acts, calls to action to view adult content, age verification prompts).\\n*   **(5 Points): Some Category Indicators Present:** Presence of a few indicators for a category, but they are not overwhelmingly strong or numerous.\\n*   **(0 Points): Lack of Category Indicators:** Few or no clear indicators for any of the defined categories are present in the domain and content.\\n\\n#### **Calculation:**\\n\\n1.  For each of the four sections (I-IV), assess the website and select the point value that best describes the presence of the described factors.\\n2.  Sum up the points from all four sections.\\n3.  The total sum represents the confidence level in percentage (%).\\n\\n#### **Confidence Level Ranges (for reference - already implicitly defined by points):**\\n\\n*   **High Confidence (80-100 Points):**  Strong evidence across multiple factors pointing clearly to a category.\\n*   **Medium Confidence (50-79 Points):** Moderate evidence, some ambiguity or less directness in indicators.\\n*   **Low Confidence (0-49 Points):** Weak or conflicting evidence, high uncertainty about the correct category.\\n\\n#### **Example:**\\n\\nLet\\'s say you are classifying `lucky-slots-online.com` with content about slot games and bonuses.\\n\\n*   **I. Keyword Strength:** Strong keywords in both Domain and Content (e.g., \"slots,\" \"online,\" \"win,\" \"bonuses\") - **25 Points**\\n*   **II. Domain-Content Alignment:** Domain and content strongly align with Gambling - **30 Points**\\n*   **III. Content Clarity:** Content is very clear about gambling activities - **20 Points**\\n*   **IV. Category Indicators:** Multiple indicators of gambling (games, bonuses, calls to action) - **10 Points**\\n\\n**Total Points: 25 + 30 + 20 + 10 = 85 Points.  Confidence: 85%**\\n\\n#### **Using this Point System:**\\n\\nWhen generating the \"reason\" for your classification, you can now also briefly mention the points you assigned for each section to justify the final confidence score. For example:\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Strong keywords in domain and content (25 pts), strong domain-content alignment (30 pts), clear gambling content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n\\n### **Output Requirements: STRICTLY ADHERE TO JSON FORMAT**\\n\\nFor each website, you **MUST** provide your classification in the following **JSON format ONLY**.  **No other format is acceptable.**\\n\\n```json\\n{\\n  \"answer\": (integer: 0, 1, 2, or 3),\\n  \"classification\": (string: \"Benign\", \"Gambling\", \"Pornography\", or \"Harmful\"),\\n  \"reason\": (string: \"Brief explanation of the classification using keywords and hints from the domain and content.\"),\\n  \"confidence\": (integer: 0 to 100, \"Level of confidence in the classification\")\\n}\\n```\\n\\n**Example Input and Expected Output:**\\n\\n**1st Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: news-today.info\\nContent: Welcome to News Today! Get the latest breaking news, top stories, and in-depth analysis from around the world. Covering politics, business, technology, sports, and culture. Stay informed with News Today - your source for reliable journalism.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 0,\\n  \"classification\": \"Benign\",\\n  \"reason\": \"Domain \\'news-today.info\\' and content mention \\'breaking news,\\' \\'top stories,\\' \\'analysis,\\' \\'reliable journalism,\\' indicating a general information/news website. Strong keywords in domain and content (35 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), some category indicators (5 pts). Total 90 points.\",\\n  \"confidence\": 90\\n}\\n```\\n\\n**2nd Sample:**\\n\\n**Input Data:**\\n\\n```\\nDomain: lucky-slots-online.com\\nContent:  Spin to win big at Lucky Slots Online! Play hundreds of exciting slot games, claim your bonuses, and join the fun.  Licensed and regulated for your safety. 24/7 customer support available. Join now and get 100 free spins!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Domain \\'lucky-slots-online.com\\' and content include keywords like \\'slots,\\' \\'win,\\' \\'casino,\\' \\'bonuses,\\' \\'free spins,\\' indicating a gambling website.\",\\n  \"confidence\": 100\\n}\\n```\\n\\n**3rd Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: adult-pleasures.net\\nContent:  Experience the hottest adult entertainment online. Unlimited access to exclusive videos and photos. 18+ only. Join our community of pleasure seekers today!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 2,\\n  \"classification\": \"Pornography\",\\n  \"reason\": \"Domain \\'adult-pleasures.net\\' and content include explicit terms like \\'adult entertainment,\\' \\'exclusive videos,\\' \\'photos,\\' \\'18+ only,\\' and \\'pleasure seekers,\\' indicating a pornography website. Strong keywords in both domain and content (25 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n**4th Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: project420.com\\nContent:  Welcome to Project420.com an ongoing community marijuana, hemp search engine and your gateway to 420 products, services and information from all over the world. Marijuana seeds, super bongs, high tech growing marijuana books, contests, communities, cool people and tons of 420 fun.  Potent marijuana websites are added frequently.  Let\\'s help each other grow.  Use the table of contents located to the left and begin your search of 420 world wide websites.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 3,\\n  \"classification\": \"Harmful\",\\n  \"reason\": \"Domain \\'project420.com\\' and content include keywords like \\'marijuana seeds,\\' \\'growing marijuana,\\' and \\'420 products,\\' indicating promotion of illegal drugs under Indonesian law (UU ITE). Strong keywords in content (25 pts), Partial Domain and Content Alignment (15 pts), clear harmful intent (20 pts), multiple indicators (10 pts). Total 70 points.\",\\n  \"confidence\": 70\\n}\\n```\\n**Execution:**\\nProcess the provided website data (input fields: `Domain`, `Content`). Apply the classification categories, definitions, and confidence point system accurately. Generate the mandatory JSON output for each entry. Begin classification now.<|im_end|>\\n<|im_start|>user\\nClassify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n \\\\nDomain: jitutogel.net, Content: \\\\\"Whatsapp Telegram Livechat DAFTAR LOGIN BERANDA SLOT GAMES LIVE CASINO POKER ONLINE E-SPORTS ARCADE LOTTERY PROMOTION Jackpot Gaming Playstar TTG Slots Spadegaming RedTiger GMW CQ9 Gaming Live Gaming Evolution Gaming Sexy Gaming SAgaming HOgaming Gameplay OpusGaming Sports Gaming CMD368 SBOBET UBOBET TFGaming Ultraplay SabaEsports JituTogel: Serunya Game Online dengan Hadiah Uang Nyata! Penyedia Games Metode Pembayaran Bank Cimb Niaga BCA Danamon Permata Mandiri BNI BRI Panin Pulsa XL Axiata Tri Telkomsel Axis E-Money QRIS Dana OVO LinkAja Gopay Â©2024 jitutogel. All rights reserved | 18+ LIVECHAT\\\\\"<|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay, let me start by looking at the domain and content provided. The domain is \"jitutogel.net\". The content mentions several things: WhatsApp, Telegram, live chat, login, and various gaming options like Slot Games, Live Casino, Poker Online, E-Sports, Arcade, Lottery Promotion, Jackpot Gaming, Playstar, TTG Slots, Spadegaming, RedTiger, GMW, CQ9 Gaming, Live Gaming, Evolution Gaming, Sexy Gaming, SA Gaming, HOGaming, Gameplay, OpusGaming, Sports Gaming, CMD368, SBOBET, UBOBET, TFGaming, Ultraplay, SABA Esports. Then there\\'s a note saying \"JituTogel: Serunya Game Online dengan Hadiah Uang Nyata!\" which translates to \"JituTogel: Enjoy Online Games with Real Money Rewards!\" followed by payment methods listed.\\n\\nFirst, I check if these keywords relate to known categories. Words like \"Slot Games\", \"Live Casino\", \"Poker Online\", \"E-Sports\", \"Jackpot Gaming\" immediately make me think of gambling. Also, phrases like \"Hadiah Uang Nyata\" mean real cash rewards, which is common in gambling contexts. Additionally, the domain has \"togel\" which refers to traditional lotteries in Indonesia, often associated with scamming due to its association with fraud in some regions. \\n\\nThe content includes details about deposit methods via banks and mobile providers, which is typical for gambling sites offering real money prizes. However, since the user hasn\\'t specified whether this site is legitimate or part of a scam, but based solely on the words used here, it seems to fit under Gambling because the main theme is providing online game opportunities with real money incentives, even though \"toto\" sometimes relates to fraudulent schemes. But according to guidelines, if it promotes/wagering, then it\\'s Gambling category.\\n\\nLooking further, the term \"JituTogel\" itself may refer to a type of lottery or bet, reinforcing the idea. Now checking against the definition for Gambling: promoting/betting on casino, poker, lottery, etc. So yes, the domain and content both indicate promotional aspects of gambling through online games and real-money rewards. Therefore, the answer should be 1 - Gambling.\\n\\nNow assessing the confidence levels. Looking at the key areas:\\n\\nSection I: Keywords strength. Terms like \"slot games\", \"live casino\", \"real money rewards\" are strong gamblind-indicative. Both domain (\"togel\" linked to lotto/scams, but also possibly genuine) and content have those elements. Since \"togel\" isn\\'t exactly a gambling term per se but here paired with \"game online dan hadiah uang nyata\" (cash reward), so likely Gambling. Maybe 35 points? Or maybe higher?\\n\\nWait, considering the domain and content together. The domain uses \"togel\" which is commonly associated with scammers in certain contexts. But in this case, combined with the list of games, it\\'s probably Gambling. So perhaps stronger keywords in both domain and content. That would give 35 points.\\n\\nSection II: Domain and content alignment. If domain says \"jitutogel.net\" and the content lists many types of online games along with real money promotions, that\\'s a solid match. So full 30 points.\\n\\nSection III: Content clarity. The content is pretty straightforwardly describing a platform that offers online games with real money prizes. Very clear indication of Gambling. Assign 20 points.\\n\\nSection IV: Category indicators. Several repeated terms: slot games, live casino, poker, real money, etc. Many signs. 10 points.\\n\\nSo adding up: 35 + 30 + 20 + 10 = 95 points. High confidence. Hence, answer 1, Classification Gambling, confidence 95.\\n</think>\\n\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Domain \\'jitutogel.net\\' and content emphasize online gaming (\\'slot games\\', \\'live casino\\', \\'poker online\\'), real money rewards (\\'Hadiah Uang Nyata\\'), and payment methods tied to financial transactions. Terms like \\'jackpot gaming\\', \\'lottery promotion\\', and \\'real cash prize\\' strongly associate with gambling activities. Domain contains \\'togel\\' (Indonesian lottery), while content highlights profit-driven offerings consistent with gambling operations.\",\\n  \"confidence\": 95\\n}<|im_end|>']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./prompt/labelling_promptv4.txt', 'r') as f:\n",
    "    system_prompt = f.read()\n",
    "with open('./prompt/class_3_sample1.txt', 'r') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# 4. Format input\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n {sample_text}\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    max_length = 30000,  # Prevent OOM\n",
    "    truncation = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 5. Generate\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens = 2048,  \n",
    "    temperature = 0.7,  \n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.2,\n",
    "    eos_token_id        = tokenizer.eos_token_id,\n",
    "    pad_token_id        = tokenizer.pad_token_id,\n",
    "    use_cache = True,\n",
    ")\n",
    "tokenizer.batch_decode(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
