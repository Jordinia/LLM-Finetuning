{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eea4c6f",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10039837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fishmon/.conda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from jsonschema import validate, ValidationError\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056f1cb",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a3927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"json\", data_files=\"./dataset/netpro_chatml_thought.jsonl\")\n",
    "\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc5a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset[\"train\"]  # Access the 'train' split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed36710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output Type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset Info:\n",
      "Dataset({\n",
      "    features: ['conversations', 'source', 'score'],\n",
      "    num_rows: 100000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOutput Type:\", type(dataset))\n",
    "print(\"Dataset Info:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8651d1",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4834df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1 with CUDA 1201 (you have 2.6.0+cu124)\n",
      "    Python  3.11.10 (you have 3.11.11)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 1. Max memory: 23.551 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.3.19 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from typing import Dict, Any\n",
    "\n",
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "config = load_config('./config/config-0405.yml')\n",
    "\n",
    "# Model and Training Config\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=config.get('model_name', 'unsloth/Llama-3.2-3B-Instruct-bnb-4bit'),\n",
    "    max_seq_length=config.get('max_seq_length', 30000),\n",
    "    dtype=config.get('dtype', None),\n",
    "    load_in_4bit=config.get('load_in_4bit', True)\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=config.get('lora_r', 32),\n",
    "    target_modules=config.get('target_modules', [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]),\n",
    "    lora_alpha=config.get('lora_alpha', 32),\n",
    "    lora_dropout=config.get('lora_dropout', 0),\n",
    "    bias=config.get('bias', 'none'),\n",
    "    use_gradient_checkpointing=config.get('use_gradient_checkpointing', 'unsloth'),\n",
    "    random_state=config.get('random_state', 3407),\n",
    "    use_rslora=config.get('use_rslora', False),\n",
    "    loftq_config=config.get('loftq_config', None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb134c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d544b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Standardizing formats (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [00:00<00:00, 187563.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_data_formats\n",
    "dataset = standardize_data_formats(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f2396b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'What is the modulus operator in programming and how can I use it to calculate the modulus of two given numbers?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'In programming, the modulus operator is represented by the \\'%\\' symbol. It calculates the remainder when one number is divided by another. To calculate the modulus of two given numbers, you can use the modulus operator in the following way:\\n\\n```python\\n# Calculate the modulus\\nModulus = a % b\\n\\nprint(\"Modulus of the given numbers is: \", Modulus)\\n```\\n\\nIn this code snippet, the variables \\'a\\' and \\'b\\' represent the two given numbers for which you want to calculate the modulus. By using the modulus operator \\'%\\', we calculate the remainder when \\'a\\' is divided by \\'b\\'. The result is then stored in the variable \\'Modulus\\'. Finally, the modulus value is printed using the \\'print\\' statement.\\n\\nFor example, if \\'a\\' is 10 and \\'b\\' is 4, the modulus calculation would be 10 % 4, which equals 2. Therefore, the output of the above code would be:\\n\\n```\\nModulus of the given numbers is: 2\\n```\\n\\nThis means that the modulus of 10 and 4 is 2.',\n",
       "   'role': 'assistant'}],\n",
       " 'source': 'infini-instruct-top-500k',\n",
       " 'score': 4.774171352386475}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10d2424f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [00:04<00:00, 20506.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e212b1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'How do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?',\n",
       "  'role': 'user'},\n",
       " {'content': 'Astronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7296673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a875a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = dataset.train_test_split(test_size=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3c5151d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'source', 'score', 'text'],\n",
       "        num_rows: 99600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversations', 'source', 'score', 'text'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b1044",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87c48ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjordinia\u001b[0m (\u001b[33mjordinia-netpro\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title wandb init\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "434ca268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH=all\n",
      "env: WANDB_SILENT=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/fishmon/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_WATCH=all\n",
    "%env WANDB_SILENT=true\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3b1d11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fishmon/AJ/LLM-Finetuning/Malicious-Web/wandb/run-20250504_162544-mighdicf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jordinia-netpro/netpro-finetune/runs/mighdicf' target=\"_blank\">Finetome-100k-test</a></strong> to <a href='https://wandb.ai/jordinia-netpro/netpro-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jordinia-netpro/netpro-finetune' target=\"_blank\">https://wandb.ai/jordinia-netpro/netpro-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jordinia-netpro/netpro-finetune/runs/mighdicf' target=\"_blank\">https://wandb.ai/jordinia-netpro/netpro-finetune/runs/mighdicf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to WANDB!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers.utils import logging\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Loads from .env file automatically\n",
    "\n",
    "# 2. Verify token loading\n",
    "if not os.getenv(\"WANDB_API_KEY\"):\n",
    "    raise ValueError(\"WANDB_API_KEY not found in .env file\")\n",
    "\n",
    "# 3. Initialize and upload\n",
    "os.environ[\"WANDB_API_KEY\"]=os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "logging.set_verbosity_info()\n",
    "project_name = \"netpro-finetune\" \n",
    "entity_name = \"jordinia-netpro\"\n",
    "run_name = \"Finetome-100k-test\" # Set your desired run name\n",
    "\n",
    "# Initialize WANDB (FIXED ENTITY/PROJECT)\n",
    "try:\n",
    "    run = wandb.init(\n",
    "        entity=entity_name,\n",
    "        project=project_name,\n",
    "        name=run_name,\n",
    "        # id=\"j4vh49mi\",\n",
    "        # resume=\"allow\" # Uncomment to resume a previous run\n",
    "    )\n",
    "    print(\"Successfully connected to WANDB!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize WANDB: {str(e)}\")\n",
    "    # Consider exiting if WANDB is critical\n",
    "    # sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "052ed88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "PyTorch: setting up devices\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99600/99600 [00:43<00:00, 2291.14 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:00<00:00, 465.09 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir                  =config.get('output_dir', 'outputs'),\n",
    "    per_device_train_batch_size =config.get('per_device_train_batch_size', 2),\n",
    "    per_device_eval_batch_size  =config.get('per_device_eval_batch_size', 1),\n",
    "    gradient_accumulation_steps =config.get('gradient_accumulation_steps', 4),\n",
    "    warmup_ratio                =config.get('warmup_ratio', 0.05),\n",
    "    max_steps                   =4980,\n",
    "    learning_rate               =2e-5,\n",
    "    fp16                        =not is_bfloat16_supported(),\n",
    "    bf16                        =is_bfloat16_supported(),\n",
    "    optim                       =config.get('optim', 'adamw_8bit'),\n",
    "    weight_decay                =config.get('weight_decay', 0.1),\n",
    "    lr_scheduler_type           =config.get('lr_scheduler_type', 'cosine'),\n",
    "    eval_strategy               =config.get('eval_strategy', 'steps'),\n",
    "    eval_steps                  =config.get('eval_steps', 50),\n",
    "    save_strategy               =config.get('save_strategy', 'steps'),\n",
    "    save_steps                  =config.get('save_steps', 100),\n",
    "    save_total_limit            =config.get('save_total_limit', 3),\n",
    "    logging_steps               =config.get('logging_steps', 10),\n",
    "    seed                        =config.get('seed', 3407),\n",
    "    report_to                   =config.get('report_to', 'wandb'),\n",
    "    load_best_model_at_end      =config.get('load_best_model_at_end', True),\n",
    "    metric_for_best_model       =config.get('metric_for_best_model', 'eval_loss'),\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model               =model,\n",
    "    tokenizer           =tokenizer,\n",
    "    train_dataset       =dataset_dict[\"train\"],\n",
    "    eval_dataset        =dataset_dict[\"test\"],\n",
    "    dataset_text_field  =config.get('dataset_text_field', 'text'),\n",
    "    max_seq_length      =config.get('max_seq_length', 30000),\n",
    "    data_collator       =DataCollatorForSeq2Seq(\n",
    "        tokenizer               =tokenizer,\n",
    "        padding                 =config.get('data_collator', {}).get('padding', True),\n",
    "        pad_to_multiple_of      =config.get('data_collator', {}).get('pad_to_multiple_of', 8),\n",
    "        max_length              =config.get('max_seq_length', 30000),\n",
    "    ),\n",
    "    dataset_num_proc    =config.get('dataset_num_proc', 2),\n",
    "    packing             =config.get('packing', False),\n",
    "    args                =args,\n",
    ")\n",
    "\n",
    "# # Calculate training length (33k samples)\n",
    "# total_samples = 33000  # Your balanced dataset size\n",
    "# batch_size = 2 * 4  # batch_size * gradient_accum\n",
    "# steps_per_epoch = total_samples // batch_size  # ~4125 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cc344",
   "metadata": {},
   "source": [
    "### Train on Completions Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07252b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_on_responses_only_custom(trainer):\n",
    "#     \"\"\"\n",
    "#     Custom version for website classification that:\n",
    "#     1. Preserves system prompt context\n",
    "#     2. Only trains on assistant responses (JSON + reasoning)\n",
    "#     3. Handles length consistency for batching\n",
    "#     \"\"\"\n",
    "#     tokenizer = trainer.tokenizer\n",
    "    \n",
    "#     # Manually define token sequences for your template\n",
    "#     SYSTEM_TOKENS = tokenizer.encode(\n",
    "#         \"<|start_header_id|>system<|end_header_id|>\\n\\n\", \n",
    "#         add_special_tokens=False\n",
    "#     )\n",
    "#     USER_TOKENS = tokenizer.encode(\n",
    "#         \"<|start_header_id|>user<|end_header_id|>\\n\\n\", \n",
    "#         add_special_tokens=False\n",
    "#     )\n",
    "#     ASSISTANT_TOKENS = tokenizer.encode(\n",
    "#         \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \n",
    "#         add_special_tokens=False\n",
    "#     )\n",
    "\n",
    "#     def custom_masking(examples):\n",
    "#         input_ids = examples[\"input_ids\"]\n",
    "        \n",
    "#         # Create labels if they don't exist\n",
    "#         labels = examples.get(\"labels\", [ids.copy() for ids in input_ids])\n",
    "#         new_labels = []\n",
    "        \n",
    "#         for seq_id, seq_labels in zip(input_ids, labels):\n",
    "#             n = len(seq_id)\n",
    "#             mask = [-100] * n\n",
    "#             i = 0\n",
    "            \n",
    "#             while i < n:\n",
    "#                 # Check for system prompt\n",
    "#                 if seq_id[i:i+len(SYSTEM_TOKENS)] == SYSTEM_TOKENS:\n",
    "#                     i += len(SYSTEM_TOKENS)\n",
    "#                     continue\n",
    "                    \n",
    "#                 # Check for user prompt\n",
    "#                 if seq_id[i:i+len(USER_TOKENS)] == USER_TOKENS:\n",
    "#                     i += len(USER_TOKENS)\n",
    "#                     continue\n",
    "                    \n",
    "#                 # Check for assistant prompt\n",
    "#                 if seq_id[i:i+len(ASSISTANT_TOKENS)] == ASSISTANT_TOKENS:\n",
    "#                     start = i\n",
    "#                     i += len(ASSISTANT_TOKENS)\n",
    "                    \n",
    "#                     # Find end of assistant response\n",
    "#                     while i < n:\n",
    "#                         if seq_id[i] == tokenizer.eos_token_id:\n",
    "#                             end = i\n",
    "#                             break\n",
    "#                         i += 1\n",
    "#                     else:\n",
    "#                         end = n\n",
    "                    \n",
    "#                     # Unmask assistant response\n",
    "#                     mask[start:end] = seq_labels[start:end]\n",
    "#                     break\n",
    "                    \n",
    "#                 i += 1\n",
    "            \n",
    "#             # Enforce length matching (critical fix)\n",
    "#             if len(mask) != len(seq_id):\n",
    "#                 mask = mask[:len(seq_id)] + [-100] * (len(seq_id) - len(mask))\n",
    "            \n",
    "#             new_labels.append(mask)\n",
    "        \n",
    "#         return {\"labels\": new_labels}\n",
    "\n",
    "#     # Apply to datasets with length verification\n",
    "#     def apply_masking(dataset):\n",
    "#         dataset = dataset.map(\n",
    "#             custom_masking,\n",
    "#             batched=True,\n",
    "#             batch_size=1000,\n",
    "#             num_proc=4,\n",
    "#         )\n",
    "#         # Verify lengths\n",
    "#         for i in range(min(3, len(dataset))):\n",
    "#             assert len(dataset[i][\"input_ids\"]) == len(dataset[i][\"labels\"]), \\\n",
    "#                 f\"Length mismatch in sample {i}\"\n",
    "#         return dataset\n",
    "    \n",
    "#     trainer.train_dataset = apply_masking(trainer.train_dataset)\n",
    "    \n",
    "#     if trainer.eval_dataset is not None:\n",
    "#         trainer.eval_dataset = apply_masking(trainer.eval_dataset)\n",
    "        \n",
    "#     return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9499b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = train_on_responses_only_custom(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96e2c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final verification (optional)\n",
    "# print(\"\\n=== Final Pre-Train Check ===\")\n",
    "# sample = trainer.train_dataset[0]\n",
    "# print(\"First sample labels preview:\")\n",
    "# print(\"Input IDs length:\", len(sample[\"input_ids\"])) \n",
    "# print(\"Labels length:\", len(sample[\"labels\"]))\n",
    "# print(\"Last 10 labels:\", sample[\"labels\"][-10:])  # Should show unmasked assistant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6841e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a sample from the training set\n",
    "# sample_index = 5  # Try different indices\n",
    "# sample = trainer.train_dataset[sample_index]\n",
    "\n",
    "# # Decode the full input context\n",
    "# print(\"==== Full Input Context ====\")\n",
    "# print(tokenizer.decode(sample[\"input_ids\"]))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Decode the labels with masking visualization\n",
    "# print(\"==== Training Targets (Masked) ====\")\n",
    "# masked_labels = []\n",
    "# for token_id, label_id in zip(sample[\"input_ids\"], sample[\"labels\"]):\n",
    "#     if label_id == -100:\n",
    "#         # Show masked tokens as blank spaces\n",
    "#         masked_labels.append(\" \")\n",
    "#     else:\n",
    "#         # Show actual token\n",
    "#         masked_labels.append(tokenizer.decode([token_id]))\n",
    "\n",
    "# print(\"\".join(masked_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "136a307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = trainer.train_dataset.select(range(2)).with_format(\"torch\")[0]\n",
    "# print(batch[\"input_ids\"])\n",
    "# print(batch[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221f500",
   "metadata": {},
   "source": [
    "### Memory Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7c95376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'source', 'score', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 99600\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b14b138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'Calculate the length of the hypotenuse of a right triangle with right angle.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'To calculate the length of the hypotenuse of a right triangle with its sides known, you can use the Pythagorean theorem. This theorem states that in a right triangle, the square of the length of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the other two sides.\\n\\nThe formula is: cÂ² = aÂ² + bÂ² \\n\\nwhere a and b are the lengths of the legs of the right triangle (the sides adjacent to the right angle), and c is the length of the hypotenuse.\\n\\nOnce you have the values for a and b, you can solve for c by taking the square root of both sides of the equation. So:\\n\\nc = âˆš(aÂ² + bÂ²)\\n\\nThat is how you calculate the length of the hypotenuse of a right triangle with a right angle.',\n",
       "   'role': 'assistant'}],\n",
       " 'source': 'infini-instruct-top-500k',\n",
       " 'score': 3.9990286827087402,\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nCalculate the length of the hypotenuse of a right triangle with right angle.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nTo calculate the length of the hypotenuse of a right triangle with its sides known, you can use the Pythagorean theorem. This theorem states that in a right triangle, the square of the length of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the other two sides.\\n\\nThe formula is: cÂ² = aÂ² + bÂ² \\n\\nwhere a and b are the lengths of the legs of the right triangle (the sides adjacent to the right angle), and c is the length of the hypotenuse.\\n\\nOnce you have the values for a and b, you can solve for c by taking the square root of both sides of the equation. So:\\n\\nc = âˆš(aÂ² + bÂ²)\\n\\nThat is how you calculate the length of the hypotenuse of a right triangle with a right angle.<|eot_id|>'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['test'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d4bd38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3090 Ti. Max memory = 23.551 GB.\n",
      "3.441 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 99,600 | Num Epochs = 1 | Total steps = 4,980\n",
      "O^O/ \\_/ \\    Batch size per device = 5 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (5 x 4 x 1) = 20\n",
      " \"-____-\"     Trainable parameters = 97,255,424/3,000,000,000 (3.24% trained)\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='4980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 139/4980 37:35 < 22:08:15, 0.06 it/s, Epoch 0.03/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.621100</td>\n",
       "      <td>0.753261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.885600</td>\n",
       "      <td>0.740443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.348000</td>\n",
       "      <td>0.699938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.272800</td>\n",
       "      <td>0.596741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.788300</td>\n",
       "      <td>0.404297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.335600</td>\n",
       "      <td>0.216271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.291800</td>\n",
       "      <td>0.134010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.106591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.376600</td>\n",
       "      <td>0.093846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>0.083319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.066653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.067629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.062189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to outputs/checkpoint-100\n",
      "loading configuration file config.json from cache at /home/fishmon/.cache/huggingface/hub/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/aae9d3e87a2c47cc465b6980017a05e1d1d61c8c/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": [\n",
      "      \"lm_head\",\n",
      "      \"multi_modal_projector\",\n",
      "      \"merger\",\n",
      "      \"modality_projection\"\n",
      "    ],\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: conversations, score, text, source. If conversations, score, text, source are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 1\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n",
    "\n",
    "# checkpoint_path = \"./outputs/checkpoint-700\"\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     trainer.train(resume_from_checkpoint=checkpoint_path)\n",
    "# else:\n",
    "#     print(f\"Warning: Checkpoint {checkpoint_path} not found. Starting from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this before the automatic evaluation at step 500\n",
    "trainer.evaluate(eval_dataset=trainer.eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d15042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fc928",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a15012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 1. Max memory: 23.551 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are an Expert Website Classifier tasked with categorizing websites (using provided Domain & Content) into distinct categories: **0 - Benign**, **1 - Gambling**, **2 - Pornography**, or **3 - Harmful**. Output strict JSON including classification, reason, and a point-based confidence score (0-100). This aids a sophisticated website prediction system for digital safety.\\n\\n### **Categories & Definitions:**\\n\\n-   **0 - Benign:** General info, news, entertainment, services, e-commerce, educational sites, blogs, informational pages, and general entertainment sites. No gambling/porn themes.\\n    -   **Includes:** Educational discussion of sensitive topics (drugs, gambling, adult, illegal), suggestive content (dating, lingerie) *without* explicit material, drugs for educational or news purposes (e.g., addiction recovery, medical cannabis research).\\n\\n-   **1 - Gambling:** Promotes/facilitates betting, casino, poker, lottery, wagering. Includes online betting platforms, casinos, poker sites, lottery sites, and any website that encourages or provides means for users to wager money on games of chance or skill.\\n    -   **Keywords:** betting, casino, poker, odds, jackpot, slots, roulette, judi, slot gacor, togel, similar gambling-related terminology within their domain name or content.\\n    \\n-   **2 - Pornography:** These are websites that contain explicit sexual content intended to cause arousal. Explicit sexual content (images, videos, text) for arousal. Immoral content (bestiality, child exploitation). or links to such materials. \\n    -   **Keywords:** porn, bokep, sex, xxx, adult, nude, erotic, explicit video/photo.\\n\\n- **3 - Harmful**: Websites that does not fall to Benign, Gambling, and Pornography category. These websites engage in or promote activities harmful to users or violating laws/regulations, including:\\n    - Malware Distribution: Hosting/downloading computer viruses, worms, ransomware, spyware, etc.\\n    - Cybercrime: Phishing kits, hacking tools, stolen data markets, carding forums.\\n    - Extremism & Terrorism: Content inciting violence, extremist ideologies, or terrorist recruitment.\\n    - Violations of Indonesian Law:\\n        - Insults, defamation, blackmail, or threats.\\n        - Hoaxes/misleading news, hate speech, or incitement of violence.\\n    - Copyright Infringement/Piracy: Illegal software/cracks, torrents, pirated media.\\n    - Drugs/Narcotics: Sale/promotion of illegal drugs (e.g., cocaine, heroin) or unregulated pharmaceuticals.\\n    - Weapons: Sale of illegal firearms, explosives, or weapons.\\n    - Other Illegal Activities: Counterfeit goods, money laundering, human trafficking.\\n    - **Examples:**  \\n        - Illegal: `darknet-drugs.com` (drug sales), `pirated-movies.id` (piracy).  \\n        - Harmful: `extremist-forum.net` (terrorism recruitment), `hackers-tools.org` (phishing kits).  \\n        - Scam/phishng: `hadiah-telkomsel7.blogspot.com` (non-genuine website).\\n    - **Keywords**:\\nmalware, ransomware, phishing, hack, terrorism, jual narkoba, senjata ilegal, konten SARA, berita bohong, ancaman, pembajakan, cracked software, carding, darknet.\\n    - **Exceptions:** \\n        - Licensed/unlicensed gambling â†’ **1 - Gambling**; scams â†’ **3 - Harmful**.\\n\\n\\n### **Input Data Context:**\\n\\nYou will be provided with data entries, each consisting of two primary fields:\\n\\n*   **Domain:** The domain name or URL of the website (e.g., `example.com`, `gamble-site.net`). This can provide hints about the website\\'s purpose.\\n*   **Content:**  The textual content scraped from the website. This content offers detailed information about the website\\'s topics, services, and themes.\\n\\n### **Labeling Instructions:**\\n\\nAnalyze both the **Domain** and the **Content** provided. Use keywords and contextual clues from both to determine the most appropriate category for the website.  Consider the primary purpose and content focus of the website when classifying.\\n\\n### **Confidence Assessment Guidelines: Point-Based System (Total Possible Points: 100)**\\n\\nTo determine the **confidence** level (0-100) for your classification, evaluate the following factors and sum up the points.  The total points will directly correspond to the confidence percentage (e.g., 95 points = 95% confidence).\\n\\n#### **I. Keyword Strength and Relevance (Maximum 40 Points)**\\n*   **(40 Points):  Exceptional Keyword Strength: Explicit and Overwhelming Keywords in BOTH Domain and Content:** Presence of extremely explicit and overwhelmingly strong keywords that are *unquestionably* indicative of a specific category in *both* the domain name AND the website content. These keywords leave absolutely no doubt about the website\\'s nature. (e.g., Domain: `casino-royal-betting.com`, Content:  \"Gamble now and win HUGE jackpots on slots, poker, roulette! Real money betting!\").  This represents the absolute strongest keyword signal possible.\\n*   **(35 Points): Clear and Strong Keywords in BOTH Domain and Content:**  Presence of highly explicit keywords strongly indicative of a specific category in both the domain name AND the website content. (e.g., Domain: `bet.com`, Content:  \"Bet on sports and casino games!\").\\n*   **(25 Points): Strong Keywords in EITHER Domain OR Content:** Presence of highly explicit keywords strongly indicative of a specific category in EITHER the domain name OR the website content, but not both.\\n*   **(15 Points): Some Relevant Keywords:** Presence of keywords related to a category, but they are less explicit, less frequent, or require more contextual interpretation in either domain or content.\\n*   **(0 Points): Weak or Generic Keywords:** Lack of clear category-specific keywords in both domain and content. Keywords are generic and do not strongly suggest any specific category.\\n\\n#### **II. Domain and Content Alignment (Maximum 30 Points)**\\n\\n*   **(30 Points): Strong Domain and Content Alignment:** Domain name strongly and unambiguously suggests a category, and the website content consistently and explicitly reinforces that category.  They tell the same clear story.\\n*   **(15 Points): Partial Domain and Content Alignment:** Domain name and content generally point towards the same category, but the alignment might be less direct, slightly ambiguous, or require some interpretation to connect them.\\n*   **(0 Points): Domain-Content Mismatch or No Alignment:** Domain name suggests one thing, but the content is unclear, suggests something different, or there\\'s no clear connection between the domain and the content\\'s apparent purpose.\\n\\n#### **III. Content Clarity and Unambiguity (Maximum 20 Points)**\\n\\n*   **(20 Points): Unambiguous and Explicit Content:** The website content is very clear, direct, and leaves virtually no room for interpretation. It unambiguously falls into one of the defined categories.\\n*   **(10 Points): Content Requires Some Interpretation:** The content generally points to a category, but requires some interpretation to confidently assign it.  There might be subtle hints, implied meanings, or a need to infer the primary purpose.\\n*   **(0 Points): Ambiguous or Conflicting Content:** The website content is vague, contradictory, or could be reasonably interpreted in multiple ways, making it difficult to confidently assign a category.\\n\\n#### **IV. Category Indicator Strength (Maximum 10 Points)**\\n\\n*   **(10 Points): Multiple Strong Category Indicators:** Presence of numerous strong and clear indicators for a specific category throughout the domain and content (e.g., for Pornography: explicit keywords, descriptions of sexual acts, calls to action to view adult content, age verification prompts).\\n*   **(5 Points): Some Category Indicators Present:** Presence of a few indicators for a category, but they are not overwhelmingly strong or numerous.\\n*   **(0 Points): Lack of Category Indicators:** Few or no clear indicators for any of the defined categories are present in the domain and content.\\n\\n#### **Calculation:**\\n\\n1.  For each of the four sections (I-IV), assess the website and select the point value that best describes the presence of the described factors.\\n2.  Sum up the points from all four sections.\\n3.  The total sum represents the confidence level in percentage (%).\\n\\n#### **Confidence Level Ranges (for reference - already implicitly defined by points):**\\n\\n*   **High Confidence (80-100 Points):**  Strong evidence across multiple factors pointing clearly to a category.\\n*   **Medium Confidence (50-79 Points):** Moderate evidence, some ambiguity or less directness in indicators.\\n*   **Low Confidence (0-49 Points):** Weak or conflicting evidence, high uncertainty about the correct category.\\n\\n#### **Example:**\\n\\nLet\\'s say you are classifying `lucky-slots-online.com` with content about slot games and bonuses.\\n\\n*   **I. Keyword Strength:** Strong keywords in both Domain and Content (e.g., \"slots,\" \"online,\" \"win,\" \"bonuses\") - **25 Points**\\n*   **II. Domain-Content Alignment:** Domain and content strongly align with Gambling - **30 Points**\\n*   **III. Content Clarity:** Content is very clear about gambling activities - **20 Points**\\n*   **IV. Category Indicators:** Multiple indicators of gambling (games, bonuses, calls to action) - **10 Points**\\n\\n**Total Points: 25 + 30 + 20 + 10 = 85 Points.  Confidence: 85%**\\n\\n#### **Using this Point System:**\\n\\nWhen generating the \"reason\" for your classification, you can now also briefly mention the points you assigned for each section to justify the final confidence score. For example:\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Strong keywords in domain and content (25 pts), strong domain-content alignment (30 pts), clear gambling content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n\\n### **Output Requirements: STRICTLY ADHERE TO JSON FORMAT**\\n\\nFor each website, you **MUST** provide your classification in the following **JSON format ONLY**.  **No other format is acceptable.**\\n\\n```json\\n{\\n  \"answer\": (integer: 0, 1, 2, or 3),\\n  \"classification\": (string: \"Benign\", \"Gambling\", \"Pornography\", or \"Harmful\"),\\n  \"reason\": (string: \"Brief explanation of the classification using keywords and hints from the domain and content.\"),\\n  \"confidence\": (integer: 0 to 100, \"Level of confidence in the classification\")\\n}\\n```\\n\\n**Example Input and Expected Output:**\\n\\n**1st Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: news-today.info\\nContent: Welcome to News Today! Get the latest breaking news, top stories, and in-depth analysis from around the world. Covering politics, business, technology, sports, and culture. Stay informed with News Today - your source for reliable journalism.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 0,\\n  \"classification\": \"Benign\",\\n  \"reason\": \"Domain \\'news-today.info\\' and content mention \\'breaking news,\\' \\'top stories,\\' \\'analysis,\\'\\'reliable journalism,\\' indicating a general information/news website. Strong keywords in domain and content (35 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), some category indicators (5 pts). Total 90 points.\",\\n  \"confidence\": 90\\n}\\n```\\n\\n**2nd Sample:**\\n\\n**Input Data:**\\n\\n```\\nDomain: lucky-slots-online.com\\nContent:  Spin to win big at Lucky Slots Online! Play hundreds of exciting slot games, claim your bonuses, and join the fun.  Licensed and regulated for your safety. 24/7 customer support available. Join now and get 100 free spins!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 1,\\n  \"classification\": \"Gambling\",\\n  \"reason\": \"Domain \\'lucky-slots-online.com\\' and content include keywords like\\'slots,\\' \\'win,\\' \\'casino,\\' \\'bonuses,\\' \\'free spins,\\' indicating a gambling website.\",\\n  \"confidence\": 100\\n}\\n```\\n\\n**3rd Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: adult-pleasures.net\\nContent:  Experience the hottest adult entertainment online. Unlimited access to exclusive videos and photos. 18+ only. Join our community of pleasure seekers today!\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 2,\\n  \"classification\": \"Pornography\",\\n  \"reason\": \"Domain \\'adult-pleasures.net\\' and content include explicit terms like \\'adult entertainment,\\' \\'exclusive videos,\\' \\'photos,\\' \\'18+ only,\\' and \\'pleasure seekers,\\' indicating a pornography website. Strong keywords in both domain and content (25 pts), strong domain-content alignment (30 pts), unambiguous content (20 pts), multiple category indicators (10 pts). Total 85 points.\",\\n  \"confidence\": 85\\n}\\n```\\n**4th Sample**\\n\\n**Input Data:**\\n\\n```\\nDomain: project420.com\\nContent:  Welcome to Project420.com an ongoing community marijuana, hemp search engine and your gateway to 420 products, services and information from all over the world. Marijuana seeds, super bongs, high tech growing marijuana books, contests, communities, cool people and tons of 420 fun.  Potent marijuana websites are added frequently.  Let\\'s help each other grow.  Use the table of contents located to the left and begin your search of 420 world wide websites.\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\n  \"answer\": 3,\\n  \"classification\": \"Harmful\",\\n  \"reason\": \"Domain \\'project420.com\\' and content include keywords like\\'marijuana seeds,\\' \\'growing marijuana,\\' and \\'420 products,\\' indicating promotion of illegal drugs under Indonesian law (UU ITE). Strong keywords in content (25 pts), Partial Domain and Content Alignment (15 pts), clear harmful intent (20 pts), multiple indicators (10 pts). Total 70 points.\",\\n  \"confidence\": 70\\n}\\n```\\n**Execution:**\\nProcess the provided website data (input fields: `Domain`, `Content`). Apply the classification categories, definitions, and confidence point system accurately. Generate the mandatory JSON output for each entry. Begin classification now.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nClassify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n \\\\nDomain: jitutogel.net, Content: \\\\\"Whatsapp Telegram Livechat DAFTAR LOGIN BERANDA SLOT GAMES LIVE CASINO POKER ONLINE E-SPORTS ARCADE LOTTERY PROMOTION Jackpot Gaming Playstar TTG Slots Spadegaming RedTiger GMW CQ9 Gaming Live Gaming Evolution Gaming Sexy Gaming SAgaming HOgaming Gameplay OpusGaming Sports Gaming CMD368 SBOBET UBOBET TFGaming Ultraplay SabaEsports JituTogel: Serunya Game Online dengan Hadiah Uang Nyata! Penyedia Games Metode Pembayaran Bank Cimb Niaga BCA Danamon Permata Mandiri BNI BRI Panin Pulsa XL Axiata Tri Telkomsel Axis E-Money QRIS Dana OVO LinkAja Gopay Â©2024 jitutogel. All rights reserved | 18+ LIVECHAT\\\\\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIt seems you\\'ve entered a valid JavaScript code snippet!\\n\\nHere\\'s what happens next:\\n\\n1. Your input is parsed as JavaScript syntax highlighting mode enabled HTML/CSS/javascript snippet `<div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div><div']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# 1. Load checkpoint\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"./outputs/checkpoint-400\",\n",
    "    max_seq_length = 30000,  # Match training config\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# 2. Prepare for inference\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",  # Must match training template\n",
    ")\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "# 3. Load prompts\n",
    "with open('./prompt/labelling_promptv4.txt', 'r') as f:\n",
    "    system_prompt = f.read()\n",
    "with open('./prompt/class_3_sample1.txt', 'r') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# 4. Format input\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n {sample_text}\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    max_length = 30000,  # Prevent OOM\n",
    "    truncation = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 5. Generate\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens = 2048,  \n",
    "    temperature = 0.7,  \n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.2,\n",
    "    eos_token_id        = tokenizer.eos_token_id,\n",
    "    pad_token_id        = tokenizer.pad_token_id,\n",
    "    use_cache = True,\n",
    ")\n",
    "tokenizer.batch_decode(outputs)\n",
    "# print(tokenizer.decode(outputs[0]))                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12225aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the corrected version:\n",
      "\n",
      "I apologize for the mistake earlier. Here's the "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n\u001b[1;32m     34\u001b[0m text_streamer \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer, skip_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_streamer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                   \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/peft/peft_model.py:1875\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1874\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1875\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1877\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:1574\u001b[0m, in \u001b[0;36munsloth_fast_generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;66;03m# Mixed precision autocast\u001b[39;00m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode(), torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[0;32m-> 1574\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1577\u001b[0m \u001b[38;5;66;03m# Return accelerate back\u001b[39;00m\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;66;03m# if accelerate_new_send_to_device is not None:\u001b[39;00m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;66;03m#     accelerate.utils.operations.send_to_device = accelerate_old_send_to_device\u001b[39;00m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;66;03m# pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/transformers/generation/utils.py:3434\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3434\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3436\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3437\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3438\u001b[0m     outputs,\n\u001b[1;32m   3439\u001b[0m     model_kwargs,\n\u001b[1;32m   3440\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3441\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:1026\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_CausalLM_fast_forward\u001b[39m(\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1010\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1024\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1026\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1034\u001b[0m         causal_mask \u001b[38;5;241m=\u001b[39m xformers\u001b[38;5;241m.\u001b[39mattn_bias\u001b[38;5;241m.\u001b[39mLowerTriangularMask() \u001b[38;5;28;01mif\u001b[39;00m HAS_XFORMERS \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:980\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward_inference\u001b[0;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m    972\u001b[0m residual\u001b[38;5;241m.\u001b[39mcopy_(X) \u001b[38;5;66;03m# residual = X\u001b[39;00m\n\u001b[1;32m    973\u001b[0m X \u001b[38;5;241m=\u001b[39m fast_rms_layernorm_inference(\n\u001b[1;32m    974\u001b[0m     decoder_layer\u001b[38;5;241m.\u001b[39mpost_attention_layernorm,\n\u001b[1;32m    975\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    978\u001b[0m     variance \u001b[38;5;241m=\u001b[39m variance,\n\u001b[1;32m    979\u001b[0m )\n\u001b[0;32m--> 980\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mfast_swiglu_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp_gate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemp_gate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp_up\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemp_up\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[1;32m    988\u001b[0m next_decoder_cache\u001b[38;5;241m.\u001b[39mappend(present_key_value)\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/llama.py:299\u001b[0m, in \u001b[0;36mfast_swiglu_inference\u001b[0;34m(self, X, temp_gate, temp_up)\u001b[0m\n\u001b[1;32m    295\u001b[0m bsz, _, hd \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# mlp_size = self.config.intermediate_size\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# temp = torch.empty((2, bsz, 1, mlp_size), dtype = X.dtype, device = \"cuda:0\")\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m gate \u001b[38;5;241m=\u001b[39m \u001b[43mfast_linear_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemp_gate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m up   \u001b[38;5;241m=\u001b[39m fast_linear_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m  up_proj, X, out \u001b[38;5;241m=\u001b[39m temp_up)\n\u001b[1;32m    301\u001b[0m gate \u001b[38;5;241m=\u001b[39m torch_nn_functional_silu(gate, inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/kernels/utils.py:440\u001b[0m, in \u001b[0;36mfast_linear_forward\u001b[0;34m(proj, X, temp_lora, out)\u001b[0m\n\u001b[1;32m    438\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch_matmul(X, W\u001b[38;5;241m.\u001b[39mt(), out \u001b[38;5;241m=\u001b[39m out)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m bsz \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m q_len \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 440\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfast_gemv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     W \u001b[38;5;241m=\u001b[39m fast_dequantize(W\u001b[38;5;241m.\u001b[39mt(), W_quant, use_global_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/kernels/utils.py:346\u001b[0m, in \u001b[0;36mfast_gemv\u001b[0;34m(X, W, quant_state, out)\u001b[0m\n\u001b[1;32m    344\u001b[0m df \u001b[38;5;241m=\u001b[39m torch_empty(absmax\u001b[38;5;241m.\u001b[39mshape, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch_cuda_device(device):\n\u001b[0;32m--> 346\u001b[0m     \u001b[43mcdequantize_blockwise_fp32\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabsmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabsmax2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes_c_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocksize2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes_c_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCUDA_STREAM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     df \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m offset\n\u001b[1;32m    351\u001b[0m     absmax \u001b[38;5;241m=\u001b[39m df\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Load the system prompt from the file\n",
    "with open('./prompt/labelling_promptv4.txt', 'r') as system_file:\n",
    "    system_prompt = system_file.read()\n",
    "\n",
    "# Load the label from the file\n",
    "with open('./prompt/class_3_sample2.txt', 'r') as label_file:\n",
    "    label = label_file.read()\n",
    "\n",
    "# Define the messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n{label}\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(input_ids = inputs, max_new_tokens = 2048, use_cache = True,\n",
    "#                          temperature = 0.7, min_p = 0.1)\n",
    "# tokenizer.batch_decode(outputs)\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 2048,\n",
    "                   use_cache = True, temperature = 1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3facb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A question that gets to the heart of our planet's atmosphere!\n",
      "\n",
      "The sky appears blue because of a phenomenon called scattering, which occurs when sunlight interacts with atmospheric gases.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. Sunlight enters Earth's atmosphere and contains all colors of visible light, except blue-violet.\n",
      "2. When sunlight encounters atmospheric gases like nitrogen and oxygen, shorter wavelengths (like violet) are scattered in all directions.\n",
      "3. However, blue light has a longer wavelength and is scattered at lower angles (about 90 degrees) compared to violet.\n",
      "4. Because blue light is scattered at lower angles, our eyes perceive it as the dominant color\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Why is the sky is blue\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee7f367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're referring to the classic poem by Edward Taylor!\n",
      "\n",
      "\"The Sky Above the Sea, or Heaven\"\n",
      "\n",
      "\"In heaven's name there's no shame\n",
      "But for shame's sake there's no shame\n",
      "For heaven's sake there's no shame\n",
      "But for shame's sake there's no shame\n",
      "But heaven's sake there's no shame\n",
      "But for shame's sake there's no shame\n",
      "In heaven's name there's no shame\"\n",
      "\n",
      "While the poem itself doesn't explicitly say why the sky is blue, it does describe the sky in heaven, where there is no shame.\n",
      "\n",
      "However, if we're looking for a scientific explanation for why the sky appears blue, it's because of the way light behaves when it passes through water droplets or clouds in the Earth's atmosphere.\n",
      "\n",
      "Light comes in various colors, including blue. When light travels through a medium like air or water, shorter wavelengths like blue are scattered more than longer wavelengths like red. This phenomenon is called Rayleigh scattering.\n",
      "\n",
      "On Earth, our atmosphere is composed of gases, including nitrogen and oxygen, and particles like pollutants and dust. These particles scatter light in all directions, including downwards from clouds or water droplets.\n",
      "\n",
      "When sunlight enters Earth's atmosphere, it encounters these particles. As light travels through the atmosphere, shorter wavelengths like blue are scattered more than longer wavelengths like red.\n",
      "\n",
      "This scattered light appears blue because of its shorter wavelength, and our eyes perceive blue light. Blue appears to be everywhere because light with blue wavelengths penetrates all around.\n",
      "\n",
      "That's a science-based answer to why the sky appears blue!\n",
      "\n",
      "However, if you'd like an answer to the original question that asks why the sky appears blue? \n",
      "\n",
      "\"The sky is blue because light with blue wavelengths penetrates all around.\"\n",
      "\n",
      "(Note: Blue appears everywhere because light with blue wavelengths penetrates all around.)\n",
      "\n",
      "However, it is worth noting that blue does not appear everywhere. It appears everywhere because light with blue wavelengths penetrates all around.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Why is the sky is blue\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 2048,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a39afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Why is the sky is blue\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0197b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Load the system prompt from the file\n",
    "with open('./prompt/labelling_promptv4.txt', 'r') as system_file:\n",
    "    system_prompt = system_file.read()\n",
    "\n",
    "# Load the label from the file\n",
    "with open('./prompt/class_3_sample2.txt', 'r') as label_file:\n",
    "    label = label_file.read()\n",
    "\n",
    "# Define the messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n{label}\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 4096,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7fada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
