# Model Configuration
model_name: "unsloth/Llama-3.2-3B-Instruct-bnb-4bit"
load_in_4bit: True
max_seq_length: 30000

# LoRA Configuration
lora_r: 64
lora_alpha: 256
lora_dropout: 0.1
target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
bias: "none"
use_gradient_checkpointing: "unsloth"

# Training Configuration
output_dir: "outputs"
per_device_train_batch_size: 4
per_device_eval_batch_size: 1
gradient_accumulation_steps: 4
warmup_ratio: 0.05
max_steps: 2100
learning_rate: 2e-5
optim: "adamw_8bit"
weight_decay: 0.1
lr_scheduler_type: "cosine"
eval_strategy: "steps"
eval_steps: 10
save_strategy: "steps"
save_steps: 100
save_total_limit: 5
logging_steps: 10
seed: 3407
report_to: "wandb"
load_best_model_at_end: True
metric_for_best_model: "eval_loss"

# Data Processing
dataset_num_proc: 2
packing: False
data_collator:
  padding: True
  pad_to_multiple_of: 8