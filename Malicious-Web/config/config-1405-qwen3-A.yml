# COMPLETIONS ONLY
# Model Configuration
model_name: "unsloth/Qwen3-4B-unsloth-bnb-4bit"
load_in_4bit: True
max_seq_length: 30000 

# LoRA Configuration
lora_r: 64
lora_alpha: 64          
lora_dropout: 0.2        
target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lm_head"]
bias: "none"
use_gradient_checkpointing: "unsloth" 
random_state: 3407       

# Training Configuration
output_dir: "outputs/qwen3-1405-A" 
per_device_train_batch_size: 2
per_device_eval_batch_size: 1     
gradient_accumulation_steps: 4    
warmup_ratio: 0.1                 
max_steps: 906                    
learning_rate: 1e-5               
optim: "adamw_8bit"
weight_decay: 0.1                
lr_scheduler_type: "linear"
eval_strategy: "steps"
eval_steps: 50                                        
save_strategy: "steps"
save_steps: 100                   
save_total_limit: 10               
logging_steps: 10                
seed: 3407
report_to: "wandb"                
load_best_model_at_end: True
metric_for_best_model: "eval_loss"
lora_model: "model/netpro-qwen3-1405-lora-A"           

# Data Processing
dataset_num_proc: 2
packing: False
data_collator:
  padding: True
  pad_to_multiple_of: 8               