{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "# Text classification with Unsloth\n",
        "\n",
        "This modified Unsloth notebook trains an LLM on any text classification dataset, where the input is a csv with columns \"text\" and \"label\".\n",
        "\n",
        "### Added features:\n",
        "\n",
        "- Trims the classification head to contain only the number tokens such as \"1\", \"2\" etc, which saves 1 GB of VRAM, allows you to train the head without massive memory usage, and makes the start of the training session more stable.\n",
        "- Only the last token in the sequence contributes to the loss, the model doesn't waste its capacity by trying to predict the input\n",
        "- includes \"group_by_length = True\" which speeds up training significantly for unbalanced sequence lengths\n",
        "- Efficiently evaluates the accuracy on the validation set using batched inference\n",
        "\n",
        "### Update 4th of May 2025:\n",
        "\n",
        "- Added support for more than 2 classes\n",
        "- The classification head is now built back up to the original size after training, no more errors in external libraries.\n",
        "- Made the batched inference part much faster and cleaner\n",
        "- Changed model to Qwen 3\n",
        "- Improved comments to explain the complicated parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5au6tB3nqm-4",
        "outputId": "a266cdf3-8bd5-4970-b572-fbff526da04b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jordinia/miniconda3/envs/unsloth_env310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "# needed as this function doesn't like it when the lm_head has its size changed\n",
        "from unsloth import tokenizer_utils\n",
        "def do_nothing(*args, **kwargs):\n",
        "    pass\n",
        "tokenizer_utils.fix_untrained_tokens = do_nothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Major: 8, Minor: 9\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "print(f\"Major: {major_version}, Minor: {minor_version}\")\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "from trl import SFTTrainer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from typing import Tuple\n",
        "import warnings\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "b169f96af8e048b290f2a706bf447dc9"
          ]
        },
        "id": "52gIVysSqm-4",
        "outputId": "f33e9c8a-a27f-447f-cbd6-f19fd7ae86aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4070 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.40s/it]\n",
            "Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        }
      ],
      "source": [
        "NUM_CLASSES = 4 # number of classes in the csv\n",
        "\n",
        "max_seq_length = 24000 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "\n",
        "# model_name = \"unsloth/Qwen3-0.6B-Base\";load_in_4bit = False\n",
        "# model_name = \"unsloth/Qwen3-1.7B-Base\";load_in_4bit = False\n",
        "model_name = \"unsloth/Qwen3-4B-Base\";load_in_4bit = False\n",
        "# model_name = \"Qwen3-4B-Base\";load_in_4bit = False\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,load_in_4bit = load_in_4bit,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now trim the classification head so the model can only say numbers 0-NUM_CLASSES and no other words. (We don't use 0 here but keeping it makes everything simpler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0P-u-f4qm-5",
        "outputId": "e83ad36c-19de-4137-f61d-5f8780c63434"
      },
      "outputs": [],
      "source": [
        "number_token_ids = []\n",
        "for i in range(0, NUM_CLASSES+1):\n",
        "    number_token_ids.append(tokenizer.encode(str(i), add_special_tokens=False)[0])\n",
        "# keep only the number tokens from lm_head\n",
        "par = torch.nn.Parameter(model.lm_head.weight[number_token_ids, :])\n",
        "\n",
        "old_shape = model.lm_head.weight.shape\n",
        "old_size = old_shape[0]\n",
        "print(par.shape)\n",
        "print(old_shape)\n",
        "\n",
        "model.lm_head.weight = par\n",
        "\n",
        "reverse_map = {value: idx for idx, value in enumerate(number_token_ids)} # will be used later to convert an idx from the old tokenizer to the new lm_head\n",
        "reverse_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxtb4LgBqm-5",
        "outputId": "4320934b-6c1d-48c5-dfb2-39250fb0f282"
      },
      "outputs": [],
      "source": [
        "from peft import LoftQConfig\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\n",
        "        \"lm_head\", # can easily be trained because it now has a small size\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # We support rank stabilized LoRA\n",
        "    # init_lora_weights = 'loftq',\n",
        "    # loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1), # And LoftQ\n",
        ")\n",
        "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(r\"C:\\Users\\rizky\\Documents\\Projects\\LLM-Finetuning\\Malicious-Web\\dataset\\netpro_raw_25k_train.csv\")\n",
        "val_df = pd.read_csv(r\"C:\\Users\\rizky\\Documents\\Projects\\LLM-Finetuning\\Malicious-Web\\dataset\\netpro_raw_25k_val.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(train_df), type(val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDXHFH-eqm-5",
        "outputId": "cca19aee-ca94-4a94-c411-8046379db26f"
      },
      "outputs": [],
      "source": [
        "print(len(train_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytTbrzM9qm-5",
        "outputId": "76c1eb41-d5ee-43e1-d897-de1a014c9b1d"
      },
      "outputs": [],
      "source": [
        "token_counts = [len(tokenizer.encode(x)) for x in train_df.Content]\n",
        "# plot the token counts\n",
        "a = plt.hist(token_counts, bins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ClfC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "CLASSIFY_CONTENT_ONLY = False  # Set to False to use Domain + Content\n",
        "\n",
        "if CLASSIFY_CONTENT_ONLY:\n",
        "    prompt = \"\"\"You are an expert Website Classifier.\n",
        "\n",
        "Website Content: {}\n",
        "\n",
        "Classify the website based on its content into one of the following categories:\n",
        "- 0: Benign (general info, news, safe entertainment, educational, marketplace, social media, etc.)\n",
        "- 1: Gambling (betting, casino, lottery, real money games, judi, slot)\n",
        "- 2: Pornography (explicit sexual content, adult themes, nudity, sexual, bokep)\n",
        "- 3: Harmful (malware, cybercrime, illegal activities, firearms, extremism, drugs, narcotics, phishing, scams, counterfeit, hacking tools, stolen data markets, carding)\n",
        "\n",
        "SOLUTION\n",
        "The correct answer is: class {}\"\"\"\n",
        "\n",
        "    def formatting_prompts_func(dataset_):\n",
        "        texts = []\n",
        "        for i in range(len(dataset_['Content'])):\n",
        "            content_ = dataset_['Content'].iloc[i]\n",
        "            label_ = dataset_['Label'].iloc[i]\n",
        "            text = prompt.format(content_, label_)\n",
        "            texts.append(text)\n",
        "        return texts\n",
        "\n",
        "else:\n",
        "    prompt = \"\"\"You are an expert Website Classifier.\n",
        "\n",
        "Domain: {}\n",
        "Website Content: {}\n",
        "\n",
        "Classify the website based on its content into one of the following categories:\n",
        "- 0: Benign (general info, news, safe entertainment, educational, marketplace, social media, etc.)\n",
        "- 1: Gambling (betting, casino, lottery, real money games, judi, slot)\n",
        "- 2: Pornography (explicit sexual content, adult themes, nudity, sexual, bokep)\n",
        "- 3: Harmful (malware, cybercrime, illegal activities, firearms, extremism, drugs, narcotics, phishing, scams, counterfeit, hacking tools, stolen data markets, carding)\n",
        "\n",
        "SOLUTION\n",
        "The correct answer is: class {}\"\"\"\n",
        "\n",
        "    def formatting_prompts_func(dataset_):\n",
        "        texts = []\n",
        "        for i in range(len(dataset_['Content'])):\n",
        "            domain_ = dataset_['Domain'].iloc[i]\n",
        "            content_ = dataset_['Content'].iloc[i]\n",
        "            label_ = dataset_['Label'].iloc[i]\n",
        "            text = prompt.format(domain_, content_, label_)\n",
        "            texts.append(text)\n",
        "        return texts\n",
        "\n",
        "# apply formatting_prompts_func to train_df\n",
        "train_df['text'] = formatting_prompts_func(train_df)\n",
        "train_dataset = datasets.Dataset.from_pandas(train_df, preserve_index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset['text'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtG1Iaemqm-6"
      },
      "outputs": [],
      "source": [
        "# this custom collator makes it so the model trains only on the last token of the sequence. It also maps from the old tokenizer to the new lm_head indices\n",
        "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *args,\n",
        "        mlm: bool = False,\n",
        "        ignore_index: int = -100,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, mlm=mlm, **kwargs)\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
        "        batch = super().torch_call(examples)\n",
        "\n",
        "        for i in range(len(examples)):\n",
        "            # Find the last non-padding token\n",
        "            last_token_idx = (batch[\"labels\"][i] != self.ignore_index).nonzero()[-1].item()\n",
        "            # Set all labels to ignore_index except for the last token\n",
        "            batch[\"labels\"][i, :last_token_idx] = self.ignore_index\n",
        "            # If the last token in the text is, for example, \"2\", then this was processed with the old tokenizer into number_token_ids[2]\n",
        "            # But we don't actually want this because number_token_ids[2] could be something like 27, which is now undefined in the new lm_head. So we map it to the new lm_head index.\n",
        "            # if this line gives you a keyerror then increase max_seq_length\n",
        "            batch[\"labels\"][i, last_token_idx] = reverse_map[ batch[\"labels\"][i, last_token_idx].item() ]\n",
        "\n",
        "\n",
        "        return batch\n",
        "collator = DataCollatorForLastTokenLM(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "477d5041a08f4a3a9f7cfa1d98ab48ff",
            "25eecdf89b8845a989ce2c8c4d9edbeb",
            "e02470ee64ad4f0fb2160b088cdcba7f",
            "b7c0858a80684aed9c19ce00dda85815",
            "d0b82646d6f549d7ad59e9ff5f426e88",
            "d94a8d1f9d294abbb156e39da065910f",
            "ebb0f92e750447a39ff23a23ea73b445",
            "d4d01dc3290b4174ab13454a28faf972",
            "c6941f8c60ef49a8a79ed265c46652c2",
            "24aa300026334db1b545bf0fa906112e",
            "6d0d021108b54147a62e50fea1ba88ea",
            "deb2982b19764ed5aec0b4d80e776279",
            "c24904c0a7294f3a93bb64aa38e70316",
            "26a77ab74a4a4e21b9afd9798a9f9a29",
            "7189bac8d0474bcea50cf8711259516c",
            "ce81350896a44331aa6b6960b7370325",
            "73d4af57ddc64fb3afd0e2ab068cbcb4",
            "672d990adee44df6b58e27fe5804986f",
            "3fe95fc9bc034a2db85ed19aedfd4250",
            "c1c0053b8e674ed6ac81316d4af82c48",
            "74a0e53405cd4d64bd597ad5461ed5dd",
            "278d35f6e08d45e2a49c3509dd442f0c",
            "b19d83c6f5ad4f04941e6a684678ac07",
            "88e100a175e64a3dab6f772847deffd6",
            "457b5df3a4294c8a966e233d34c15a82",
            "18af2c44a24b4aaabfd192e3fc4bf655",
            "61944d9473394be5b19cfd48fd504481",
            "68d18369acc540a594aef61a2de07e63",
            "69c676782e0b496b820b55c45424177d",
            "08dcaabc623c4759a00fbee5430e3ba9",
            "45a3bcaffc3f4184b9ae869f7b0ccef4",
            "c45f02fbb40e4041ba7f95074f8e1e82",
            "0fc1037a17e541eba92a2d6f400ac6eb",
            "7bd2cc9aa724408fa9e70795744cad85",
            "fb0588bffd7a4238bac3f73052e09335",
            "68ff2006b3794e23b4ccbc2c83c52b9b",
            "910f2e6fffd24cd7b6e68c932c2a2524",
            "6345dc6f40c6444aa05ed2aba7809a3c",
            "92eab7fadbed4bc2bc2935b4e3d800cf",
            "2dfef2cd79c8463cb7eadad6a04aba91",
            "3be5c37493e742aebf0aa29b6723283b",
            "273be47263384901b6cf9f249ee3409d",
            "2ff515b72bbb43c889e2172c83933803",
            "a1cd830a712d490181d176267ce7b6f0",
            "1a8da471604841ec9f6c8e0073471c00",
            "f899ae16708544379d63960be07b7c32",
            "0bdbf9b92f7b4925a5ac28df93fd0fb0",
            "c1d8820a789f4899a839e6d28a4f333c",
            "e711d7f85eee4fe195fad9fbddcfece2",
            "56ed5fd876d94ebbaa8b4e905c348a0d",
            "5d461f10bdc44c6b95d070fa9d7425d1",
            "f4e0e7a39ad9484f930e6673c35c2f5d",
            "baad9118cade4cee9600a7fbf6426e0a",
            "16fac1aad22444c4ad42ad0e6e1dfdc9",
            "ac35368de2d746b4bed736459d187dbd",
            "3560a34c6f3147a0b92e8ec1ff0368dd"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "adb8cb5d-0ec3-4b79-83a7-5691873873e8"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 1,\n",
        "    packing = False, # not needed because group_by_length is True\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_steps = 10,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs/NetPro-Qwen3-4B-ClfDC\",\n",
        "        num_train_epochs = 1,\n",
        "        # report_to = \"wandb\",\n",
        "        report_to = \"none\",\n",
        "        group_by_length = True,\n",
        "    ),\n",
        "    data_collator=collator,\n",
        "    dataset_text_field=\"text\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "815b67fb-14a2-43ab-d587-3cbe038b4349"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "16039f41-2abb-44e6-f020-4e1d4fcc0102"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "ff1b0842-5966-4dc2-bd98-c20832526b31"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "This part evaluates the model on the val set with batched inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_pretrained(\"model/NetPro-Qwen3-4B-ClfDC\")  # Local saving\n",
        "tokenizer.save_pretrained(\"model/NetPro-Qwen3-4B-ClfDC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv() \n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if not HF_TOKEN:\n",
        "    raise ValueError(\"HF_TOKEN not found in .env file\")\n",
        "\n",
        "# Now push to hub using the cleaned name and loaded token\n",
        "model.push_to_hub(\"NetPro-Qwen3-4B-ClfDC\", token=HF_TOKEN)\n",
        "tokenizer.push_to_hub(\"NetPro-Qwen3-4B-ClfDC\", token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# stop running all cells\n",
        "1/0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Giufn78Pqm-6",
        "outputId": "e3638156-14f9-4972-8f52-f211a12ff2c5"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNUh3lfnqm-7"
      },
      "source": [
        "### remake the old lm_head but with unused tokens having -1000 bias and 0 weights (improves compatibility with libraries like vllm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-Mm8Synqm-7",
        "outputId": "0828c397-80b6-44b1-c098-6e0918371f31"
      },
      "outputs": [],
      "source": [
        "# Save the current (trimmed) lm_head and bias\n",
        "trimmed_lm_head = model.lm_head.weight.data.clone()\n",
        "trimmed_lm_head_bias = model.lm_head.bias.data.clone() if hasattr(model.lm_head, \"bias\") and model.lm_head.bias is not None else torch.zeros(len(number_token_ids), device=trimmed_lm_head.device)\n",
        "\n",
        "# Create a new lm_head with shape [old_size, hidden_dim]\n",
        "hidden_dim = trimmed_lm_head.shape[1]\n",
        "new_lm_head = torch.full((old_size, hidden_dim), 0, dtype=trimmed_lm_head.dtype, device=trimmed_lm_head.device)\n",
        "new_lm_head_bias = torch.full((old_size,), -1000.0, dtype=trimmed_lm_head_bias.dtype, device=trimmed_lm_head_bias.device)\n",
        "\n",
        "# Fill in the weights and bias for the allowed tokens (number_token_ids)\n",
        "for new_idx, orig_token_id in enumerate(number_token_ids):\n",
        "    new_lm_head[orig_token_id] = trimmed_lm_head[new_idx]\n",
        "    new_lm_head_bias[orig_token_id] = trimmed_lm_head_bias[new_idx]\n",
        "\n",
        "# Update the model's lm_head weight and bias\n",
        "with torch.no_grad():\n",
        "    new_lm_head_module = torch.nn.Linear(hidden_dim, old_size, bias=True, device=model.device)\n",
        "    new_lm_head_module.weight.data.copy_(new_lm_head)\n",
        "    new_lm_head_module.bias.data.copy_(new_lm_head_bias)\n",
        "    model.lm_head.modules_to_save[\"default\"] = new_lm_head_module\n",
        "\n",
        "print(f\"Remade lm_head: shape = {model.lm_head.weight.shape}. Allowed tokens: {number_token_ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-kTdxlXqm-7"
      },
      "source": [
        "# Batched Inference on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3woGtEqjqm-7",
        "outputId": "8d03e750-21ea-4aa9-f25b-8cd050db9544"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Prepare inference prompt\n",
        "inference_prompt_template = prompt.split(\"class {}\")[0] + \"class \"\n",
        "\n",
        "# Sort validation set by length for efficient batching\n",
        "val_df['token_length'] = val_df['Content'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=False)))\n",
        "val_df_sorted = val_df.sort_values(by='token_length').reset_index(drop=True)\n",
        "\n",
        "display = 50\n",
        "batch_size = 16\n",
        "device = model.device\n",
        "correct = 0\n",
        "results = []\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for i in tqdm(range(0, len(val_df_sorted), batch_size), desc=\"Evaluating\"):\n",
        "        batch = val_df_sorted.iloc[i:i+batch_size]\n",
        "        prompts = [inference_prompt_template.format(text) for text in batch['Content']]\n",
        "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_length).to(device)\n",
        "        logits = model(**inputs).logits\n",
        "        last_idxs = inputs.attention_mask.sum(1) - 1\n",
        "        last_logits = logits[torch.arange(len(batch)), last_idxs, :]\n",
        "        probs_all = F.softmax(last_logits, dim=-1)\n",
        "        probs = probs_all[:, number_token_ids] # only keep the logits for the number tokens\n",
        "        preds = torch.argmax(probs, dim=-1).cpu().numpy() # looks like [1 1 1 1 3 1 3 1 3 1 1 1 1 2 2 3]\n",
        "\n",
        "        true_labels = batch['Label'].tolist()\n",
        "        correct += sum([p == t for p, t in zip(preds, true_labels)])\n",
        "        # Store a few samples for display\n",
        "        for j in range(len(batch)):\n",
        "            results.append({\n",
        "                \"text\": batch['text'].iloc[j][:200],\n",
        "                \"true\": true_labels[j],\n",
        "                \"pred\": preds[j],\n",
        "                \"probs\": probs[j][1:].float().cpu().numpy(), # ignore prob for class 0 and convert from tensor to float\n",
        "                \"ok\": preds[j] == true_labels[j]\n",
        "            })\n",
        "\n",
        "accuracy = 100 * correct / len(val_df_sorted)\n",
        "print(f\"\\nValidation accuracy: {accuracy:.2f}% ({correct}/{len(val_df_sorted)})\")\n",
        "\n",
        "print(\"\\n--- Random samples ---\")\n",
        "for s in random.sample(results, min(display, len(results))):\n",
        "    print(f\"\\nText: {s['text']}\")\n",
        "    print(f\"True: {s['true']}  Pred: {s['pred']} {'‚úÖ' if s['ok'] else '‚ùå'}\")\n",
        "    print(\"Probs:\", \", \".join([f\"{k}: {v:.3f}\" for k, v in enumerate(s['probs'], start=1)]))\n",
        "\n",
        "# Clean up\n",
        "if 'token_length' in val_df:\n",
        "    del val_df['token_length']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJcjBTqnqm-7"
      },
      "source": [
        "Now if you closed the notebook kernel and want to reload the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'NetPro-Qwen3-0.6B-ClfDC'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 19 (delta 1), reused 0 (delta 0), pack-reused 6 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (19/19), 1.72 MiB | 1.25 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/jordinia/NetPro-Qwen3-0.6B-ClfDC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# --- Configuration (should match your training setup) ---\n",
        "base_model_name = \"unsloth/Qwen3-4B-Base\" # The original base model\n",
        "load_in_4bit_at_load_time = False # Matches your inference script\n",
        "max_seq_length_at_load_time = 24000 # Matches your inference script\n",
        "dtype_at_load_time = None # Matches your inference script\n",
        "\n",
        "checkpoint_path = \"./outputs/netpro/checkpoint-6252\"\n",
        "NUM_CLASSES = 4 # Same as during training\n",
        "\n",
        "# --- 1. Load the original base model ---\n",
        "print(f\"Loading base model: {base_model_name}...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=base_model_name,\n",
        "    max_seq_length=max_seq_length_at_load_time,\n",
        "    dtype=dtype_at_load_time,\n",
        "    load_in_4bit=load_in_4bit_at_load_time,\n",
        ")\n",
        "print(\"Base model loaded.\")\n",
        "\n",
        "# --- 2. Re-apply the lm_head modification (EXACTLY as done in training) ---\n",
        "print(\"Modifying lm_head to match training setup...\")\n",
        "number_token_ids = []\n",
        "for i in range(0, NUM_CLASSES+1):\n",
        "    number_token_ids.append(tokenizer.encode(str(i), add_special_tokens=False)[0])\n",
        "# keep only the number tokens from lm_head\n",
        "par = torch.nn.Parameter(model.lm_head.weight[number_token_ids, :])\n",
        "\n",
        "old_shape = model.lm_head.weight.shape\n",
        "old_size = old_shape[0]\n",
        "print(par.shape)\n",
        "print(old_shape)\n",
        "\n",
        "model.lm_head.weight = par\n",
        "\n",
        "reverse_map = {value: idx for idx, value in enumerate(number_token_ids)} # will be used later to convert an idx from the old tokenizer to the new lm_head\n",
        "reverse_map\n",
        "\n",
        "# --- 3. Load the LoRA adapter from the specific checkpoint ---\n",
        "# Now that the model's lm_head has the correct (shrunken) shape,\n",
        "# PeftModel can load the adapter weights without a size mismatch.\n",
        "print(f\"Loading LoRA adapter from: {checkpoint_path}\")\n",
        "model = PeftModel.from_pretrained(\n",
        "    model, # The base model WITH THE MODIFIED lm_head\n",
        "    checkpoint_path,\n",
        "    is_trainable=False\n",
        ")\n",
        "print(\"LoRA adapter loaded successfully.\")\n",
        "\n",
        "# --- lm head ---\n",
        "# Save the current (trimmed) lm_head and bias\n",
        "trimmed_lm_head = model.lm_head.weight.data.clone()\n",
        "trimmed_lm_head_bias = model.lm_head.bias.data.clone() if hasattr(model.lm_head, \"bias\") and model.lm_head.bias is not None else torch.zeros(len(number_token_ids), device=trimmed_lm_head.device)\n",
        "\n",
        "# Create a new lm_head with shape [old_size, hidden_dim]\n",
        "hidden_dim = trimmed_lm_head.shape[1]\n",
        "new_lm_head = torch.full((old_size, hidden_dim), 0, dtype=trimmed_lm_head.dtype, device=trimmed_lm_head.device)\n",
        "new_lm_head_bias = torch.full((old_size,), -1000.0, dtype=trimmed_lm_head_bias.dtype, device=trimmed_lm_head_bias.device)\n",
        "\n",
        "# Fill in the weights and bias for the allowed tokens (number_token_ids)\n",
        "for new_idx, orig_token_id in enumerate(number_token_ids):\n",
        "    new_lm_head[orig_token_id] = trimmed_lm_head[new_idx]\n",
        "    new_lm_head_bias[orig_token_id] = trimmed_lm_head_bias[new_idx]\n",
        "\n",
        "# Update the model's lm_head weight and bias\n",
        "with torch.no_grad():\n",
        "    new_lm_head_module = torch.nn.Linear(hidden_dim, old_size, bias=True, device=model.device)\n",
        "    new_lm_head_module.weight.data.copy_(new_lm_head)\n",
        "    new_lm_head_module.bias.data.copy_(new_lm_head_bias)\n",
        "    model.lm_head.modules_to_save[\"default\"] = new_lm_head_module\n",
        "\n",
        "print(f\"Remade lm_head: shape = {model.lm_head.weight.shape}. Allowed tokens: {number_token_ids}\")\n",
        "\n",
        "# --- 4. Prepare for inference ---\n",
        "FastLanguageModel.for_inference(model) # Unsloth's optimization for inference\n",
        "print(\"Model prepared for inference.\")\n",
        "\n",
        "# --- 5. Your Inference Prompt and Generation ---\n",
        "prompt_template = \"\"\"You are an expert Website Classifier.\n",
        "\n",
        "Website Content: {}\n",
        "\n",
        "Classify the website based on its content into one of the following categories:\n",
        "- 0: Benign (general info, news, safe entertainment, educational, marketplace, social media, etc.)\n",
        "- 1: Gambling (betting, casino, lottery, real money games, judi, slot)\n",
        "- 2: Pornography (explicit sexual content, adult themes, nudity, sexual, bokep)\n",
        "- 3: Harmful (malware, cybercrime, illegal activities, firearms, extremism, drugs, narcotics, phishing, scams, counterfeit, hacking tools, stolen data markets, carding)\n",
        "\n",
        "SOLUTION\n",
        "The correct answer is: class \"\"\" # Note: Removed the final {} for inference\n",
        "\n",
        "website_content_example = \"Whatsapp Telegram Livechat DAFTAR LOGIN BERANDA SLOT GAMES LIVE CASINO POKER ONLINE E-SPORTS ARCADE LOTTERY PROMOTION Jackpot Gaming Playstar TTG Slots Spadegaming RedTiger GMW CQ9 Gaming Live Gaming Evolution Gaming Sexy Gaming SAgaming HOgaming Gameplay OpusGaming Sports Gaming CMD368 SBOBET UBOBET TFGaming Ultraplay SabaEsports JituTogel: Serunya Game Online dengan Hadiah Uang Nyata! Penyedia Games Metode Pembayaran Bank Cimb Niaga BCA Danamon Permata Mandiri BNI BRI Panin Pulsa XL Axiata Tri Telkomsel Axis E-Money QRIS Dana OVO LinkAja Gopay ¬©2024 jitutogel. All rights reserved | 18+ LIVECHAT\"\n",
        "# Format the prompt for inference (model should generate the class number)\n",
        "full_prompt_for_inference = prompt_template.format(website_content_example)\n",
        "\n",
        "inputs = tokenizer(full_prompt_for_inference, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "print(\"Generating output...\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=1, use_cache=True, pad_token_id=tokenizer.eos_token_id) # Added pad_token_id\n",
        "# For classification, you typically want deterministic output, so low/zero temperature:\n",
        "# outputs = model.generate(**inputs, max_new_tokens=1, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(\"Full decoded output:\", decoded_outputs)\n",
        "\n",
        "# Extract just the newly generated token\n",
        "generated_sequence = outputs[0]\n",
        "input_length = inputs.input_ids.shape[1]\n",
        "newly_generated_tokens = generated_sequence[input_length:]\n",
        "predicted_class_token = tokenizer.decode(newly_generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(f\"Predicted class token: '{predicted_class_token}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base model: unsloth/Qwen3-0.6B-Base...\n",
            "==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4070 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Base model loaded.\n",
            "Modifying lm_head to match training setup...\n",
            "torch.Size([5, 1024])\n",
            "torch.Size([151936, 1024])\n",
            "Loading LoRA adapter from: ./model/NetPro-Qwen3-0.6B-ClfDC\n",
            "LoRA adapter loaded successfully.\n",
            "Remade lm_head: shape = torch.Size([151936, 1024]). Allowed tokens: [15, 16, 17, 18, 19]\n",
            "Model prepared for inference.\n",
            "Generating output...\n",
            "Full decoded output: ['You are an expert Website Classifier.\\n\\nDomain: example.com \\nWebsite Content: sample content \\n\\nClassify the website based on its content into one of the following categories:\\n- 0: Benign (general info, news, safe entertainment, educational, marketplace, social media, etc.)\\n- 1: Gambling (betting, casino, lottery, real money games, judi, slot)\\n- 2: Pornography (explicit sexual content, adult themes, nudity, sexual, bokep)\\n- 3: Harmful (malware, cybercrime, illegal activities, firearms, extremism, drugs, narcotics, phishing, scams, counterfeit, hacking tools, stolen data markets, carding)\\n\\nSOLUTION\\nThe correct answer is: class 0']\n",
            "Predicted class token: '0'\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# --- Configuration (should match your training setup) ---\n",
        "base_model_name = \"unsloth/Qwen3-0.6B-Base\" # The original base model\n",
        "load_in_4bit_at_load_time = False # Matches your inference script\n",
        "max_seq_length_at_load_time = 24000 # Matches your inference script\n",
        "dtype_at_load_time = None # Matches your inference script\n",
        "\n",
        "checkpoint_path = \"./model/NetPro-Qwen3-0.6B-ClfDC\"\n",
        "NUM_CLASSES = 4 # Same as during training\n",
        "\n",
        "# --- 1. Load the original base model ---\n",
        "print(f\"Loading base model: {base_model_name}...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=base_model_name,\n",
        "    max_seq_length=max_seq_length_at_load_time,\n",
        "    dtype=dtype_at_load_time,\n",
        "    load_in_4bit=load_in_4bit_at_load_time,\n",
        ")\n",
        "print(\"Base model loaded.\")\n",
        "\n",
        "# --- 2. Re-apply the lm_head modification (EXACTLY as done in training) ---\n",
        "print(\"Modifying lm_head to match training setup...\")\n",
        "number_token_ids = []\n",
        "for i in range(0, NUM_CLASSES+1):\n",
        "    number_token_ids.append(tokenizer.encode(str(i), add_special_tokens=False)[0])\n",
        "# keep only the number tokens from lm_head\n",
        "par = torch.nn.Parameter(model.lm_head.weight[number_token_ids, :])\n",
        "\n",
        "old_shape = model.lm_head.weight.shape\n",
        "old_size = old_shape[0]\n",
        "print(par.shape)\n",
        "print(old_shape)\n",
        "\n",
        "model.lm_head.weight = par\n",
        "\n",
        "reverse_map = {value: idx for idx, value in enumerate(number_token_ids)} # will be used later to convert an idx from the old tokenizer to the new lm_head\n",
        "reverse_map\n",
        "\n",
        "# --- 3. Load the LoRA adapter from the specific checkpoint ---\n",
        "# Now that the model's lm_head has the correct (shrunken) shape,\n",
        "# PeftModel can load the adapter weights without a size mismatch.\n",
        "print(f\"Loading LoRA adapter from: {checkpoint_path}\")\n",
        "model = PeftModel.from_pretrained(\n",
        "    model, # The base model WITH THE MODIFIED lm_head\n",
        "    checkpoint_path,\n",
        "    is_trainable=False\n",
        ")\n",
        "print(\"LoRA adapter loaded successfully.\")\n",
        "\n",
        "# --- lm head ---\n",
        "# Save the current (trimmed) lm_head and bias\n",
        "trimmed_lm_head = model.lm_head.weight.data.clone()\n",
        "trimmed_lm_head_bias = model.lm_head.bias.data.clone() if hasattr(model.lm_head, \"bias\") and model.lm_head.bias is not None else torch.zeros(len(number_token_ids), device=trimmed_lm_head.device)\n",
        "\n",
        "# Create a new lm_head with shape [old_size, hidden_dim]\n",
        "hidden_dim = trimmed_lm_head.shape[1]\n",
        "new_lm_head = torch.full((old_size, hidden_dim), 0, dtype=trimmed_lm_head.dtype, device=trimmed_lm_head.device)\n",
        "new_lm_head_bias = torch.full((old_size,), -1000.0, dtype=trimmed_lm_head_bias.dtype, device=trimmed_lm_head_bias.device)\n",
        "\n",
        "# Fill in the weights and bias for the allowed tokens (number_token_ids)\n",
        "for new_idx, orig_token_id in enumerate(number_token_ids):\n",
        "    new_lm_head[orig_token_id] = trimmed_lm_head[new_idx]\n",
        "    new_lm_head_bias[orig_token_id] = trimmed_lm_head_bias[new_idx]\n",
        "\n",
        "# Update the model's lm_head weight and bias\n",
        "with torch.no_grad():\n",
        "    new_lm_head_module = torch.nn.Linear(hidden_dim, old_size, bias=True, device=model.device)\n",
        "    new_lm_head_module.weight.data.copy_(new_lm_head)\n",
        "    new_lm_head_module.bias.data.copy_(new_lm_head_bias)\n",
        "    model.lm_head.modules_to_save[\"default\"] = new_lm_head_module\n",
        "\n",
        "print(f\"Remade lm_head: shape = {model.lm_head.weight.shape}. Allowed tokens: {number_token_ids}\")\n",
        "\n",
        "# --- 4. Prepare for inference ---\n",
        "FastLanguageModel.for_inference(model) # Unsloth's optimization for inference\n",
        "print(\"Model prepared for inference.\")\n",
        "\n",
        "# --- 5. Your Inference Prompt and Generation ---\n",
        "prompt_template = \"\"\"You are an expert Website Classifier.\n",
        "\n",
        "Domain: {} \n",
        "Website Content: {} \n",
        "\n",
        "Classify the website based on its content into one of the following categories:\n",
        "- 0: Benign (general info, news, safe entertainment, educational, marketplace, social media, etc.)\n",
        "- 1: Gambling (betting, casino, lottery, real money games, judi, slot)\n",
        "- 2: Pornography (explicit sexual content, adult themes, nudity, sexual, bokep)\n",
        "- 3: Harmful (malware, cybercrime, illegal activities, firearms, extremism, drugs, narcotics, phishing, scams, counterfeit, hacking tools, stolen data markets, carding)\n",
        "\n",
        "SOLUTION\n",
        "The correct answer is: class \"\"\"  # Note: Removed the final {} for inference\n",
        "\n",
        "# Example values\n",
        "website_domain_example = \"example.com\"\n",
        "website_content_example = \"sample content\"\n",
        "\n",
        "# Format the prompt for inference (model should generate the class number)\n",
        "full_prompt_for_inference = prompt_template.format(website_domain_example, website_content_example)\n",
        "\n",
        "inputs = tokenizer(full_prompt_for_inference, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "print(\"Generating output...\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=1, use_cache=True, pad_token_id=tokenizer.eos_token_id) # Added pad_token_id\n",
        "# For classification, you typically want deterministic output, so low/zero temperature:\n",
        "# outputs = model.generate(**inputs, max_new_tokens=1, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(\"Full decoded output:\", decoded_outputs)\n",
        "\n",
        "# Extract just the newly generated token\n",
        "generated_sequence = outputs[0]\n",
        "input_length = inputs.input_ids.shape[1]\n",
        "newly_generated_tokens = generated_sequence[input_length:]\n",
        "predicted_class_token = tokenizer.decode(newly_generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(f\"Predicted class token: '{predicted_class_token}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_df = pd.read_csv(\"dataset/netpro_raw_7k_val.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_df['Content'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predict = val_df['Content'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_prompt_for_inference = prompt_template.format(predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_prompt_for_inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_df = pd.read_csv(\"dataset/netpro_raw_7k_val.csv\")\n",
        "predict = val_df['Content'][0]\n",
        "full_prompt_for_inference = prompt_template.format(predict)\n",
        "inputs = tokenizer(full_prompt_for_inference, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "print(\"Generating output...\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=1, use_cache=True, pad_token_id=tokenizer.eos_token_id) # Added pad_token_id\n",
        "# For classification, you typically want deterministic output, so low/zero temperature:\n",
        "# outputs = model.generate(**inputs, max_new_tokens=1, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(\"Full decoded output:\", decoded_outputs)\n",
        "\n",
        "# Extract just the newly generated token\n",
        "generated_sequence = outputs[0]\n",
        "input_length = inputs.input_ids.shape[1]\n",
        "newly_generated_tokens = generated_sequence[input_length:]\n",
        "predicted_class_token = tokenizer.decode(newly_generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(f\"Predicted class token: '{predicted_class_token}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(predicted_class_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_class_int = int(predicted_class_token.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(predicted_class_int)\n",
        "predicted_class_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if val_df['Label'][0] == predicted_class_int:\n",
        "    print(\"Correct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from tqdm import tqdm\n",
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# --- Configuration (should match your training setup) ---\n",
        "base_model_name = \"unsloth/Qwen3-0.6B-Base\" # The original base model\n",
        "load_in_4bit_at_load_time = False # Matches your inference script\n",
        "max_seq_length_at_load_time = 24000 # Matches your inference script\n",
        "dtype_at_load_time = None # Matches your inference script\n",
        "\n",
        "checkpoint_path = \"./model/NetPro-Qwen3-0.6B-ClfDC\"\n",
        "NUM_CLASSES = 4 # Same as during training\n",
        "\n",
        "# --- 1. Load the original base model ---\n",
        "print(f\"Loading base model: {base_model_name}...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=base_model_name,\n",
        "    max_seq_length=max_seq_length_at_load_time,\n",
        "    dtype=dtype_at_load_time,\n",
        "    load_in_4bit=load_in_4bit_at_load_time,\n",
        ")\n",
        "print(\"Base model loaded.\")\n",
        "\n",
        "# --- 2. Re-apply the lm_head modification (EXACTLY as done in training) ---\n",
        "print(\"Modifying lm_head to match training setup...\")\n",
        "number_token_ids = []\n",
        "for i in range(0, NUM_CLASSES+1):\n",
        "    number_token_ids.append(tokenizer.encode(str(i), add_special_tokens=False)[0])\n",
        "# keep only the number tokens from lm_head\n",
        "par = torch.nn.Parameter(model.lm_head.weight[number_token_ids, :])\n",
        "\n",
        "old_shape = model.lm_head.weight.shape\n",
        "old_size = old_shape[0]\n",
        "print(par.shape)\n",
        "print(old_shape)\n",
        "\n",
        "model.lm_head.weight = par\n",
        "\n",
        "reverse_map = {value: idx for idx, value in enumerate(number_token_ids)} # will be used later to convert an idx from the old tokenizer to the new lm_head\n",
        "reverse_map\n",
        "\n",
        "# --- 3. Load the LoRA adapter from the specific checkpoint ---\n",
        "# Now that the model's lm_head has the correct (shrunken) shape,\n",
        "# PeftModel can load the adapter weights without a size mismatch.\n",
        "print(f\"Loading LoRA adapter from: {checkpoint_path}\")\n",
        "model = PeftModel.from_pretrained(\n",
        "    model, # The base model WITH THE MODIFIED lm_head\n",
        "    checkpoint_path,\n",
        "    is_trainable=False\n",
        ")\n",
        "print(\"LoRA adapter loaded successfully.\")\n",
        "\n",
        "# --- lm head ---\n",
        "# Save the current (trimmed) lm_head and bias\n",
        "trimmed_lm_head = model.lm_head.weight.data.clone()\n",
        "trimmed_lm_head_bias = model.lm_head.bias.data.clone() if hasattr(model.lm_head, \"bias\") and model.lm_head.bias is not None else torch.zeros(len(number_token_ids), device=trimmed_lm_head.device)\n",
        "\n",
        "# Create a new lm_head with shape [old_size, hidden_dim]\n",
        "hidden_dim = trimmed_lm_head.shape[1]\n",
        "new_lm_head = torch.full((old_size, hidden_dim), 0, dtype=trimmed_lm_head.dtype, device=trimmed_lm_head.device)\n",
        "new_lm_head_bias = torch.full((old_size,), -1000.0, dtype=trimmed_lm_head_bias.dtype, device=trimmed_lm_head_bias.device)\n",
        "\n",
        "# Fill in the weights and bias for the allowed tokens (number_token_ids)\n",
        "for new_idx, orig_token_id in enumerate(number_token_ids):\n",
        "    new_lm_head[orig_token_id] = trimmed_lm_head[new_idx]\n",
        "    new_lm_head_bias[orig_token_id] = trimmed_lm_head_bias[new_idx]\n",
        "\n",
        "# Update the model's lm_head weight and bias\n",
        "with torch.no_grad():\n",
        "    new_lm_head_module = torch.nn.Linear(hidden_dim, old_size, bias=True, device=model.device)\n",
        "    new_lm_head_module.weight.data.copy_(new_lm_head)\n",
        "    new_lm_head_module.bias.data.copy_(new_lm_head_bias)\n",
        "    model.lm_head.modules_to_save[\"default\"] = new_lm_head_module\n",
        "\n",
        "print(f\"Remade lm_head: shape = {model.lm_head.weight.shape}. Allowed tokens: {number_token_ids}\")\n",
        "\n",
        "# --- 4. Prepare for inference ---\n",
        "FastLanguageModel.for_inference(model) # Unsloth's optimization for inference\n",
        "print(\"Model prepared for inference.\")\n",
        "\n",
        "prompt_template = \"\"\"You are an expert Website Classifier.\n",
        "\n",
        "Website Content: {}\n",
        "\n",
        "Classify the website based on its content into one of the following categories:\n",
        "- 0: Benign (general info, news, safe entertainment, educational, marketplace, social media, etc.)\n",
        "- 1: Gambling (betting, casino, lottery, real money games, judi, slot)\n",
        "- 2: Pornography (explicit sexual content, adult themes, nudity, sexual, bokep)\n",
        "- 3: Harmful (malware, cybercrime, illegal activities, firearms, extremism, drugs, narcotics, phishing, scams, counterfeit, hacking tools, stolen data markets, carding)\n",
        "\n",
        "SOLUTION\n",
        "The correct answer is: class \"\"\"\n",
        "\n",
        "# Load validation data\n",
        "val_df = pd.read_csv(\"dataset/netpro_raw_7k_val.csv\", encoding=\"utf-8\")\n",
        "\n",
        "# Store predictions\n",
        "predicted_labels = []\n",
        "\n",
        "for content in tqdm(val_df['Content'], desc=\"Predicting\"):\n",
        "    full_prompt_for_inference = prompt_template.format(content)\n",
        "    inputs = tokenizer(full_prompt_for_inference, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=1, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
        "    generated_sequence = outputs[0]\n",
        "    input_length = inputs.input_ids.shape[1]\n",
        "    newly_generated_tokens = generated_sequence[input_length:]\n",
        "    predicted_class_token = tokenizer.decode(newly_generated_tokens, skip_special_tokens=True)\n",
        "    try:\n",
        "        predicted_class_int = int(predicted_class_token.strip())\n",
        "    except Exception:\n",
        "        predicted_class_int = -1  # or any invalid class\n",
        "    predicted_labels.append(predicted_class_int)\n",
        "\n",
        "# True labels\n",
        "true_labels = val_df['Label']\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
        "report = classification_report(true_labels, predicted_labels)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(f\"Accuracy : {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall   : {recall:.4f}\")\n",
        "print(f\"F1 Score : {f1:.4f}\")\n",
        "print(\"\\nDetailed classification report:\\n\")\n",
        "print(report)\n",
        "\n",
        "# Save metrics to .txt\n",
        "with open(\"classification_report_0,6b_val_llm_7k.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"Evaluation Metrics:\\n\")\n",
        "    f.write(f\"Accuracy : {accuracy:.4f}\\n\")\n",
        "    f.write(f\"Precision: {precision:.4f}\\n\")\n",
        "    f.write(f\"Recall   : {recall:.4f}\\n\")\n",
        "    f.write(f\"F1 Score : {f1:.4f}\\n\\n\")\n",
        "    f.write(\"Detailed classification report:\\n\")\n",
        "    f.write(report)\n",
        "\n",
        "# Add predictions to DataFrame and export\n",
        "val_df['predicted_label'] = predicted_labels\n",
        "val_df.to_csv(\"classified_output_1,6B_val_llm_7K.csv\", index=False)\n",
        "print(\"\\nClassification complete. Output saved to 'classified_output_val_llm.csv' and 'classification_report_0,6b_val_llm_7k.txt'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5081962,
          "sourceId": 8512897,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30733,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "unsloth_env310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
