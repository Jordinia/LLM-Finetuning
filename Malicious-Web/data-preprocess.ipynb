{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eea4c6f",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10039837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fishmon/.conda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from jsonschema import validate, ValidationError\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd424d2",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "473b0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the classification dictionary (outside the function is cleaner)\n",
    "classification_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"answer\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 3},\n",
    "        \"classification\": {\"type\": \"string\"},\n",
    "        \"reason\": {\"type\": \"string\"},\n",
    "        \"confidence\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100}\n",
    "    },\n",
    "    \"required\": [\"answer\", \"classification\", \"reason\", \"confidence\"]\n",
    "}\n",
    "\n",
    "def to_sharegpt_with_thought(system, input_suffix, dataset) -> Dataset:\n",
    "    \"\"\"\n",
    "    Convert website classification dataset to ShareGPT format including reasoning ('thought'),\n",
    "    with JSON validation and enhanced error handling, specifically checking for unhashable types.\n",
    "    Returns a Hugging Face Dataset object.\n",
    "    \n",
    "    Args:\n",
    "        system (str): System prompt\n",
    "        input_suffix (str): Suffix to append to the human message (can be empty if not needed)\n",
    "        dataset (pd.DataFrame): Input DataFrame with columns:\n",
    "            ['Domain', 'Content', 'Label', 'classification', 'reason', 'confidence', 'thought']\n",
    "            \n",
    "    Returns:\n",
    "        datasets.Dataset: Hugging Face Dataset with a 'conversations' column, \n",
    "                          where each row contains a list representing one conversation.\n",
    "                          Returns an empty Dataset if input dataset is empty or all rows fail.\n",
    "    \"\"\"\n",
    "    if not isinstance(dataset, pd.DataFrame) or dataset.empty:\n",
    "        print(\"Input is not a valid or non-empty DataFrame. Returning empty Dataset.\")\n",
    "        # Return an empty Dataset with the expected structure\n",
    "        return Dataset.from_dict({\"conversations\": []}) \n",
    "        \n",
    "    # This list will temporarily hold the conversation lists\n",
    "    conversation_data_list = [] \n",
    "    error_count = 0\n",
    "    processed_count = 0\n",
    "\n",
    "    human_template = f\"{input_suffix}\\nDomain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "    if not input_suffix:\n",
    "         human_template = f\"Domain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "\n",
    "    print(f\"Starting conversion for {len(dataset)} rows...\")\n",
    "\n",
    "    for idx, row in dataset.iterrows():\n",
    "        try:\n",
    "            # --- Data Extraction and Basic Type Check ---\n",
    "            domain = row.get('Domain', 'N/A') \n",
    "            content = str(row['Content']) if pd.notna(row['Content']) else \"\" \n",
    "            thought_text = str(row['thought']).strip() if pd.notna(row['thought']) else \"No thought provided.\"\n",
    "            \n",
    "            label = row['Label']\n",
    "            classification = row['classification']\n",
    "            reason = row['reason']\n",
    "            confidence = row['confidence']\n",
    "\n",
    "            if pd.isna(label) or pd.isna(classification) or pd.isna(reason) or pd.isna(confidence):\n",
    "                 raise ValueError(\"One or more required classification fields are NaN\")\n",
    "\n",
    "            if isinstance(label, (list, dict)) or \\\n",
    "               isinstance(classification, (list, dict)) or \\\n",
    "               isinstance(reason, (list, dict)) or \\\n",
    "               isinstance(confidence, (list, dict)):\n",
    "                 raise TypeError(\"One or more classification fields contain unhashable list/dict types\")\n",
    "\n",
    "            human_value = human_template.format(domain=domain, content=content)\n",
    "            \n",
    "            # --- Prepare and Validate Classification Dictionary ---\n",
    "            classification_dict = {\n",
    "                \"answer\": int(label), \n",
    "                \"classification\": str(classification),\n",
    "                \"reason\": str(reason), \n",
    "                \"confidence\": int(confidence)\n",
    "            }\n",
    "            validate(instance=classification_dict, schema=classification_schema) \n",
    "            \n",
    "            # --- Serialize and Format Output ---\n",
    "            final_json_str = json.dumps(classification_dict, ensure_ascii=False, indent=2) \n",
    "            gpt_value = f\"<think>\\n{thought_text}\\n</think>\\n```json\\n{final_json_str}\\n```\"\n",
    "            \n",
    "            # This is the list for a single conversation\n",
    "            conversation = [\n",
    "                {\"from\": \"system\", \"value\": system},\n",
    "                {\"from\": \"human\", \"value\": human_value},\n",
    "                {\"from\": \"gpt\", \"value\": gpt_value} \n",
    "            ]\n",
    "            \n",
    "            # Append the conversation list to our temporary list\n",
    "            conversation_data_list.append(conversation) \n",
    "            processed_count += 1\n",
    "\n",
    "        # --- Error Handling ---\n",
    "        except (ValidationError, ValueError, TypeError) as e: \n",
    "            error_count += 1\n",
    "            # Avoid printing excessive errors if many occur\n",
    "            if error_count < 20 or error_count % 100 == 0: \n",
    "                 print(f\"Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "            continue \n",
    "        except Exception as e: \n",
    "             error_count += 1\n",
    "             if error_count < 20 or error_count % 100 == 0:\n",
    "                 print(f\"UNEXPECTED Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "             continue\n",
    "\n",
    "    print(f\"\\nConversion finished.\")\n",
    "    print(f\"Successfully processed: {processed_count} rows\")\n",
    "    print(f\"Errors encountered: {error_count} rows\")\n",
    "\n",
    "    # --- Convert the list of conversation lists to a Hugging Face Dataset ---\n",
    "    if conversation_data_list:\n",
    "        # Create the dictionary format expected by from_dict\n",
    "        hf_dataset_dict = {\"conversations\": conversation_data_list} \n",
    "        # Create and return the Dataset object\n",
    "        return Dataset.from_dict(hf_dataset_dict)\n",
    "    else:\n",
    "        print(\"No valid data processed. Returning empty Dataset.\")\n",
    "        # Return an empty Dataset with the expected structure\n",
    "        return Dataset.from_dict({\"conversations\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca07748d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversion for 776 rows...\n",
      "\n",
      "Conversion finished.\n",
      "Successfully processed: 776 rows\n",
      "Errors encountered: 0 rows\n",
      "\n",
      "Output Type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset Info:\n",
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 776\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./dataset/harmful.csv')\n",
    "with open('./prompt/labelling_promptv4.txt', 'r', encoding='utf-8') as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "# Convert to ShareGPT format with Unicode preservation\n",
    "dataset = to_sharegpt_with_thought(\n",
    "    system=system_prompt,\n",
    "    input_suffix=\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n\",\n",
    "    dataset=df\n",
    ")\n",
    "\n",
    "# Now you can check the type and print the Dataset info\n",
    "print(\"\\nOutput Type:\", type(dataset))\n",
    "print(\"Dataset Info:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d4a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving {len(dataset)} conversations to JSONL...\")\n",
    "# Save dataset in JSONL format\n",
    "try:\n",
    "    # with open('./dataset/harmful_sharegpt_thought2.jsonl', 'w', encoding='utf-8') as f:\n",
    "    #     # Iterate directly through the list of conversation lists\n",
    "    #     for conversation_list in dataset: \n",
    "    #         f.write(json.dumps(conversation_list, ensure_ascii=False) + '\\n')\n",
    "    with open('./dataset/harmful_sharegpt_thought2.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in dataset:\n",
    "            f.write(json.dumps(item['conversations'], ensure_ascii=False) + '\\n')\n",
    "    print(\"Successfully saved data to harmful_sharegpt_thought3.jsonl\")\n",
    "except Exception as e:\n",
    "     print(f\"Error saving JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc5e696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1 with CUDA 1201 (you have 2.6.0+cu124)\n",
      "    Python  3.11.10 (you have 3.11.11)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Standardizing formats (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 776/776 [00:01<00:00, 521.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2260c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output Type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset Info:\n",
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 776\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOutput Type:\", type(dataset))\n",
    "print(\"Dataset Info:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8651d1",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6c0212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 30000 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-bnb-4bit\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
