{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9eac53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading base model: unsloth/Qwen3-0.6B-Base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rizky\\.conda\\envs\\unsloth\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.8: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 24.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Base model loaded.\n",
      "Modifying lm_head to match training setup...\n",
      "torch.Size([5, 1024])\n",
      "torch.Size([151936, 1024])\n",
      "Loading LoRA adapter from: ./model/NetPro-Qwen3-0.6B-ClfDC\n",
      "LoRA adapter loaded successfully.\n",
      "Remade lm_head: shape = torch.Size([151936, 1024]). Allowed tokens: [15, 16, 17, 18, 19]\n",
      "Model prepared for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 992/992 [01:17<00:00, 12.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy : 0.9224\n",
      "Precision: 0.9249\n",
      "Recall   : 0.9224\n",
      "F1 Score : 0.9207\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92       248\n",
      "           1       0.97      0.98      0.97       248\n",
      "           2       0.89      0.98      0.94       248\n",
      "           3       0.95      0.78      0.86       248\n",
      "\n",
      "    accuracy                           0.92       992\n",
      "   macro avg       0.92      0.92      0.92       992\n",
      "weighted avg       0.92      0.92      0.92       992\n",
      "\n",
      "\n",
      "Classification complete. Output saved to './classifications_ClfDC/output_0,6B_ClfDC3.csv' and './classifications_ClfDC/report_0,6B_ClfDC3.txt'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# --- Configuration (should match your training setup) ---\n",
    "base_model_name = \"unsloth/Qwen3-0.6B-Base\" # The original base model\n",
    "load_in_4bit_at_load_time = False # Matches your inference script\n",
    "max_seq_length_at_load_time = 24000 # Matches your inference script\n",
    "dtype_at_load_time = None # Matches your inference script\n",
    "output_csv_filename = \"./classifications_ClfDC/output_0,6B_ClfDC.csv\"\n",
    "output_report_filename = \"./classifications_ClfDC/report_0,6B_ClfDC.txt\"\n",
    "val_filename = \"dataset/test_balanced.csv\"\n",
    "\n",
    "checkpoint_path = \"./model/NetPro-Qwen3-0.6B-ClfDC\"\n",
    "NUM_CLASSES = 4 # Same as during training\n",
    "\n",
    "# --- 1. Load the original base model ---\n",
    "print(f\"Loading base model: {base_model_name}...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=base_model_name,\n",
    "    max_seq_length=max_seq_length_at_load_time,\n",
    "    dtype=dtype_at_load_time,\n",
    "    load_in_4bit=load_in_4bit_at_load_time,\n",
    ")\n",
    "print(\"Base model loaded.\")\n",
    "\n",
    "# --- 2. Re-apply the lm_head modification (EXACTLY as done in training) ---\n",
    "print(\"Modifying lm_head to match training setup...\")\n",
    "number_token_ids = []\n",
    "for i in range(0, NUM_CLASSES+1):\n",
    "    number_token_ids.append(tokenizer.encode(str(i), add_special_tokens=False)[0])\n",
    "# keep only the number tokens from lm_head\n",
    "par = torch.nn.Parameter(model.lm_head.weight[number_token_ids, :])\n",
    "\n",
    "old_shape = model.lm_head.weight.shape\n",
    "old_size = old_shape[0]\n",
    "print(par.shape)\n",
    "print(old_shape)\n",
    "\n",
    "model.lm_head.weight = par\n",
    "\n",
    "reverse_map = {value: idx for idx, value in enumerate(number_token_ids)} # will be used later to convert an idx from the old tokenizer to the new lm_head\n",
    "reverse_map\n",
    "\n",
    "# --- 3. Load the LoRA adapter from the specific checkpoint ---\n",
    "# Now that the model's lm_head has the correct (shrunken) shape,\n",
    "# PeftModel can load the adapter weights without a size mismatch.\n",
    "print(f\"Loading LoRA adapter from: {checkpoint_path}\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, # The base model WITH THE MODIFIED lm_head\n",
    "    checkpoint_path,\n",
    "    is_trainable=False\n",
    ")\n",
    "print(\"LoRA adapter loaded successfully.\")\n",
    "\n",
    "# --- lm head ---\n",
    "# Save the current (trimmed) lm_head and bias\n",
    "trimmed_lm_head = model.lm_head.weight.data.clone()\n",
    "trimmed_lm_head_bias = model.lm_head.bias.data.clone() if hasattr(model.lm_head, \"bias\") and model.lm_head.bias is not None else torch.zeros(len(number_token_ids), device=trimmed_lm_head.device)\n",
    "\n",
    "# Create a new lm_head with shape [old_size, hidden_dim]\n",
    "hidden_dim = trimmed_lm_head.shape[1]\n",
    "new_lm_head = torch.full((old_size, hidden_dim), 0, dtype=trimmed_lm_head.dtype, device=trimmed_lm_head.device)\n",
    "new_lm_head_bias = torch.full((old_size,), -1000.0, dtype=trimmed_lm_head_bias.dtype, device=trimmed_lm_head_bias.device)\n",
    "\n",
    "# Fill in the weights and bias for the allowed tokens (number_token_ids)\n",
    "for new_idx, orig_token_id in enumerate(number_token_ids):\n",
    "    new_lm_head[orig_token_id] = trimmed_lm_head[new_idx]\n",
    "    new_lm_head_bias[orig_token_id] = trimmed_lm_head_bias[new_idx]\n",
    "\n",
    "# Update the model's lm_head weight and bias\n",
    "with torch.no_grad():\n",
    "    new_lm_head_module = torch.nn.Linear(hidden_dim, old_size, bias=True, device=model.device)\n",
    "    new_lm_head_module.weight.data.copy_(new_lm_head)\n",
    "    new_lm_head_module.bias.data.copy_(new_lm_head_bias)\n",
    "    model.lm_head.modules_to_save[\"default\"] = new_lm_head_module\n",
    "\n",
    "print(f\"Remade lm_head: shape = {model.lm_head.weight.shape}. Allowed tokens: {number_token_ids}\")\n",
    "\n",
    "# --- 4. Prepare for inference ---\n",
    "FastLanguageModel.for_inference(model) # Unsloth's optimization for inference\n",
    "print(\"Model prepared for inference.\")\n",
    "\n",
    "# ...existing code...\n",
    "\n",
    "prompt_template = \"\"\"You are an expert Website Classifier.\n",
    "\n",
    "Domain: \"{}\" \n",
    "Website Content: \"{}\" \n",
    "\n",
    "Classify the website based on its content into one of the following categories:\n",
    "- 0: Benign (general info, news, safe entertainment, educational, marketplace, social media, etc.)\n",
    "- 1: Gambling (betting, casino, lottery, real money games, judi, slot)\n",
    "- 2: Pornography (explicit sexual content, adult themes, nudity, sexual, bokep)\n",
    "- 3: Harmful (malware, cybercrime, illegal activities, firearms, extremism, drugs, narcotics, phishing, scams, counterfeit, hacking tools, stolen data markets, carding)\n",
    "\n",
    "SOLUTION\n",
    "The correct answer is: class \"\"\"\n",
    "\n",
    "# Load validation data\n",
    "val_df = pd.read_csv(val_filename, encoding=\"utf-8\")\n",
    "\n",
    "# Store predictions\n",
    "predicted_labels = []\n",
    "\n",
    "for _, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Predicting\"):\n",
    "    domain = row['Domain']\n",
    "    content = row['Content']\n",
    "    full_prompt_for_inference = prompt_template.format(domain, content)\n",
    "    inputs = tokenizer(full_prompt_for_inference, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_sequence = outputs[0]\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    newly_generated_tokens = generated_sequence[input_length:]\n",
    "    predicted_class_token = tokenizer.decode(newly_generated_tokens, skip_special_tokens=True)\n",
    "    try:\n",
    "        predicted_class_int = int(predicted_class_token.strip())\n",
    "    except Exception:\n",
    "        predicted_class_int = -1  # or any invalid class\n",
    "    predicted_labels.append(predicted_class_int)\n",
    "\n",
    "# True labels\n",
    "true_labels = val_df['Label']\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "f1 = f1_score(  true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "print(\"\\nDetailed classification report:\\n\")\n",
    "print(report)\n",
    "\n",
    "# Save metrics to .txt\n",
    "with open(output_report_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Evaluation Metrics:\\n\")\n",
    "    f.write(f\"Accuracy : {accuracy:.4f}\\n\")\n",
    "    f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "    f.write(f\"Recall   : {recall:.4f}\\n\")\n",
    "    f.write(f\"F1 Score : {f1:.4f}\\n\\n\")\n",
    "    f.write(\"Detailed classification report:\\n\")\n",
    "    f.write(report)\n",
    "\n",
    "# Add predictions to DataFrame and export\n",
    "val_df['predicted_label'] = predicted_labels\n",
    "val_df.to_csv(output_csv_filename, index=False)\n",
    "print(f\"\\nClassification complete. Output saved to '{output_csv_filename}' and '{output_report_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d6b9fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading base model: unsloth/Qwen3-1.7B-Base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rizky\\.conda\\envs\\unsloth\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.8: Fast Qwen3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 24.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Base model loaded.\n",
      "Modifying lm_head to match training setup...\n",
      "torch.Size([5, 2048])\n",
      "torch.Size([151936, 2048])\n",
      "Loading LoRA adapter from: ./model/NetPro-Qwen3-1.7B-ClfDC\n",
      "LoRA adapter loaded successfully.\n",
      "Remade lm_head: shape = torch.Size([151936, 2048]). Allowed tokens: [15, 16, 17, 18, 19]\n",
      "Model prepared for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 992/992 [02:04<00:00,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy : 0.9294\n",
      "Precision: 0.9312\n",
      "Recall   : 0.9294\n",
      "F1 Score : 0.9280\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92       248\n",
      "           1       0.95      0.98      0.96       248\n",
      "           2       0.92      0.99      0.96       248\n",
      "           3       0.96      0.80      0.87       248\n",
      "\n",
      "    accuracy                           0.93       992\n",
      "   macro avg       0.93      0.93      0.93       992\n",
      "weighted avg       0.93      0.93      0.93       992\n",
      "\n",
      "\n",
      "Classification complete. Output saved to './classifications_ClfDC/output_1,7B_ClfDC.csv' and './classifications_ClfDC/report_1,7B_ClfDC.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# --- Configuration (should match your training setup) ---\n",
    "base_model_name = \"unsloth/Qwen3-1.7B-Base\" # The original base model\n",
    "load_in_4bit_at_load_time = False # Matches your inference script\n",
    "max_seq_length_at_load_time = 24000 # Matches your inference script\n",
    "dtype_at_load_time = None # Matches your inference script\n",
    "output_csv_filename = \"./classifications_ClfDC/output_1,7B_ClfDC.csv\"\n",
    "output_report_filename = \"./classifications_ClfDC/report_1,7B_ClfDC.txt\"\n",
    "val_filename = \"dataset/test_balanced.csv\"\n",
    "\n",
    "checkpoint_path = \"./model/NetPro-Qwen3-1.7B-ClfDC\"\n",
    "NUM_CLASSES = 4 # Same as during training\n",
    "\n",
    "# --- 1. Load the original base model ---\n",
    "print(f\"Loading base model: {base_model_name}...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=base_model_name,\n",
    "    max_seq_length=max_seq_length_at_load_time,\n",
    "    dtype=dtype_at_load_time,\n",
    "    load_in_4bit=load_in_4bit_at_load_time,\n",
    ")\n",
    "print(\"Base model loaded.\")\n",
    "\n",
    "# --- 2. Re-apply the lm_head modification (EXACTLY as done in training) ---\n",
    "print(\"Modifying lm_head to match training setup...\")\n",
    "number_token_ids = []\n",
    "for i in range(0, NUM_CLASSES+1):\n",
    "    number_token_ids.append(tokenizer.encode(str(i), add_special_tokens=False)[0])\n",
    "# keep only the number tokens from lm_head\n",
    "par = torch.nn.Parameter(model.lm_head.weight[number_token_ids, :])\n",
    "\n",
    "old_shape = model.lm_head.weight.shape\n",
    "old_size = old_shape[0]\n",
    "print(par.shape)\n",
    "print(old_shape)\n",
    "\n",
    "model.lm_head.weight = par\n",
    "\n",
    "reverse_map = {value: idx for idx, value in enumerate(number_token_ids)} # will be used later to convert an idx from the old tokenizer to the new lm_head\n",
    "reverse_map\n",
    "\n",
    "# --- 3. Load the LoRA adapter from the specific checkpoint ---\n",
    "# Now that the model's lm_head has the correct (shrunken) shape,\n",
    "# PeftModel can load the adapter weights without a size mismatch.\n",
    "print(f\"Loading LoRA adapter from: {checkpoint_path}\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, # The base model WITH THE MODIFIED lm_head\n",
    "    checkpoint_path,\n",
    "    is_trainable=False\n",
    ")\n",
    "print(\"LoRA adapter loaded successfully.\")\n",
    "\n",
    "# --- lm head ---\n",
    "# Save the current (trimmed) lm_head and bias\n",
    "trimmed_lm_head = model.lm_head.weight.data.clone()\n",
    "trimmed_lm_head_bias = model.lm_head.bias.data.clone() if hasattr(model.lm_head, \"bias\") and model.lm_head.bias is not None else torch.zeros(len(number_token_ids), device=trimmed_lm_head.device)\n",
    "\n",
    "# Create a new lm_head with shape [old_size, hidden_dim]\n",
    "hidden_dim = trimmed_lm_head.shape[1]\n",
    "new_lm_head = torch.full((old_size, hidden_dim), 0, dtype=trimmed_lm_head.dtype, device=trimmed_lm_head.device)\n",
    "new_lm_head_bias = torch.full((old_size,), -1000.0, dtype=trimmed_lm_head_bias.dtype, device=trimmed_lm_head_bias.device)\n",
    "\n",
    "# Fill in the weights and bias for the allowed tokens (number_token_ids)\n",
    "for new_idx, orig_token_id in enumerate(number_token_ids):\n",
    "    new_lm_head[orig_token_id] = trimmed_lm_head[new_idx]\n",
    "    new_lm_head_bias[orig_token_id] = trimmed_lm_head_bias[new_idx]\n",
    "\n",
    "# Update the model's lm_head weight and bias\n",
    "with torch.no_grad():\n",
    "    new_lm_head_module = torch.nn.Linear(hidden_dim, old_size, bias=True, device=model.device)\n",
    "    new_lm_head_module.weight.data.copy_(new_lm_head)\n",
    "    new_lm_head_module.bias.data.copy_(new_lm_head_bias)\n",
    "    model.lm_head.modules_to_save[\"default\"] = new_lm_head_module\n",
    "\n",
    "print(f\"Remade lm_head: shape = {model.lm_head.weight.shape}. Allowed tokens: {number_token_ids}\")\n",
    "\n",
    "# --- 4. Prepare for inference ---\n",
    "FastLanguageModel.for_inference(model) # Unsloth's optimization for inference\n",
    "print(\"Model prepared for inference.\")\n",
    "\n",
    "# ...existing code...\n",
    "\n",
    "prompt_template = \"\"\"You are an expert Website Classifier.\n",
    "\n",
    "Domain: \"{}\" \n",
    "Website Content: \"{}\" \n",
    "\n",
    "Classify the website based on its content into one of the following categories:\n",
    "- 0: Benign (general info, news, safe entertainment, educational, marketplace, social media, etc.)\n",
    "- 1: Gambling (betting, casino, lottery, real money games, judi, slot)\n",
    "- 2: Pornography (explicit sexual content, adult themes, nudity, sexual, bokep)\n",
    "- 3: Harmful (malware, cybercrime, illegal activities, firearms, extremism, drugs, narcotics, phishing, scams, counterfeit, hacking tools, stolen data markets, carding)\n",
    "\n",
    "SOLUTION\n",
    "The correct answer is: class \"\"\"\n",
    "\n",
    "# Load validation data\n",
    "val_df = pd.read_csv(val_filename, encoding=\"utf-8\")\n",
    "\n",
    "# Store predictions\n",
    "predicted_labels = []\n",
    "\n",
    "for _, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Predicting\"):\n",
    "    domain = row['Domain']\n",
    "    content = row['Content']\n",
    "    full_prompt_for_inference = prompt_template.format(domain, content)\n",
    "    inputs = tokenizer(full_prompt_for_inference, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_sequence = outputs[0]\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    newly_generated_tokens = generated_sequence[input_length:]\n",
    "    predicted_class_token = tokenizer.decode(newly_generated_tokens, skip_special_tokens=True)\n",
    "    try:\n",
    "        predicted_class_int = int(predicted_class_token.strip())\n",
    "    except Exception:\n",
    "        predicted_class_int = -1  # or any invalid class\n",
    "    predicted_labels.append(predicted_class_int)\n",
    "\n",
    "# True labels\n",
    "true_labels = val_df['Label']\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "f1 = f1_score(  true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "print(\"\\nDetailed classification report:\\n\")\n",
    "print(report)\n",
    "\n",
    "# Save metrics to .txt\n",
    "with open(output_report_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Evaluation Metrics:\\n\")\n",
    "    f.write(f\"Accuracy : {accuracy:.4f}\\n\")\n",
    "    f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "    f.write(f\"Recall   : {recall:.4f}\\n\")\n",
    "    f.write(f\"F1 Score : {f1:.4f}\\n\\n\")\n",
    "    f.write(\"Detailed classification report:\\n\")\n",
    "    f.write(report)\n",
    "\n",
    "# Add predictions to DataFrame and export\n",
    "val_df['predicted_label'] = predicted_labels\n",
    "val_df.to_csv(output_csv_filename, index=False)\n",
    "print(f\"\\nClassification complete. Output saved to '{output_csv_filename}' and '{output_report_filename}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
