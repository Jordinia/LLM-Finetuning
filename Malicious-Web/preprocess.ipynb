{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfb8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from jsonschema import validate, ValidationError\n",
    "from datasets import Dataset, load_dataset\n",
    "import os\n",
    "\n",
    "def mergeCsv(output_file, *input_files):\n",
    "    \"\"\"\n",
    "    Merges multiple CSV files into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        output_file (str): The name of the output CSV file.\n",
    "        *input_files (str): Paths to the input CSV files to be merged.\n",
    "    \"\"\"\n",
    "    # List to store DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Read each CSV file and append to the list\n",
    "    for file in input_files:\n",
    "        if os.path.exists(file):\n",
    "            df = pd.read_csv(file)\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    if dataframes:\n",
    "        merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "        # Save the merged DataFrame to the output file\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"Merged CSV saved as: {output_file}\")\n",
    "    else:\n",
    "        print(\"No valid files to merge.\")\n",
    "\n",
    "\n",
    "def countRow(input_file):\n",
    "    \"\"\"\n",
    "    Counts the number of rows in a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of rows in the CSV file.\n",
    "    \"\"\"\n",
    "    if os.path.exists(input_file):\n",
    "        df = pd.read_csv(input_file)\n",
    "        row_count = len(df)\n",
    "        print(f\"Number of rows in {input_file}: {row_count}\")\n",
    "    else:\n",
    "        print(f\"File not found: {input_file}\")\n",
    "\n",
    "def checkDuplicate(file_path):\n",
    "    \"\"\"\n",
    "    Checks for duplicate rows in a CSV file based on the 'Domain' column.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check for duplicates based on the 'Domain' column\n",
    "        duplicates = df[df.duplicated(subset='Domain', keep=False)]\n",
    "\n",
    "        # Print the duplicates if any\n",
    "        if not duplicates.empty:\n",
    "            print(f\"Found {len(duplicates)} duplicate rows based on the 'Domain' column:\")\n",
    "        else:\n",
    "            print(\"No duplicates found based on the 'Domain' column.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "def removeDuplicate(file_path, output_file):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows in a CSV file based on the 'Domain' column, keeping the first occurrence.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "        output_file (str): Path to save the deduplicated CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Remove duplicates based on the 'Domain' column, keeping the first occurrence\n",
    "        deduplicated_df = df.drop_duplicates(subset='Domain', keep='first')\n",
    "\n",
    "        # Save the deduplicated DataFrame to a new file\n",
    "        deduplicated_df.to_csv(output_file, index=False)\n",
    "        print(f\"Duplicates removed. Deduplicated file saved as: {output_file}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "\n",
    "def load_and_print_all_columns(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file and print all columns.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(df.columns.tolist())\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53335fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/merged_combined_dedup_final.csv: 96926\n",
      "Found 4491 duplicate rows based on the 'Domain' column:\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/merged_combined_dedup_final.csv\")\n",
    "checkDuplicate(\"./dataset/merged_combined_dedup_final.csv\")\n",
    "load_and_print_all_columns(\"./dataset/merged_combined_dedup_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a1a338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/test.csv: 19367\n",
      "Found 324 duplicate rows based on the 'Domain' column:\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/test.csv\")\n",
    "checkDuplicate(\"./dataset/test.csv\")\n",
    "load_and_print_all_columns(\"./dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767c9a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/train.csv: 77461\n",
      "Found 3449 duplicate rows based on the 'Domain' column:\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/train.csv\")\n",
    "checkDuplicate(\"./dataset/train.csv\")\n",
    "load_and_print_all_columns(\"./dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67700f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 63426 occurrences\n",
      "Label 2: 7384 occurrences\n",
      "Label 1: 5660 occurrences\n",
      "Label 3: 991 occurrences\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = \"./dataset/train.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count occurrences of each unique value in the 'Label' column\n",
    "    label_counts = df['Label'].value_counts()\n",
    "    \n",
    "    # Print the occurrences\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count} occurrences\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04cceb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 15857 occurrences\n",
      "Label 2: 1846 occurrences\n",
      "Label 1: 1416 occurrences\n",
      "Label 3: 248 occurrences\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = \"./dataset/test.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count occurrences of each unique value in the 'Label' column\n",
    "    label_counts = df['Label'].value_counts()\n",
    "    \n",
    "    # Print the occurrences\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count} occurrences\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8fbf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"./dataset/train.csv\")\n",
    "\n",
    "# Define target counts\n",
    "target_counts = {0: 8000, 1: 5660, 2: 7384, 3: 3964}\n",
    "\n",
    "# Process each label\n",
    "processed_dfs = []\n",
    "\n",
    "# Label 0 (Undersample)\n",
    "df_label_0 = df[df['Label'] == 0].sample(target_counts[0], random_state=42)\n",
    "processed_dfs.append(df_label_0)\n",
    "\n",
    "# Label 1 (Keep original)\n",
    "df_label_1 = df[df['Label'] == 1]\n",
    "processed_dfs.append(df_label_1)\n",
    "\n",
    "# Label 2 (Keep original)\n",
    "df_label_2 = df[df['Label'] == 2]\n",
    "processed_dfs.append(df_label_2)\n",
    "\n",
    "# Label 3 (Oversample)\n",
    "df_label_3 = df[df['Label'] == 3]\n",
    "if not df_label_3.empty:\n",
    "    num_original = len(df_label_3)\n",
    "    factor = target_counts[3] // num_original\n",
    "    remainder = target_counts[3] % num_original\n",
    "    oversampled = pd.concat([df_label_3] * factor + [df_label_3.sample(remainder)])\n",
    "    processed_dfs.append(oversampled)\n",
    "\n",
    "# Combine and shuffle\n",
    "df_balanced = shuffle(pd.concat(processed_dfs), random_state=42)\n",
    "\n",
    "# Save to CSV\n",
    "df_balanced.to_csv(\"train_balanced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10bfb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/train_balanced.csv: 25008\n",
      "Found 5187 duplicate rows based on the 'Domain' column:\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/train_balanced.csv\")\n",
    "checkDuplicate(\"./dataset/train_balanced.csv\")\n",
    "load_and_print_all_columns(\"./dataset/train_balanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46cc84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 2500 occurrences\n",
      "Label 2: 1800 occurrences\n",
      "Label 1: 1800 occurrences\n",
      "Label 3: 1205 occurrences\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "file_path = \"./dataset/balanced_dataset.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count occurrences of each unique value in the 'Label' column\n",
    "    label_counts = df['Label'].value_counts()\n",
    "    \n",
    "    # Print the occurrences\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count} occurrences\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2b6d0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset saved as './dataset/netpro_7k_val.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the deduplicated dataset\n",
    "file_path = \"./dataset/test.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Configure the number of samples to take from each label\n",
    "label_sample_sizes = {\n",
    "    0: 18,  # Benign\n",
    "    1: 18,  # Gambling\n",
    "    2: 18,  # Pornography\n",
    "    3: 6   # Harmful\n",
    "}\n",
    "\n",
    "# Process each label\n",
    "validation_dataframes = []\n",
    "remaining_dataframes = []\n",
    "\n",
    "for label, sample_size in label_sample_sizes.items():\n",
    "    label_df = df[df[\"Label\"] == label]\n",
    "    \n",
    "    # Take 10 samples for validation\n",
    "    validation_sample = label_df.sample(n=sample_size, random_state=42)\n",
    "    validation_dataframes.append(validation_sample)\n",
    "    \n",
    "    # Remove the sampled rows from the original dataset\n",
    "    remaining_data = label_df.drop(validation_sample.index)\n",
    "    remaining_dataframes.append(remaining_data)\n",
    "\n",
    "# Combine validation samples and save\n",
    "validation_df = pd.concat(validation_dataframes).reset_index(drop=True)\n",
    "validation_df.to_csv(\"./dataset/netpro_7k_val.csv\", index=False)\n",
    "print(\"Validation dataset saved as './dataset/netpro_7k_val.csv'\")\n",
    "\n",
    "# # Combine remaining data and save back to the original file\n",
    "# remaining_df = pd.concat(remaining_dataframes).reset_index(drop=True)\n",
    "# remaining_df.to_csv(\"./dataset/netpro_7k_train.csv\", index=False)\n",
    "# print(\"Updated dataset saved as './dataset/netpro_7k_train.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca87cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/netpro_7k_train.csv: 7245\n",
      "No duplicates found based on the 'Domain' column.\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n",
      "Number of rows in ./dataset/netpro_7k_val.csv: 60\n",
      "No duplicates found based on the 'Domain' column.\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/netpro_7k_train.csv\")\n",
    "checkDuplicate(\"./dataset/netpro_7k_train.csv\")\n",
    "load_and_print_all_columns(\"./dataset/netpro_7k_train.csv\")\n",
    "countRow(\"./dataset/netpro_7k_val.csv\")\n",
    "checkDuplicate(\"./dataset/netpro_7k_val.csv\")\n",
    "load_and_print_all_columns(\"./dataset/netpro_7k_val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb1416",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3698396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the schema for the classification dictionary (outside the function is cleaner)\n",
    "# classification_schema = {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"answer\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 3},\n",
    "#         \"classification\": {\"type\": \"string\"},\n",
    "#         \"reason\": {\"type\": \"string\"},\n",
    "#         \"confidence\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100}\n",
    "#     },\n",
    "#     \"required\": [\"answer\", \"classification\", \"reason\", \"confidence\"]\n",
    "# }\n",
    "\n",
    "# def to_sharegpt_with_thought(system, input_suffix, dataset) -> Dataset:\n",
    "#     \"\"\"\n",
    "#     Convert website classification dataset to ShareGPT format including reasoning ('thought'),\n",
    "#     with JSON validation and enhanced error handling, specifically checking for unhashable types.\n",
    "#     Returns a Hugging Face Dataset object.\n",
    "    \n",
    "#     Args:\n",
    "#         system (str): System prompt\n",
    "#         input_suffix (str): Suffix to append to the human message (can be empty if not needed)\n",
    "#         dataset (pd.DataFrame): Input DataFrame with columns:\n",
    "#             ['Domain', 'Content', 'Label', 'classification', 'reason', 'confidence', 'thought']\n",
    "            \n",
    "#     Returns:\n",
    "#         datasets.Dataset: Hugging Face Dataset with a 'conversations' column, \n",
    "#                           where each row contains a list representing one conversation.\n",
    "#                           Returns an empty Dataset if input dataset is empty or all rows fail.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(dataset, pd.DataFrame) or dataset.empty:\n",
    "#         print(\"Input is not a valid or non-empty DataFrame. Returning empty Dataset.\")\n",
    "#         # Return an empty Dataset with the expected structure\n",
    "#         return Dataset.from_dict({\"conversations\": []}) \n",
    "        \n",
    "#     # This list will temporarily hold the conversation lists\n",
    "#     conversation_data_list = [] \n",
    "#     error_count = 0\n",
    "#     processed_count = 0\n",
    "\n",
    "#     human_template = f\"{input_suffix}\\nDomain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "#     if not input_suffix:\n",
    "#          human_template = f\"Domain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "\n",
    "#     print(f\"Starting conversion for {len(dataset)} rows...\")\n",
    "\n",
    "#     for idx, row in dataset.iterrows():\n",
    "#         try:\n",
    "#             # --- Data Extraction and Basic Type Check ---\n",
    "#             domain = row.get('Domain', 'N/A') \n",
    "#             content = str(row['Content']) if pd.notna(row['Content']) else \"\" \n",
    "#             thought_text = str(row['thought']).strip() if pd.notna(row['thought']) else \"No thought provided.\"\n",
    "            \n",
    "#             label = row['Label']\n",
    "#             classification = row['classification']\n",
    "#             reason = row['reason']\n",
    "#             confidence = row['confidence']\n",
    "\n",
    "#             if pd.isna(label) or pd.isna(classification) or pd.isna(reason) or pd.isna(confidence):\n",
    "#                  raise ValueError(\"One or more required classification fields are NaN\")\n",
    "\n",
    "#             if isinstance(label, (list, dict)) or \\\n",
    "#                isinstance(classification, (list, dict)) or \\\n",
    "#                isinstance(reason, (list, dict)) or \\\n",
    "#                isinstance(confidence, (list, dict)):\n",
    "#                  raise TypeError(\"One or more classification fields contain unhashable list/dict types\")\n",
    "\n",
    "#             human_value = human_template.format(domain=domain, content=content)\n",
    "            \n",
    "#             # --- Prepare and Validate Classification Dictionary ---\n",
    "#             classification_dict = {\n",
    "#                 \"answer\": int(label), \n",
    "#                 \"classification\": str(classification),\n",
    "#                 \"reason\": str(reason), \n",
    "#                 \"confidence\": int(confidence)\n",
    "#             }\n",
    "#             validate(instance=classification_dict, schema=classification_schema) \n",
    "            \n",
    "#             # --- Serialize and Format Output ---\n",
    "#             final_json_str = json.dumps(classification_dict, ensure_ascii=False, indent=2) \n",
    "#             gpt_value = f\"<think>\\n{thought_text}\\n</think>\\n```json\\n{final_json_str}\\n```\"\n",
    "            \n",
    "#             # This is the list for a single conversation\n",
    "#             conversation = [\n",
    "#                 {\"from\": \"system\", \"value\": system},\n",
    "#                 {\"from\": \"human\", \"value\": human_value},\n",
    "#                 {\"from\": \"gpt\", \"value\": gpt_value} \n",
    "#             ]\n",
    "            \n",
    "#             # Append the conversation list to our temporary list\n",
    "#             conversation_data_list.append(conversation) \n",
    "#             processed_count += 1\n",
    "\n",
    "#         # --- Error Handling ---\n",
    "#         except (ValidationError, ValueError, TypeError) as e: \n",
    "#             error_count += 1\n",
    "#             # Avoid printing excessive errors if many occur\n",
    "#             if error_count < 20 or error_count % 100 == 0: \n",
    "#                  print(f\"Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "#             continue \n",
    "#         except Exception as e: \n",
    "#              error_count += 1\n",
    "#              if error_count < 20 or error_count % 100 == 0:\n",
    "#                  print(f\"UNEXPECTED Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "#              continue\n",
    "\n",
    "#     print(f\"\\nConversion finished.\")\n",
    "#     print(f\"Successfully processed: {processed_count} rows\")\n",
    "#     print(f\"Errors encountered: {error_count} rows\")\n",
    "\n",
    "#     # --- Convert the list of conversation lists to a Hugging Face Dataset ---\n",
    "#     if conversation_data_list:\n",
    "#         # Create the dictionary format expected by from_dict\n",
    "#         hf_dataset_dict = {\"conversations\": conversation_data_list} \n",
    "#         # Create and return the Dataset object\n",
    "#         return Dataset.from_dict(hf_dataset_dict)\n",
    "#     else:\n",
    "#         print(\"No valid data processed. Returning empty Dataset.\")\n",
    "#         # Return an empty Dataset with the expected structure\n",
    "#         return Dataset.from_dict({\"conversations\": []})\n",
    "    \n",
    "\n",
    "# Define the schema for the classification dictionary\n",
    "classification_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"answer\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 3},\n",
    "        \"classification\": {\"type\": \"string\"},\n",
    "        \"reason\": {\"type\": \"string\"},\n",
    "        \"confidence\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100}\n",
    "    },\n",
    "    \"required\": [\"answer\", \"classification\", \"reason\", \"confidence\"]\n",
    "}\n",
    "\n",
    "def to_sharegpt_with_thought(system, input_suffix, dataset) -> Dataset:\n",
    "    \"\"\"\n",
    "    Convert website classification dataset to ShareGPT format including reasoning ('Thought'),\n",
    "    with JSON validation and enhanced error handling, specifically checking for unhashable types.\n",
    "    Returns a Hugging Face Dataset object.\n",
    "    \n",
    "    Args:\n",
    "        system (str): System prompt\n",
    "        input_suffix (str): Suffix to append to the human message (can be empty if not needed)\n",
    "        dataset (pd.DataFrame): Input DataFrame with columns:\n",
    "            ['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n",
    "            \n",
    "    Returns:\n",
    "        datasets.Dataset: Hugging Face Dataset with a 'conversations' column, \n",
    "                          where each row contains a list representing one conversation\n",
    "    \"\"\"\n",
    "    # Validate input DataFrame\n",
    "    if not isinstance(dataset, pd.DataFrame) or dataset.empty:\n",
    "        print(\"Input is not a valid or non-empty DataFrame. Returning empty Dataset.\")\n",
    "        return Dataset.from_dict({\"conversations\": []}) \n",
    "        \n",
    "    # Validate required columns\n",
    "    required_columns = ['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n",
    "    if not all(col in dataset.columns for col in required_columns):\n",
    "        missing = [col for col in required_columns if col not in dataset.columns]\n",
    "        print(f\"Missing required columns: {missing}. Returning empty Dataset.\")\n",
    "        return Dataset.from_dict({\"conversations\": []})\n",
    "\n",
    "    conversation_data_list = [] \n",
    "    error_count = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Template for human message\n",
    "    human_template = f\"{input_suffix}\\nDomain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "    if not input_suffix:\n",
    "        human_template = f\"Domain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "    \n",
    "    print(f\"Starting conversion for {len(dataset)} rows...\")\n",
    "    \n",
    "    for idx, row in dataset.iterrows():\n",
    "        try:\n",
    "            # Extract data from DataFrame row\n",
    "            domain = str(row['Domain']) if pd.notna(row['Domain']) else \"N/A\"\n",
    "            content = str(row['Content']) if pd.notna(row['Content']) else \"\"\n",
    "            thought = str(row['Thought']) if pd.notna(row['Thought']) else \"No thought provided.\"\n",
    "            \n",
    "            # Validate classification fields\n",
    "            label = row['Label']\n",
    "            classification = row['Classification']\n",
    "            reason = row['Reason']\n",
    "            confidence = row['Confidence']\n",
    "            \n",
    "            if pd.isna(label) or pd.isna(classification) or pd.isna(reason) or pd.isna(confidence):\n",
    "                raise ValueError(\"One or more required classification fields are NaN\")\n",
    "                \n",
    "            if isinstance(label, (list, dict)) or isinstance(classification, (list, dict)) or \\\n",
    "               isinstance(reason, (list, dict)) or isinstance(confidence, (list, dict)):\n",
    "                raise TypeError(\"One or more classification fields contain unhashable list/dict types\")\n",
    "            \n",
    "            # Create human message\n",
    "            human_value = human_template.format(domain=domain, content=content)\n",
    "            \n",
    "            # Prepare and validate classification dictionary\n",
    "            classification_dict = {\n",
    "                \"answer\": int(label),\n",
    "                \"classification\": str(classification),\n",
    "                \"reason\": str(reason),\n",
    "                \"confidence\": int(confidence)\n",
    "            }\n",
    "            validate(instance=classification_dict, schema=classification_schema)\n",
    "            \n",
    "            # Create GPT response with thought and JSON\n",
    "            final_json_str = json.dumps(classification_dict, ensure_ascii=False, indent=2)\n",
    "            gpt_value = f\"<think>\\n{thought}\\n</think>\\n```json\\n{final_json_str}\\n```\"\n",
    "            # Create conversation structure\n",
    "            conversation = [\n",
    "                {\"from\": \"system\", \"value\": system},\n",
    "                {\"from\": \"human\", \"value\": human_value},\n",
    "                {\"from\": \"gpt\", \"value\": gpt_value}\n",
    "            ]\n",
    "            \n",
    "            conversation_data_list.append(conversation)\n",
    "            processed_count += 1\n",
    "            \n",
    "        except (ValidationError, ValueError, TypeError) as e:\n",
    "            error_count += 1\n",
    "            if error_count < 20 or error_count % 100 == 0:\n",
    "                print(f\"Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            if error_count < 20 or error_count % 100 == 0:\n",
    "                print(f\"UNEXPECTED Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Print conversion summary\n",
    "    print(f\"\\nConversion finished.\")\n",
    "    print(f\"Successfully processed: {processed_count} rows\")\n",
    "    print(f\"Errors encountered: {error_count} rows\")\n",
    "    \n",
    "    # Return Dataset\n",
    "    if conversation_data_list:\n",
    "        return Dataset.from_dict({\"conversations\": conversation_data_list})\n",
    "    else:\n",
    "        print(\"No valid data processed. Returning empty Dataset.\")\n",
    "        return Dataset.from_dict({\"conversations\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9ce0246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversion for 25008 rows...\n",
      "\n",
      "Conversion finished.\n",
      "Successfully processed: 25008 rows\n",
      "Errors encountered: 0 rows\n",
      "Starting conversion for 60 rows...\n",
      "\n",
      "Conversion finished.\n",
      "Successfully processed: 60 rows\n",
      "Errors encountered: 0 rows\n",
      "\n",
      "Output Type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset Info:\n",
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 25008\n",
      "})\n",
      "\n",
      "Output Type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset Info:\n",
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 60\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "with open('./prompt/labelling_promptv4.txt', 'r', encoding='utf-8') as f:\n",
    "    system_prompt = f.read()\n",
    "dftrain         = pd.read_csv('./dataset/netpro_25k_train.csv')\n",
    "dfvalidation    = pd.read_csv('./dataset/netpro_25k_val.csv')\n",
    "\n",
    "# Convert to ShareGPT format with Unicode preservation\n",
    "train_dataset = to_sharegpt_with_thought(\n",
    "    system=system_prompt,\n",
    "    input_suffix=\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n\",\n",
    "    dataset=dftrain\n",
    ")\n",
    "validation_dataset = to_sharegpt_with_thought(\n",
    "    system=system_prompt,\n",
    "    input_suffix=\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n\",\n",
    "    dataset=dfvalidation\n",
    ")\n",
    "\n",
    "# Now you can check the type and print the Dataset info\n",
    "print(\"\\nOutput Type:\", type(train_dataset))\n",
    "print(\"Dataset Info:\")\n",
    "print(train_dataset)\n",
    "print(\"\\nOutput Type:\", type(validation_dataset))\n",
    "print(\"Dataset Info:\")\n",
    "print(validation_dataset)\n",
    "# Save the datasets to disk\n",
    "# train_dataset.save_to_disk('./dataset/train_dataset')\n",
    "# validation_dataset.save_to_disk('./dataset/validation_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab5966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 25008 conversations to JSONL...\n",
      "Successfully saved data to netpro_7k_sharegpt_thought_train.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving {len(train_dataset)} conversations to JSONL...\")\n",
    "\n",
    "try:\n",
    "    with open('./dataset/netpro_25k_sharegpt_thought_train.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in train_dataset:\n",
    "            json_line = {\"conversations\": item['conversations']}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + '\\n')\n",
    "    print(\"Successfully saved data to netpro_25k_sharegpt_thought_train.jsonl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ea62677",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dataset/netpro_25k_sharegpt_thought_train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        try:\n",
    "            json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error in line {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c63fa9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 60 conversations to JSONL...\n",
      "Successfully saved data to netpro_25k_sharegpt_thought_val.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving {len(validation_dataset)} conversations to JSONL...\")\n",
    "\n",
    "try:\n",
    "    with open('./dataset/netpro_25k_sharegpt_thought_val.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in validation_dataset:\n",
    "            json_line = {\"conversations\": item['conversations']}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + '\\n')\n",
    "    print(\"Successfully saved data to netpro_25k_sharegpt_thought_val.jsonl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a43bab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dataset/netpro_25k_sharegpt_thought_val.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        try:\n",
    "            json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error in line {i}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f0450",
   "metadata": {},
   "source": [
    "### Standardize ShareGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82fbc089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee8bf5b1f364228a08b3179de442030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f944e5054f2a44a0b09673e48577e003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from JSONL\n",
    "train_dataset = load_dataset(\"json\", data_files=\"./dataset/netpro_25k_sharegpt_thought_train.jsonl\")\n",
    "validation_dataset = load_dataset(\"json\", data_files=\"./dataset/netpro_25k_sharegpt_thought_val.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7a65817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 60\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbf0ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset[\"train\"] \n",
    "validation_dataset = validation_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b19459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357dc58c54b54614ba2299b04739145c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Standardizing formats (num_proc=32):   0%|          | 0/25008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52000e4432f841709edf48e523f832c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Standardizing formats (num_proc=32):   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "train_dataset = standardize_sharegpt(train_dataset, \"conversations\")\n",
    "validation_dataset = standardize_sharegpt(validation_dataset, \"conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0281ae2",
   "metadata": {},
   "source": [
    "### Save ChatML JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3867391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 25008 standardized conversations to JSONL...\n",
      "Successfully saved standardized data to netpro_25k_chatml_thought_train2.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving {len(train_dataset)} standardized conversations to JSONL...\")\n",
    "# Save standardized dataset in JSONL format\n",
    "try:\n",
    "    with open('./dataset/netpro_25k_chatml_thought_train2.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in train_dataset:\n",
    "            # Wrap each conversation in a dictionary with the key \"conversations\"\n",
    "            json_line = {\"conversations\": item['conversations']}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + '\\n')\n",
    "    print(\"Successfully saved standardized data to netpro_25k_chatml_thought_train2.jsonl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f26e301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 60 standardized conversations to JSONL...\n",
      "Successfully saved standardized data to netpro_25k_chatml_thought_val.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving {len(validation_dataset)} standardized conversations to JSONL...\")\n",
    "# Save standardized dataset in JSONL format\n",
    "try:\n",
    "    with open('./dataset/netpro_25k_chatml_thought_val.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in validation_dataset:\n",
    "            # Wrap each conversation in a dictionary with the key \"conversations\"\n",
    "            json_line = {\"conversations\": item['conversations']}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + '\\n')\n",
    "    print(\"Successfully saved standardized data to netpro_25k_chatml_thought_val.jsonl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491cce9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e110db04",
   "metadata": {},
   "source": [
    "### Upload To Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93194795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c58f4f6eab49c9bba0dc39ea432e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/25008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57833eb67b64ab68dfa1c638af8a926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from huggingface_hub import HfApi\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "# Combine train and validation datasets into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset\n",
    "})\n",
    "dataset_dict.save_to_disk(\"./dataset/chatml_7k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd74039c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b7a10e1f50459aac8fc09049c89bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9b9b46702147aa99e27e5caa85ebae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d245e99a3441c4b74cb98908afc477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b102f8a85ff7442a8f343c4beab98677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb3bc83324943d7af8853c4cc41bc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5354ea0bd5324ca6a2dacab866137190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully pushed to the Hugging Face Hub with train and validation splits.\n"
     ]
    }
   ],
   "source": [
    "# Push the DatasetDict to the Hugging Face Hub\n",
    "from datasets import load_from_disk\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Loads from .env file automatically\n",
    "\n",
    "if not os.getenv(\"HF_TOKEN\"):\n",
    "    raise ValueError(\"HF_TOKEN not found in .env file\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Reload the dataset to ensure it's in the correct format\n",
    "dataset_dict = load_from_disk(\"./dataset/chatml_thought_25k\")\n",
    "\n",
    "# Push to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(\"jordinia/netpro-finetune\", config_name=\"chatml_thought_25k\")\n",
    "\n",
    "print(\"Dataset successfully pushed to the Hugging Face Hub with train and validation splits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e24968c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53348aba763459b971c95dfb4dcd721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1bef696ffb4c179fd5799b8469d922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/39 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4438f5c958a6484c9cdd344206e06dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/39 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8412e515a4654cd69b2c405357e4550f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ded6bb848e41a7aa99bc5db59c6a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/a6/84/a6841de6caf821ca512f175ac20a94a238de85992149a4a1e9c31dc35b707563/6086db9d5d94a8531728633a7e81eafc3c12c1dbf6ffd073add1d25168e78404?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250514%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250514T161735Z&X-Amz-Expires=86400&X-Amz-Signature=b56d697e2d3bfdb75ecfcb2bb172192d5261e3377c8795ec352917ad1875abd0&X-Amz-SignedHeaders=host&partNumber=1&uploadId=FAQuc.F.RKlwC9kJnnK2HOw30z88hEnasfDbOMlESDnfCzTe0sXFR7Z7b4q5FesMNpm2tNuwged.T1ezfWnaV.ocUbNAUGsAPCd95dyuezeqkXDCvEtadJvRLVUHR7Ee&x-id=UploadPart (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2427)')))\"), '(Request ID: a22e2905-7739-407d-acb7-841f662baabf)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/a6/84/a6841de6caf821ca512f175ac20a94a238de85992149a4a1e9c31dc35b707563/6086db9d5d94a8531728633a7e81eafc3c12c1dbf6ffd073add1d25168e78404?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250514%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250514T161735Z&X-Amz-Expires=86400&X-Amz-Signature=b56d697e2d3bfdb75ecfcb2bb172192d5261e3377c8795ec352917ad1875abd0&X-Amz-SignedHeaders=host&partNumber=1&uploadId=FAQuc.F.RKlwC9kJnnK2HOw30z88hEnasfDbOMlESDnfCzTe0sXFR7Z7b4q5FesMNpm2tNuwged.T1ezfWnaV.ocUbNAUGsAPCd95dyuezeqkXDCvEtadJvRLVUHR7Ee&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\', port=443): Max retries exceeded with url: /repos/a6/84/a6841de6caf821ca512f175ac20a94a238de85992149a4a1e9c31dc35b707563/6086db9d5d94a8531728633a7e81eafc3c12c1dbf6ffd073add1d25168e78404?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250514%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250514T161735Z&X-Amz-Expires=86400&X-Amz-Signature=b56d697e2d3bfdb75ecfcb2bb172192d5261e3377c8795ec352917ad1875abd0&X-Amz-SignedHeaders=host&partNumber=1&uploadId=FAQuc.F.RKlwC9kJnnK2HOw30z88hEnasfDbOMlESDnfCzTe0sXFR7Z7b4q5FesMNpm2tNuwged.T1ezfWnaV.ocUbNAUGsAPCd95dyuezeqkXDCvEtadJvRLVUHR7Ee&x-id=UploadPart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A0C76788D0>: Failed to resolve \\'hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: bb171621-fdde-4d67-bb17-7cb59aa2af84)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/a6/84/a6841de6caf821ca512f175ac20a94a238de85992149a4a1e9c31dc35b707563/6086db9d5d94a8531728633a7e81eafc3c12c1dbf6ffd073add1d25168e78404?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250514%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250514T161735Z&X-Amz-Expires=86400&X-Amz-Signature=b56d697e2d3bfdb75ecfcb2bb172192d5261e3377c8795ec352917ad1875abd0&X-Amz-SignedHeaders=host&partNumber=1&uploadId=FAQuc.F.RKlwC9kJnnK2HOw30z88hEnasfDbOMlESDnfCzTe0sXFR7Z7b4q5FesMNpm2tNuwged.T1ezfWnaV.ocUbNAUGsAPCd95dyuezeqkXDCvEtadJvRLVUHR7Ee&x-id=UploadPart\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11e9daca41c4cce9746faf19f661159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pushed raw-7k config with train and validation splits.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Step 1: Load environment variables and token\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HF_TOKEN not found in .env file\")\n",
    "\n",
    "# Step 2: Load the raw CSVs into datasets\n",
    "train_dataset = Dataset.from_csv(\"./dataset/train.csv\")\n",
    "val_dataset = Dataset.from_csv(\"./dataset/test.csv\")\n",
    "\n",
    "# Step 3: Create a DatasetDict with train and validation splits\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "})\n",
    "\n",
    "# Step 4: Push to the Hub with config name \"raw-7k\"\n",
    "dataset_dict.push_to_hub(\"jordinia/netpro-finetune\", config_name=\"full\", token=hf_token)\n",
    "\n",
    "print(\"Successfully pushed raw-7k config with train and validation splits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25480c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "netpro_chatml_thought.jsonl: 100%|██████████| 710M/710M [01:22<00:00, 8.62MB/s]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jordinia/netpro-finetune/commit/e5a321ee51600a2778b55d5d33d8059d33dad9fb', commit_message='Initial dataset upload', commit_description='', oid='e5a321ee51600a2778b55d5d33d8059d33dad9fb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jordinia/netpro-finetune', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jordinia/netpro-finetune'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "# 1. Load environment variables\n",
    "load_dotenv()  # Loads from .env file automatically\n",
    "\n",
    "# 2. Verify token loading\n",
    "if not os.getenv(\"HF_TOKEN\"):\n",
    "    raise ValueError(\"HF_TOKEN not found in .env file\")\n",
    "\n",
    "# 3. Initialize and upload\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./dataset/netpro_chatml_thought.jsonl\",\n",
    "    path_in_repo=\"data/netpro_chatml_thought.jsonl\",\n",
    "    repo_id=\"jordinia/netpro-finetune\",\n",
    "    repo_type=\"dataset\",\n",
    "    commit_message=\"Initial dataset upload\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "776e2553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 33262/33262 [00:01<00:00, 19689.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# dataset = load_dataset(\"json\", data_files=\"./dataset/netpro_chatml_thought.jsonl\")\n",
    "\n",
    "dataset = load_dataset(\"jordinia/netpro-finetune\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "823d045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|██████████| 33262/33262 [00:00<00:00, 75387.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"./dataset/chatml_thought_33k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4163842a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fishmon/.conda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"./dataset/chatml_thought_33k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b942c610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 33262\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d332c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = dataset.train_test_split(test_size=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6e3d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 33128\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 134\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ef3be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 17/17 [00:00<00:00, 31.86ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 17/17 [00:00<00:00, 34.30ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:30<00:00, 15.36s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 265.26ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pushed raw-7k config with train and validation splits.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Loads from .env file automatically\n",
    "\n",
    "if not os.getenv(\"HF_TOKEN\"):\n",
    "    raise ValueError(\"HF_TOKEN not found in .env file\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# Step 4: Push to the Hub with config name \"raw-7k\"\n",
    "dataset_dict.push_to_hub(\"jordinia/netpro-finetune\", config_name=\"chatml_thought_33k\", token=hf_token)\n",
    "\n",
    "print(\"Successfully pushed raw-7k config with train and validation splits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "315458e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step     Train Loss   grad_norm    Learning Rate  \n",
      "--------------------------------------------------\n",
      "1        0.8794       66.098       0.0            \n",
      "2        0.1284       13.109       1e-05          \n",
      "3        0.1405       14.897       2e-05          \n",
      "4        0.0963       19.853       3e-05          \n",
      "5        0.1028       8.9562       4e-05          \n",
      "6        0.0356       4.6695       5e-05          \n",
      "7        0.0134       0.9691       6e-05          \n",
      "8        0.1371       34.132       7e-05          \n",
      "9        2.0593       63.384       8e-05          \n",
      "10       0.0004       0.0          9e-05          \n",
      "11       2.2151       61.359       0.0001         \n",
      "12       0.0007       0.0615       9.999999       \n",
      "13       -0.002       0.0264       9.999997       \n",
      "14       0.0035       1.2783       9.999994       \n",
      "15       2.8499       69.349       9.999989       \n",
      "16       -0.000       0.3443       9.999984       \n",
      "17       1.1129       246.83       9.999977       \n",
      "18       0.0023       0.0694       9.999968       \n",
      "19       0.0056       0.2041       9.999959       \n",
      "20       0.0047       0.0723       9.999948       \n",
      "21       0.0078       0.1375       9.999936       \n",
      "22       2.5549       66.016       9.999923       \n",
      "23       0.0042       0.2581       9.999908       \n",
      "24       1.7941       61.883       9.999892       \n",
      "25       0.0037       0.2685       9.999875       \n",
      "26       1.5572       62.718       9.999857       \n",
      "27       0.004        0.3041       9.999837       \n",
      "28       0.0057       0.4160       9.999816       \n",
      "29       3.4846       139.06       9.999794       \n",
      "30       0.0073       0.6420       9.999771       \n",
      "31       1.6707       60.397       9.999746       \n",
      "32       -0.000       0.5610       9.999720       \n",
      "33       0.0033       0.0901       9.999693       \n",
      "34       1.8747       63.903       9.999665       \n",
      "35       0.0111       0.0667       9.999635       \n",
      "36       2.9131       127.58       9.999604       \n",
      "37       2.7235       126.76       9.999571       \n",
      "38       2.0364       76.617       9.999538       \n",
      "39       0.2993       44.081       9.999503       \n",
      "40       0.2114       34.804       9.999467       \n",
      "41       1.6186       115.91       9.999430       \n",
      "42       2.454        110.21       9.999391       \n",
      "43       0.4472       54.161       9.999351       \n",
      "44       0.0908       14.970       9.999310       \n",
      "45       0.5421       46.282       9.999267       \n",
      "46       2.1038       59.720       9.999224       \n",
      "47       1.4424       90.532       9.999179       \n",
      "48       1.4362       76.635       9.999133       \n",
      "49       0.7601       38.976       9.999085       \n",
      "50       1.1431       83.007       9.999036       \n",
      "51       0.6686       50.333       9.998986       \n",
      "52       0.6852       69.671       9.998935       \n",
      "53       0.6808       54.483       9.998882       \n",
      "54       0.4477       53.093       9.998829       \n",
      "55       0.958        76.853       9.998774       \n",
      "56       0.1013       13.771       9.998717       \n",
      "57       0.5593       37.457       9.998660       \n",
      "58       0.5222       38.808       9.998601       \n",
      "59       0.6586       37.338       9.998541       \n",
      "60       0.4866       25.493       9.998479       \n",
      "61       0.2915       17.750       9.998416       \n",
      "62       0.0627       13.721       9.998352       \n",
      "63       0.3236       47.477       9.998287       \n",
      "64       0.0154       3.1557       9.998221       \n",
      "65       -0.002       0.9564       9.998153       \n",
      "66       1.2339       61.189       9.998084       \n",
      "67       -0.003       0.0618       9.998014       \n",
      "68       1.5631       61.478       9.997942       \n",
      "69       0.0015       0.1838       9.997869       \n",
      "70       0.0203       4.4761       9.997795       \n",
      "71       0.0038       0.0541       9.997720       \n",
      "72       2.3311       60.641       9.997643       \n",
      "73       0.0077       0.1791       9.997565       \n",
      "74       0.0002       0.0927       9.997486       \n",
      "75       2.0602       63.900       9.997406       \n",
      "76       1.8199       64.391       9.997324       \n",
      "77       0.009        0.9138       9.997241       \n",
      "78       1.1952       63.123       9.997157       \n",
      "79       1.0913       106.39       9.997072       \n",
      "80       0.0717       50.296       9.996985       \n",
      "81       0.0152       1.1137       9.996897       \n",
      "82       0.0035       0.5397       9.996808       \n",
      "83       1.5468       63.237       9.996717       \n",
      "84       2.0294       173.20       9.996625       \n",
      "85       2.188        104.11       9.996532       \n",
      "86       1.3572       120.24       9.996438       \n",
      "87       0.8228       42.367       9.996342       \n",
      "88       0.0073       0.7775       9.996245       \n",
      "89       0.3082       42.618       9.996147       \n",
      "90       0.3275       42.719       9.996048       \n",
      "91       1.142        58.718       9.995947       \n",
      "92       0.1799       34.447       9.995845       \n",
      "93       0.0057       0.3742       9.995742       \n",
      "94       0.8799       67.891       9.995638       \n",
      "95       0.3235       44.947       9.995532       \n",
      "96       0.4665       105.25       9.995425       \n",
      "97       6.2194       163.14       9.995317       \n",
      "98       4.6416       164.94       9.995207       \n",
      "99       3.097        106.32       9.995096       \n",
      "100      4.6951       163.48       9.994984       \n",
      "101      3.8597       159.65       9.994871       \n",
      "102      1.7937       104.52       9.994756       \n",
      "103      0.5714       40.496       9.994640       \n",
      "104      0.4385       45.594       9.994523       \n",
      "105      0.472        40.476       9.994405       \n",
      "106      1.1898       84.893       9.994285       \n",
      "107      0.1717       24.152       9.994164       \n",
      "108      0.1653       24.298       9.994042       \n",
      "109      0.7989       55.964       9.993919       \n",
      "110      0.1388       22.025       9.993794       \n",
      "111      0.5627       50.140       9.993668       \n",
      "112      0.5288       46.319       9.993541       \n",
      "113      0.0625       6.0162       9.993412       \n",
      "114      0.7956       115.93       9.993283       \n",
      "115      0.9853       53.805       9.993152       \n",
      "116      0.5112       75.838       9.993019       \n",
      "117      0.1167       21.444       9.992886       \n",
      "118      0.0951       10.211       9.992751       \n",
      "119      0.0012       0.1006       9.992615       \n",
      "120      0.0068       2.4729       9.992477       \n",
      "121      0.0448       9.1585       9.992339       \n",
      "122      0.0028       0.6479       9.992199       \n",
      "123      0.0067       0.0140       9.992058       \n",
      "124      -0.004       0.0904       9.991915       \n",
      "125      0.8652       70.403       9.991772       \n",
      "126      0.6236       121.96       9.991627       \n",
      "127      0.0063       0.2621       9.991481       \n",
      "128      0.0043       0.0          9.991333       \n",
      "129      0.0021       0.0220       9.991184       \n",
      "130      2.3857       58.153       9.991034       \n",
      "131      -0.005       0.0          9.990883       \n",
      "132      0.0077       0.0          9.990731       \n",
      "133      0.0031       0.0258       9.990577       \n",
      "134      4.7789       98.709       9.990422       \n",
      "135      0.2863       79.210       9.990265       \n",
      "136      0.0041       0.0513       9.990108       \n",
      "137      0.0029       0.1756       9.989949       \n",
      "138      0.8136       69.142       9.989789       \n",
      "139      0.0057       1.1574       9.989628       \n",
      "140      1.4029       56.730       9.989465       \n",
      "141      -0.004       0.0591       9.989301       \n",
      "142      0.0883       33.140       9.989136       \n",
      "143      1.2293       80.318       9.988969       \n",
      "144      0.2127       28.641       9.988802       \n",
      "145      0.1016       19.164       9.988633       \n",
      "146      0.0018       0.0325       9.988463       \n",
      "147      1.9251       111.41       9.988291       \n",
      "148      0.1198       23.866       9.988118       \n",
      "149      0.0005       0.0737       9.987944       \n",
      "150      0.0409       8.0851       9.987769       \n",
      "151      0.0093       0.6956       9.987592       \n",
      "152      0.0058       0.1858       9.987415       \n",
      "153      0.0049       0.1073       9.987236       \n",
      "154      1.6563       59.871       9.987055       \n",
      "155      2.0439       62.148       9.986874       \n",
      "156      3.8542       119.53       9.986691       \n",
      "157      0.0001       0.0          9.986507       \n",
      "158      0.0049       0.1224       9.986321       \n",
      "159      0.0044       0.1393       9.986135       \n",
      "160      -0.002       0.3197       9.985947       \n",
      "161      -0.000       0.5659       9.985758       \n",
      "162      1.9634       58.905       9.985567       \n",
      "163      1.4336       59.864       9.985375       \n",
      "164      -0.000       0.0420       9.985182       \n",
      "165      2.0559       56.175       9.984988       \n",
      "166      2.3109       62.452       9.984793       \n",
      "167      0.01         0.2442       9.984596       \n",
      "168      0.009        1.1570       9.984398       \n",
      "169      0.0112       0.6456       9.984199       \n",
      "170      0.0058       0.9159       9.983998       \n",
      "171      -0.001       0.6762       9.983796       \n",
      "172      0.0675       17.474       9.983593       \n",
      "173      0.0084       0.2960       9.983389       \n",
      "174      -0.000       0.0926       9.983183       \n",
      "175      0.0553       9.4988       9.982977       \n",
      "176      1.7869       59.209       9.982768       \n",
      "177      0.0179       3.8246       9.982559       \n",
      "178      -0.003       0.0219       9.982348       \n",
      "179      2.146        63.248       9.982137       \n",
      "180      0.0005       0.1511       9.981923       \n",
      "181      0.0072       0.8543       9.981709       \n",
      "182      0.0035       0.5635       9.981493       \n",
      "183      0.2225       191.55       9.981276       \n",
      "184      0.004        0.0628       9.981058       \n",
      "185      0.0045       0.1756       9.980839       \n",
      "186      0.0008       0.1379       9.980618       \n",
      "187      1.7209       61.441       9.980396       \n",
      "188      2.5816       60.168       9.980173       \n",
      "189      -0.000       1.1545       9.979948       \n",
      "190      5.3089       132.39       9.979722       \n",
      "191      0.0021       0.1580       9.979495       \n",
      "192      3.9438       105.68       9.979267       \n",
      "193      5.4122       101.11       9.979038       \n",
      "194      2.063        96.543       9.978807       \n",
      "195      2.4031       56.412       9.978575       \n",
      "196      0.2444       21.385       9.978341       \n",
      "197      0.1394       18.937       9.978107       \n",
      "198      3.5655       133.48       9.977871       \n",
      "199      3.153        149.84       9.977634       \n",
      "200      1.6685       136.19       9.977395       \n",
      "201      0.9117       68.634       9.977156       \n",
      "202      1.3468       88.133       9.976915       \n",
      "203      0.3193       29.661       9.976673       \n",
      "204      0.7419       55.934       9.976429       \n",
      "205      1.7687       47.598       9.976184       \n",
      "206      0.1784       24.418       9.975939       \n",
      "207      0.0534       5.9675       9.975691       \n",
      "208      0.0895       12.380       9.975443       \n",
      "209      0.0171       1.5034       9.975193       \n",
      "210      0.0114       1.0514       9.974942       \n",
      "211      1.1885       49.247       9.974690       \n",
      "212      0.0006       0.0559       9.974436       \n",
      "213      0.0032       0.5324       9.974182       \n",
      "214      2.406        47.072       9.973926       \n",
      "215      0.0037       0.0437       9.973668       \n",
      "216      3.2506       38.104       9.973410       \n",
      "217      0.0051       0.1439       9.973150       \n",
      "218      -0.001       0.0437       9.972889       \n",
      "219      0.0017       0.0395       9.972627       \n",
      "220      1.7784       49.089       9.972363       \n",
      "221      0.0049       1.0646       9.972098       \n",
      "222      0.048        11.838       9.971832       \n",
      "223      0.003        0.1588       9.971565       \n",
      "224      0.3452       282.26       9.971296       \n",
      "225      1.4965       49.315       9.971026       \n",
      "226      0.0061       0.0874       9.970755       \n",
      "227      1.4385       52.400       9.970483       \n",
      "228      0.0022       0.5483       9.970209       \n",
      "229      1.3417       50.730       9.969934       \n",
      "230      0.7094       44.508       9.969658       \n",
      "231      0.0053       0.6980       9.969380       \n",
      "232      0.0065       1.1593       9.969102       \n",
      "233      0.0802       15.589       9.968822       \n",
      "234      1.7691       72.909       9.968540       \n",
      "235      0.0181       2.9855       9.968258       \n",
      "236      0.0034       0.1012       9.967974       \n",
      "237      1.24         61.340       9.967689       \n",
      "238      0.0537       9.7327       9.967403       \n",
      "239      0.0594       12.515       9.967115       \n",
      "240      0.0687       13.558       9.966827       \n",
      "241      -0.000       0.0423       9.966537       \n",
      "242      4.2107       92.635       9.966245       \n",
      "243      2.0053       43.684       9.965953       \n",
      "244      -0.001       0.1686       9.965659       \n",
      "245      1.1011       55.028       9.965364       \n",
      "246      0.0021       0.2298       9.965068       \n",
      "247      0.0032       0.4285       9.964770       \n",
      "248      3.0903       99.334       9.964471       \n",
      "249      0.0525       9.7233       9.964171       \n",
      "250      0.703        38.356       9.963870       \n",
      "251      0.0034       0.0738       9.963567       \n",
      "252      1.0892       37.395       9.963263       \n",
      "253      0.0427       8.1601       9.962958       \n",
      "254      0.0194       3.9182       9.962652       \n",
      "255      0.0052       0.3917       9.962344       \n",
      "256      0.0068       1.4951       9.962035       \n",
      "257      1.7559       50.344       9.961725       \n",
      "258      0.0109       0.9293       9.961414       \n",
      "259      1.3635       36.286       9.961101       \n",
      "260      0.0003       0.1138       9.960787       \n",
      "261      0.001        0.1240       9.960472       \n",
      "262      -0.000       0.1259       9.960156       \n",
      "263      0.0005       0.1529       9.959838       \n",
      "264      0.0334       13.494       9.959519       \n",
      "265      0.0016       0.3220       9.959199       \n",
      "266      -0.000       0.1754       9.958877       \n",
      "267      1.9353       48.177       9.958555       \n",
      "268      0.0001       0.0901       9.958231       \n",
      "269      0.0097       1.1817       9.957905       \n",
      "270      5.5328       72.400       9.957579       \n",
      "271      0.0028       0.1880       9.957251       \n",
      "272      2.2724       55.661       9.956922       \n",
      "273      0.0077       0.1643       9.956592       \n",
      "274      0.0023       0.1269       9.956260       \n",
      "275      0.0024       0.0623       9.955928       \n",
      "276      2.6541       93.620       9.955594       \n",
      "277      1.5864       50.659       9.955258       \n",
      "278      -0.0         0.0678       9.954922       \n",
      "279      0.0006       0.1476       9.954584       \n",
      "280      -0.000       0.3874       9.954245       \n",
      "281      1.8899       32.788       9.953905       \n",
      "282      0.0074       1.0457       9.953563       \n",
      "283      0.0018       0.1597       9.953220       \n",
      "284      0.0026       0.4207       9.952876       \n",
      "285      1.2489       46.164       9.952531       \n",
      "286      0.0064       0.2351       9.952184       \n",
      "287      0.0208       2.7885       9.951837       \n",
      "288      0.0056       0.4184       9.951488       \n",
      "289      -0.0         0.1076       9.951137       \n",
      "290      1.4778       49.101       9.950786       \n",
      "291      0.0041       0.1497       9.950433       \n",
      "292      0.8791       31.184       9.950079       \n",
      "293      -0.001       0.2668       9.949723       \n",
      "294      0.0026       0.3116       9.949367       \n",
      "295      0.0118       1.4351       9.949009       \n",
      "296      0.0219       4.7108       9.948650       \n",
      "297      3.5045       161.82       9.948290       \n",
      "298      3.8588       98.738       9.947928       \n",
      "299      0.002        0.2646       9.947565       \n",
      "300      0.0047       0.6902       9.947201       \n",
      "301      0.0063       0.1822       9.946836       \n",
      "302      0.0021       0.0711       9.946469       \n",
      "303      -0.000       0.0499       9.946101       \n",
      "304      1.1261       74.497       9.945732       \n",
      "305      1.3736       48.563       9.945362       \n",
      "306      3.4179       110.56       9.944990       \n",
      "307      2.816        25.016       9.944617       \n",
      "308      1.4143       64.733       9.944243       \n",
      "309      0.001        0.2565       9.943868       \n",
      "310      0.0063       0.4847       9.943491       \n",
      "311      0.0055       0.6827       9.943113       \n",
      "312      1.7898       39.947       9.942734       \n",
      "313      0.0121       3.2402       9.942353       \n",
      "314      1.6562       38.663       9.941972       \n",
      "315      1.8936       40.428       9.941589       \n",
      "316      0.0544       4.9585       9.941205       \n",
      "317      0.023        1.9795       9.940819       \n",
      "318      0.0184       1.5947       9.940433       \n",
      "319      1.4921       38.336       9.940045       \n",
      "320      0.6531       36.359       9.939656       \n",
      "321      0.4658       46.292       9.939265       \n",
      "322      0.0205       2.1764       9.938873       \n",
      "323      0.0041       0.9219       9.938481       \n",
      "324      2.7479       64.031       9.938086       \n",
      "325      1.3316       61.602       9.937691       \n",
      "326      0.0072       0.1703       9.937294       \n",
      "327      0.558        38.764       9.936896       \n",
      "328      0.0845       10.598       9.936497       \n",
      "329      0.0314       2.8280       9.936097       \n",
      "330      0.0027       0.9397       9.935695       \n",
      "331      0.0084       0.9246       9.935292       \n",
      "332      1.5449       48.080       9.934888       \n",
      "333      0.0019       0.3838       9.934483       \n",
      "334      0.0163       1.6158       9.934076       \n",
      "335      0.0032       0.0479       9.933668       \n",
      "336      0.0409       7.1061       9.933259       \n",
      "337      0.0025       0.0          9.932848       \n",
      "338      -0.003       0.0          9.932437       \n",
      "339      -0.003       0.0127       9.932024       \n",
      "340      0.0023       0.0          9.931610       \n",
      "341      0.0088       0.0          9.931194       \n",
      "342      2.7039       70.627       9.930778       \n",
      "343      5.1637       122.31       9.930360       \n",
      "344      0.0086       0.0          9.929941       \n",
      "345      0.0034       0.0930       9.929520       \n",
      "346      6.142        67.230       9.929098       \n",
      "347      6.4694       169.73       9.928676       \n",
      "348      1.2373       85.947       9.928251       \n",
      "349      2.3707       109.06       9.927826       \n",
      "350      4.3395       86.897       9.927399       \n",
      "351      0.004        0.1876       9.926971       \n",
      "352      -0.002       0.0185       9.926542       \n",
      "353      -0.000       0.0          9.926112       \n",
      "354      0.0055       0.7072       9.925680       \n",
      "355      -0.001       0.1690       9.925247       \n",
      "356      1.5549       45.230       9.924813       \n",
      "357      0.7353       74.899       9.924378       \n",
      "358      0.0008       0.1808       9.923941       \n",
      "359      0.1267       18.826       9.923503       \n",
      "360      0.5394       42.390       9.923064       \n",
      "361      0.0543       9.3976       9.922624       \n",
      "362      0.0004       0.1906       9.922182       \n",
      "363      1.769        39.380       9.921739       \n",
      "364      0.0071       1.0327       9.921295       \n",
      "365      0.0064       1.0003       9.920850       \n",
      "366      0.0526       7.5072       9.920403       \n",
      "367      0.0033       0.1178       9.919955       \n",
      "368      0.0001       0.1098       9.919506       \n",
      "369      1.4162       160.43       9.919056       \n",
      "370      0.0019       0.2060       9.918604       \n",
      "371      -0.002       0.0644       9.918151       \n",
      "372      0.0051       0.0479       9.917697       \n",
      "373      -0.003       0.0465       9.917242       \n",
      "374      -0.001       0.0430       9.916785       \n",
      "375      0.0041       0.0472       9.916328       \n",
      "376      0.7377       43.935       9.915868       \n",
      "377      0.2493       45.188       9.915408       \n",
      "378      0.0048       0.1037       9.914947       \n",
      "379      -0.001       0.0332       9.914484       \n",
      "380      0.0002       0.2136       9.914020       \n",
      "381      -0.000       0.1387       9.913554       \n",
      "382      1.908        39.329       9.913088       \n",
      "383      0.0019       0.0514       9.912620       \n",
      "384      -0.000       0.0279       9.912151       \n",
      "385      0.0013       0.1224       9.911681       \n",
      "386      -0.001       0.0755       9.911209       \n",
      "387      0.0013       0.2291       9.910737       \n",
      "388      -0.000       0.0800       9.910262       \n",
      "389      0.0035       0.0501       9.909787       \n",
      "390      0.0088       0.2868       9.909311       \n",
      "391      2.377        71.498       9.908833       \n",
      "392      -0.002       0.0840       9.908354       \n",
      "393      0.0974       20.610       9.907874       \n",
      "394      -0.001       0.1470       9.907392       \n",
      "395      0.0034       0.0319       9.906910       \n",
      "396      0.0042       0.1623       9.906426       \n",
      "397      3.9071       99.598       9.905940       \n",
      "398      0.0011       0.1301       9.905454       \n",
      "399      0.0669       8.3697       9.904966       \n",
      "400      1.4491       39.845       9.904477       \n",
      "401      0.0068       0.0641       9.903987       \n",
      "402      -0.001       0.0197       9.903496       \n",
      "403      0.0944       22.160       9.903003       \n",
      "404      0.0055       0.1015       9.902509       \n",
      "405      -0.001       0.0          9.902014       \n",
      "406      0.4875       55.524       9.901518       \n",
      "407      -0.001       0.0          9.901020       \n",
      "408      0.0023       0.3570       9.900521       \n",
      "409      0.0053       0.2425       9.900021       \n",
      "410      1.0651       55.782       9.899520       \n",
      "411      0.003        0.7212       9.899017       \n",
      "412      -0.000       0.1798       9.898513       \n",
      "413      0.0037       0.1537       9.898008       \n",
      "414      1.0041       34.211       9.897502       \n",
      "415      0.0011       0.1145       9.896995       \n",
      "416      0.0082       0.2972       9.896486       \n",
      "417      -0.000       0.0189       9.895976       \n",
      "418      0.0026       0.5034       9.895464       \n",
      "419      0.0007       0.2836       9.894952       \n",
      "420      0.0077       0.2274       9.894438       \n",
      "421      0.029        3.4121       9.893923       \n",
      "422      0.0228       3.2435       9.893407       \n",
      "423      0.0073       0.0488       9.892890       \n",
      "424      0.0103       0.3766       9.892371       \n",
      "425      0.0031       0.0339       9.891851       \n",
      "426      0.0097       1.9644       9.891330       \n",
      "427      0.0097       2.1979       9.890807       \n",
      "428      2.4149       42.375       9.890284       \n",
      "429      0.0009       0.0          9.889759       \n",
      "430      3.2607       44.903       9.889233       \n",
      "431      2.3015       49.481       9.888705       \n",
      "432      2.0665       39.332       9.888177       \n",
      "433      0.0016       0.0800       9.887647       \n",
      "434      0.0106       1.4758       9.887116       \n",
      "435      0.0038       0.0320       9.886583       \n",
      "436      0.0009       0.1121       9.886050       \n",
      "437      0.0047       0.0709       9.885515       \n",
      "438      1.8477       34.875       9.884979       \n",
      "439      0.0029       0.1322       9.884442       \n",
      "440      0.0035       0.1288       9.883903       \n",
      "441      0.0079       0.9135       9.883363       \n",
      "442      3.076        62.394       9.882822       \n",
      "443      0.0085       0.3290       9.882280       \n",
      "444      0.0048       0.3373       9.881737       \n",
      "445      0.0027       0.1459       9.881192       \n",
      "446      1.5417       52.600       9.880646       \n",
      "447      0.3147       36.295       9.880099       \n",
      "448      1.3887       39.627       9.879550       \n",
      "449      0.0031       0.2337       9.879001       \n",
      "450      0.4125       38.037       9.878450       \n",
      "451      0.0028       0.1185       9.877898       \n",
      "452      0.022        3.1725       9.877344       \n",
      "453      0.0834       11.948       9.876790       \n",
      "454      0.0042       0.2669       9.876234       \n",
      "455      -0.000       0.0361       9.875677       \n",
      "456      0.0026       0.2826       9.875119       \n",
      "457      0.0039       0.0886       9.874559       \n",
      "458      0.003        0.2298       9.873998       \n",
      "459      0.0034       0.0481       9.873436       \n",
      "460      -0.000       0.0833       9.872873       \n",
      "461      -0.002       0.0213       9.872309       \n",
      "462      0.002        0.0270       9.871743       \n",
      "463      0.0006       0.0539       9.871176       \n",
      "464      0.0028       0.0308       9.870608       \n",
      "465      -0.001       0.0217       9.870038       \n",
      "466      0.0017       0.0209       9.869468       \n",
      "467      -0.000       0.0          9.868896       \n",
      "468      1.9265       32.544       9.868323       \n",
      "469      0.9587       95.172       9.867748       \n",
      "470      0.0011       0.0233       9.867173       \n",
      "471      0.0019       0.0152       9.866596       \n",
      "472      0.004        0.0270       9.866018       \n",
      "473      1.6714       38.070       9.865439       \n",
      "474      0.0025       0.0645       9.864858       \n",
      "475      0.004        0.0211       9.864276       \n",
      "476      1.1503       52.723       9.863694       \n",
      "477      0.0073       0.3302       9.863109       \n",
      "478      0.0048       0.5628       9.862524       \n",
      "479      1.9778       51.400       9.861937       \n",
      "480      0.0029       0.1062       9.861349       \n",
      "481      0.992        56.753       9.860760       \n",
      "482      0.0059       0.1915       9.860170       \n",
      "483      0.071        11.038       9.859578       \n",
      "484      0.0011       0.0399       9.858985       \n",
      "485      0.0262       5.3142       9.858391       \n",
      "486      0.0039       0.1739       9.857796       \n",
      "487      0.002        0.1938       9.857200       \n",
      "488      1.2556       48.916       9.856602       \n",
      "489      0.0022       0.0620       9.856003       \n",
      "490      0.7422       51.432       9.855403       \n",
      "491      0.0111       1.4619       9.854801       \n",
      "492      2.1814       34.832       9.854199       \n",
      "493      0.0156       1.7571       9.853595       \n",
      "494      0.0058       0.3754       9.852990       \n",
      "495      0.8496       46.295       9.852383       \n",
      "496      0.0066       0.4235       9.851776       \n",
      "497      0.0035       0.2874       9.851167       \n",
      "498      0.0238       3.3541       9.850557       \n",
      "499      0.0016       0.1593       9.849946       \n",
      "500      2.0755       97.229       9.849333       \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the trainer state\n",
    "with open(\"outputs/checkpoint-500/trainer_state.json\", \"r\") as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "# Print organized logs\n",
    "print(f\"\\n{'Step':<8} {'Train Loss':<12} {'grad_norm':<12} {'Learning Rate':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for entry in logs[\"log_history\"]:\n",
    "    step = entry.get(\"step\", \"N/A\")\n",
    "    train_loss = entry.get(\"loss\", \"N/A\")\n",
    "    eval_loss = entry.get(\"grad_norm\", \"N/A\")\n",
    "    lr = entry.get(\"learning_rate\", \"N/A\")\n",
    "    \n",
    "    print(f\"{step:<8} {str(train_loss)[:6]:<12} {str(eval_loss)[:6]:<12} {str(lr)[:8]:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72802655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
