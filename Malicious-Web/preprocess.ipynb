{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cfb8e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fishmon/.conda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from jsonschema import validate, ValidationError\n",
    "from datasets import Dataset, load_dataset\n",
    "import os\n",
    "\n",
    "def mergeCsv(output_file, *input_files):\n",
    "    \"\"\"\n",
    "    Merges multiple CSV files into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        output_file (str): The name of the output CSV file.\n",
    "        *input_files (str): Paths to the input CSV files to be merged.\n",
    "    \"\"\"\n",
    "    # List to store DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Read each CSV file and append to the list\n",
    "    for file in input_files:\n",
    "        if os.path.exists(file):\n",
    "            df = pd.read_csv(file)\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    if dataframes:\n",
    "        merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "        # Save the merged DataFrame to the output file\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"Merged CSV saved as: {output_file}\")\n",
    "    else:\n",
    "        print(\"No valid files to merge.\")\n",
    "\n",
    "\n",
    "def countRow(input_file):\n",
    "    \"\"\"\n",
    "    Counts the number of rows in a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of rows in the CSV file.\n",
    "    \"\"\"\n",
    "    if os.path.exists(input_file):\n",
    "        df = pd.read_csv(input_file)\n",
    "        row_count = len(df)\n",
    "        print(f\"Number of rows in {input_file}: {row_count}\")\n",
    "    else:\n",
    "        print(f\"File not found: {input_file}\")\n",
    "\n",
    "def checkDuplicate(file_path):\n",
    "    \"\"\"\n",
    "    Checks for duplicate rows in a CSV file based on the 'Domain' column.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check for duplicates based on the 'Domain' column\n",
    "        duplicates = df[df.duplicated(subset='Domain', keep=False)]\n",
    "\n",
    "        # Print the duplicates if any\n",
    "        if not duplicates.empty:\n",
    "            print(f\"Found {len(duplicates)} duplicate rows based on the 'Domain' column:\")\n",
    "        else:\n",
    "            print(\"No duplicates found based on the 'Domain' column.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "def removeDuplicate(file_path, output_file):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows in a CSV file based on the 'Domain' column, keeping the first occurrence.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "        output_file (str): Path to save the deduplicated CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Remove duplicates based on the 'Domain' column, keeping the first occurrence\n",
    "        deduplicated_df = df.drop_duplicates(subset='Domain', keep='first')\n",
    "\n",
    "        # Save the deduplicated DataFrame to a new file\n",
    "        deduplicated_df.to_csv(output_file, index=False)\n",
    "        print(f\"Duplicates removed. Deduplicated file saved as: {output_file}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "\n",
    "def load_and_print_all_columns(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file and print all columns.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(df.columns.tolist())\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53335fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/merged_combined_dedup_final.csv: 96828\n",
      "Found 4480 duplicate rows based on the 'Domain' column:\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/merged_combined_dedup_final.csv\")\n",
    "checkDuplicate(\"./dataset/merged_combined_dedup_final.csv\")\n",
    "load_and_print_all_columns(\"./dataset/merged_combined_dedup_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67700f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 79283 occurrences\n",
      "Label 2: 9230 occurrences\n",
      "Label 1: 7076 occurrences\n",
      "Label 3: 1239 occurrences\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = \"./dataset/merged_combined_dedup_final.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count occurrences of each unique value in the 'Label' column\n",
    "    label_counts = df['Label'].value_counts()\n",
    "    \n",
    "    # Print the occurrences\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count} occurrences\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8fbf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"./dataset/merged_combined_dedup_final.csv\")\n",
    "\n",
    "# Undersample Benign to 12,000\n",
    "benign = df[df[\"Label\"] == 0].sample(n=12000, random_state=42)\n",
    "\n",
    "# Use all Gambling and Pornography\n",
    "gambling = df[df[\"Label\"] == 1]\n",
    "porn = df[df[\"Label\"] == 2]\n",
    "\n",
    "# Oversample Harmful by 4x via duplication\n",
    "harmful = df[df[\"Label\"] == 3]\n",
    "harmful_oversampled = pd.concat([harmful] * 4, ignore_index=True)\n",
    "\n",
    "# Combine and shuffle\n",
    "balanced_df = pd.concat([benign, gambling, porn, harmful_oversampled])\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "balanced_df.to_csv(\"./dataset/balanced_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10bfb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/balanced_dataset.csv: 33262\n",
      "Found 6624 duplicate rows based on the 'Domain' column:\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/balanced_dataset.csv\")\n",
    "checkDuplicate(\"./dataset/balanced_dataset.csv\")\n",
    "load_and_print_all_columns(\"./dataset/balanced_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d46cc84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 12000 occurrences\n",
      "Label 2: 9230 occurrences\n",
      "Label 1: 7076 occurrences\n",
      "Label 3: 4956 occurrences\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "file_path = \"./dataset/balanced_dataset.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count occurrences of each unique value in the 'Label' column\n",
    "    label_counts = df['Label'].value_counts()\n",
    "    \n",
    "    # Print the occurrences\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count} occurrences\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb1416",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3698396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the schema for the classification dictionary (outside the function is cleaner)\n",
    "# classification_schema = {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"answer\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 3},\n",
    "#         \"classification\": {\"type\": \"string\"},\n",
    "#         \"reason\": {\"type\": \"string\"},\n",
    "#         \"confidence\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100}\n",
    "#     },\n",
    "#     \"required\": [\"answer\", \"classification\", \"reason\", \"confidence\"]\n",
    "# }\n",
    "\n",
    "# def to_sharegpt_with_thought(system, input_suffix, dataset) -> Dataset:\n",
    "#     \"\"\"\n",
    "#     Convert website classification dataset to ShareGPT format including reasoning ('thought'),\n",
    "#     with JSON validation and enhanced error handling, specifically checking for unhashable types.\n",
    "#     Returns a Hugging Face Dataset object.\n",
    "    \n",
    "#     Args:\n",
    "#         system (str): System prompt\n",
    "#         input_suffix (str): Suffix to append to the human message (can be empty if not needed)\n",
    "#         dataset (pd.DataFrame): Input DataFrame with columns:\n",
    "#             ['Domain', 'Content', 'Label', 'classification', 'reason', 'confidence', 'thought']\n",
    "            \n",
    "#     Returns:\n",
    "#         datasets.Dataset: Hugging Face Dataset with a 'conversations' column, \n",
    "#                           where each row contains a list representing one conversation.\n",
    "#                           Returns an empty Dataset if input dataset is empty or all rows fail.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(dataset, pd.DataFrame) or dataset.empty:\n",
    "#         print(\"Input is not a valid or non-empty DataFrame. Returning empty Dataset.\")\n",
    "#         # Return an empty Dataset with the expected structure\n",
    "#         return Dataset.from_dict({\"conversations\": []}) \n",
    "        \n",
    "#     # This list will temporarily hold the conversation lists\n",
    "#     conversation_data_list = [] \n",
    "#     error_count = 0\n",
    "#     processed_count = 0\n",
    "\n",
    "#     human_template = f\"{input_suffix}\\nDomain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "#     if not input_suffix:\n",
    "#          human_template = f\"Domain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "\n",
    "#     print(f\"Starting conversion for {len(dataset)} rows...\")\n",
    "\n",
    "#     for idx, row in dataset.iterrows():\n",
    "#         try:\n",
    "#             # --- Data Extraction and Basic Type Check ---\n",
    "#             domain = row.get('Domain', 'N/A') \n",
    "#             content = str(row['Content']) if pd.notna(row['Content']) else \"\" \n",
    "#             thought_text = str(row['thought']).strip() if pd.notna(row['thought']) else \"No thought provided.\"\n",
    "            \n",
    "#             label = row['Label']\n",
    "#             classification = row['classification']\n",
    "#             reason = row['reason']\n",
    "#             confidence = row['confidence']\n",
    "\n",
    "#             if pd.isna(label) or pd.isna(classification) or pd.isna(reason) or pd.isna(confidence):\n",
    "#                  raise ValueError(\"One or more required classification fields are NaN\")\n",
    "\n",
    "#             if isinstance(label, (list, dict)) or \\\n",
    "#                isinstance(classification, (list, dict)) or \\\n",
    "#                isinstance(reason, (list, dict)) or \\\n",
    "#                isinstance(confidence, (list, dict)):\n",
    "#                  raise TypeError(\"One or more classification fields contain unhashable list/dict types\")\n",
    "\n",
    "#             human_value = human_template.format(domain=domain, content=content)\n",
    "            \n",
    "#             # --- Prepare and Validate Classification Dictionary ---\n",
    "#             classification_dict = {\n",
    "#                 \"answer\": int(label), \n",
    "#                 \"classification\": str(classification),\n",
    "#                 \"reason\": str(reason), \n",
    "#                 \"confidence\": int(confidence)\n",
    "#             }\n",
    "#             validate(instance=classification_dict, schema=classification_schema) \n",
    "            \n",
    "#             # --- Serialize and Format Output ---\n",
    "#             final_json_str = json.dumps(classification_dict, ensure_ascii=False, indent=2) \n",
    "#             gpt_value = f\"<think>\\n{thought_text}\\n</think>\\n```json\\n{final_json_str}\\n```\"\n",
    "            \n",
    "#             # This is the list for a single conversation\n",
    "#             conversation = [\n",
    "#                 {\"from\": \"system\", \"value\": system},\n",
    "#                 {\"from\": \"human\", \"value\": human_value},\n",
    "#                 {\"from\": \"gpt\", \"value\": gpt_value} \n",
    "#             ]\n",
    "            \n",
    "#             # Append the conversation list to our temporary list\n",
    "#             conversation_data_list.append(conversation) \n",
    "#             processed_count += 1\n",
    "\n",
    "#         # --- Error Handling ---\n",
    "#         except (ValidationError, ValueError, TypeError) as e: \n",
    "#             error_count += 1\n",
    "#             # Avoid printing excessive errors if many occur\n",
    "#             if error_count < 20 or error_count % 100 == 0: \n",
    "#                  print(f\"Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "#             continue \n",
    "#         except Exception as e: \n",
    "#              error_count += 1\n",
    "#              if error_count < 20 or error_count % 100 == 0:\n",
    "#                  print(f\"UNEXPECTED Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "#              continue\n",
    "\n",
    "#     print(f\"\\nConversion finished.\")\n",
    "#     print(f\"Successfully processed: {processed_count} rows\")\n",
    "#     print(f\"Errors encountered: {error_count} rows\")\n",
    "\n",
    "#     # --- Convert the list of conversation lists to a Hugging Face Dataset ---\n",
    "#     if conversation_data_list:\n",
    "#         # Create the dictionary format expected by from_dict\n",
    "#         hf_dataset_dict = {\"conversations\": conversation_data_list} \n",
    "#         # Create and return the Dataset object\n",
    "#         return Dataset.from_dict(hf_dataset_dict)\n",
    "#     else:\n",
    "#         print(\"No valid data processed. Returning empty Dataset.\")\n",
    "#         # Return an empty Dataset with the expected structure\n",
    "#         return Dataset.from_dict({\"conversations\": []})\n",
    "    \n",
    "\n",
    "# Define the schema for the classification dictionary\n",
    "classification_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"answer\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 3},\n",
    "        \"classification\": {\"type\": \"string\"},\n",
    "        \"reason\": {\"type\": \"string\"},\n",
    "        \"confidence\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100}\n",
    "    },\n",
    "    \"required\": [\"answer\", \"classification\", \"reason\", \"confidence\"]\n",
    "}\n",
    "\n",
    "def to_sharegpt_with_thought(system, input_suffix, dataset) -> Dataset:\n",
    "    \"\"\"\n",
    "    Convert website classification dataset to ShareGPT format including reasoning ('Thought'),\n",
    "    with JSON validation and enhanced error handling, specifically checking for unhashable types.\n",
    "    Returns a Hugging Face Dataset object.\n",
    "    \n",
    "    Args:\n",
    "        system (str): System prompt\n",
    "        input_suffix (str): Suffix to append to the human message (can be empty if not needed)\n",
    "        dataset (pd.DataFrame): Input DataFrame with columns:\n",
    "            ['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n",
    "            \n",
    "    Returns:\n",
    "        datasets.Dataset: Hugging Face Dataset with a 'conversations' column, \n",
    "                          where each row contains a list representing one conversation\n",
    "    \"\"\"\n",
    "    # Validate input DataFrame\n",
    "    if not isinstance(dataset, pd.DataFrame) or dataset.empty:\n",
    "        print(\"Input is not a valid or non-empty DataFrame. Returning empty Dataset.\")\n",
    "        return Dataset.from_dict({\"conversations\": []}) \n",
    "        \n",
    "    # Validate required columns\n",
    "    required_columns = ['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n",
    "    if not all(col in dataset.columns for col in required_columns):\n",
    "        missing = [col for col in required_columns if col not in dataset.columns]\n",
    "        print(f\"Missing required columns: {missing}. Returning empty Dataset.\")\n",
    "        return Dataset.from_dict({\"conversations\": []})\n",
    "\n",
    "    conversation_data_list = [] \n",
    "    error_count = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Template for human message\n",
    "    human_template = f\"{input_suffix}\\nDomain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "    if not input_suffix:\n",
    "        human_template = f\"Domain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "    \n",
    "    print(f\"Starting conversion for {len(dataset)} rows...\")\n",
    "    \n",
    "    for idx, row in dataset.iterrows():\n",
    "        try:\n",
    "            # Extract data from DataFrame row\n",
    "            domain = str(row['Domain']) if pd.notna(row['Domain']) else \"N/A\"\n",
    "            content = str(row['Content']) if pd.notna(row['Content']) else \"\"\n",
    "            thought = str(row['Thought']) if pd.notna(row['Thought']) else \"No thought provided.\"\n",
    "            \n",
    "            # Validate classification fields\n",
    "            label = row['Label']\n",
    "            classification = row['Classification']\n",
    "            reason = row['Reason']\n",
    "            confidence = row['Confidence']\n",
    "            \n",
    "            if pd.isna(label) or pd.isna(classification) or pd.isna(reason) or pd.isna(confidence):\n",
    "                raise ValueError(\"One or more required classification fields are NaN\")\n",
    "                \n",
    "            if isinstance(label, (list, dict)) or isinstance(classification, (list, dict)) or \\\n",
    "               isinstance(reason, (list, dict)) or isinstance(confidence, (list, dict)):\n",
    "                raise TypeError(\"One or more classification fields contain unhashable list/dict types\")\n",
    "            \n",
    "            # Create human message\n",
    "            human_value = human_template.format(domain=domain, content=content)\n",
    "            \n",
    "            # Prepare and validate classification dictionary\n",
    "            classification_dict = {\n",
    "                \"answer\": int(label),\n",
    "                \"classification\": str(classification),\n",
    "                \"reason\": str(reason),\n",
    "                \"confidence\": int(confidence)\n",
    "            }\n",
    "            validate(instance=classification_dict, schema=classification_schema)\n",
    "            \n",
    "            # Create GPT response with thought and JSON\n",
    "            final_json_str = json.dumps(classification_dict, ensure_ascii=False, indent=2)\n",
    "            gpt_value = f\"<think>\\n{thought}\\n</think>\\n```json\\n{final_json_str}\\n```\"\n",
    "            # Create conversation structure\n",
    "            conversation = [\n",
    "                {\"from\": \"system\", \"value\": system},\n",
    "                {\"from\": \"human\", \"value\": human_value},\n",
    "                {\"from\": \"gpt\", \"value\": gpt_value}\n",
    "            ]\n",
    "            \n",
    "            conversation_data_list.append(conversation)\n",
    "            processed_count += 1\n",
    "            \n",
    "        except (ValidationError, ValueError, TypeError) as e:\n",
    "            error_count += 1\n",
    "            if error_count < 20 or error_count % 100 == 0:\n",
    "                print(f\"Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            if error_count < 20 or error_count % 100 == 0:\n",
    "                print(f\"UNEXPECTED Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Print conversion summary\n",
    "    print(f\"\\nConversion finished.\")\n",
    "    print(f\"Successfully processed: {processed_count} rows\")\n",
    "    print(f\"Errors encountered: {error_count} rows\")\n",
    "    \n",
    "    # Return Dataset\n",
    "    if conversation_data_list:\n",
    "        return Dataset.from_dict({\"conversations\": conversation_data_list})\n",
    "    else:\n",
    "        print(\"No valid data processed. Returning empty Dataset.\")\n",
    "        return Dataset.from_dict({\"conversations\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ce0246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversion for 33262 rows...\n",
      "\n",
      "Conversion finished.\n",
      "Successfully processed: 33262 rows\n",
      "Errors encountered: 0 rows\n",
      "\n",
      "Output Type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset Info:\n",
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 33262\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./dataset/balanced_dataset.csv')\n",
    "with open('./prompt/labelling_promptv4.txt', 'r', encoding='utf-8') as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "# Convert to ShareGPT format with Unicode preservation\n",
    "dataset = to_sharegpt_with_thought(\n",
    "    system=system_prompt,\n",
    "    input_suffix=\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n\",\n",
    "    dataset=df\n",
    ")\n",
    "\n",
    "# Now you can check the type and print the Dataset info\n",
    "print(\"\\nOutput Type:\", type(dataset))\n",
    "print(\"Dataset Info:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bab5966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 33262 conversations to JSONL...\n",
      "Successfully saved data to netpro_sharegpt_thought.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving {len(dataset)} conversations to JSONL...\")\n",
    "\n",
    "try:\n",
    "    with open('./dataset/netpro_sharegpt_thought.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in dataset:\n",
    "            json_line = {\"conversations\": item['conversations']}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + '\\n')\n",
    "    print(\"Successfully saved data to netpro_sharegpt_thought.jsonl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea62677",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dataset/netpro_sharegpt_thought.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        try:\n",
    "            json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error in line {i}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f0450",
   "metadata": {},
   "source": [
    "### Standardize ShareGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82fbc089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 33262 examples [00:00, 40941.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from JSONL\n",
    "dataset = load_dataset(\"json\", data_files=\"./dataset/netpro_sharegpt_thought.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948a5834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 33262\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbf0ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"]  # Access the 'train' split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54a1fdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output Type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset Info:\n",
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 33262\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOutput Type:\", type(dataset))\n",
    "print(\"Dataset Info:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b19459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1 with CUDA 1201 (you have 2.6.0+cu124)\n",
      "    Python  3.11.10 (you have 3.11.11)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Standardizing formats (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33262/33262 [00:01<00:00, 27355.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0281ae2",
   "metadata": {},
   "source": [
    "### Save ChatML JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3867391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 33262 standardized conversations to JSONL...\n",
      "Successfully saved standardized data to netpro_chatml_thought.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving {len(dataset)} standardized conversations to JSONL...\")\n",
    "# Save standardized dataset in JSONL format\n",
    "try:\n",
    "    with open('./dataset/netpro_chatml_thought.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in dataset:\n",
    "            # Wrap each conversation in a dictionary with the key \"conversations\"\n",
    "            json_line = {\"conversations\": item['conversations']}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + '\\n')\n",
    "    print(\"Successfully saved standardized data to netpro_chatml_thought.jsonl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110db04",
   "metadata": {},
   "source": [
    "### Upload To Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25480c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "netpro_chatml_thought.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 710M/710M [01:22<00:00, 8.62MB/s]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jordinia/netpro-finetune/commit/e5a321ee51600a2778b55d5d33d8059d33dad9fb', commit_message='Initial dataset upload', commit_description='', oid='e5a321ee51600a2778b55d5d33d8059d33dad9fb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jordinia/netpro-finetune', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jordinia/netpro-finetune'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 1. Load environment variables\n",
    "load_dotenv()  # Loads from .env file automatically\n",
    "\n",
    "# 2. Verify token loading\n",
    "if not os.getenv(\"HF_TOKEN\"):\n",
    "    raise ValueError(\"HF_TOKEN not found in .env file\")\n",
    "\n",
    "# 3. Initialize and upload\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./dataset/netpro_chatml_thought.jsonl\",\n",
    "    path_in_repo=\"data/netpro_chatml_thought.jsonl\",\n",
    "    repo_id=\"jordinia/netpro-finetune\",\n",
    "    repo_type=\"dataset\",\n",
    "    commit_message=\"Initial dataset upload\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
