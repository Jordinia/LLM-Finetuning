{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cfb8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def mergeCsv(output_file, *input_files):\n",
    "    \"\"\"\n",
    "    Merges multiple CSV files into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        output_file (str): The name of the output CSV file.\n",
    "        *input_files (str): Paths to the input CSV files to be merged.\n",
    "    \"\"\"\n",
    "    # List to store DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Read each CSV file and append to the list\n",
    "    for file in input_files:\n",
    "        if os.path.exists(file):\n",
    "            df = pd.read_csv(file)\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    if dataframes:\n",
    "        merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "        # Save the merged DataFrame to the output file\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"Merged CSV saved as: {output_file}\")\n",
    "    else:\n",
    "        print(\"No valid files to merge.\")\n",
    "\n",
    "\n",
    "def countRow(input_file):\n",
    "    \"\"\"\n",
    "    Counts the number of rows in a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of rows in the CSV file.\n",
    "    \"\"\"\n",
    "    if os.path.exists(input_file):\n",
    "        df = pd.read_csv(input_file)\n",
    "        row_count = len(df)\n",
    "        print(f\"Number of rows in {input_file}: {row_count}\")\n",
    "    else:\n",
    "        print(f\"File not found: {input_file}\")\n",
    "\n",
    "def checkDuplicate(file_path):\n",
    "    \"\"\"\n",
    "    Checks for duplicate rows in a CSV file based on the 'Domain' column.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check for duplicates based on the 'Domain' column\n",
    "        duplicates = df[df.duplicated(subset='Domain', keep=False)]\n",
    "\n",
    "        # Print the duplicates if any\n",
    "        if not duplicates.empty:\n",
    "            print(f\"Found {len(duplicates)} duplicate rows based on the 'Domain' column:\")\n",
    "        else:\n",
    "            print(\"No duplicates found based on the 'Domain' column.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "def removeDuplicate(file_path, output_file):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows in a CSV file based on the 'Domain' column, keeping the first occurrence.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "        output_file (str): Path to save the deduplicated CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Remove duplicates based on the 'Domain' column, keeping the first occurrence\n",
    "        deduplicated_df = df.drop_duplicates(subset='Domain', keep='first')\n",
    "\n",
    "        # Save the deduplicated DataFrame to a new file\n",
    "        deduplicated_df.to_csv(output_file, index=False)\n",
    "        print(f\"Duplicates removed. Deduplicated file saved as: {output_file}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "\n",
    "def load_and_print_all_columns(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file and print all columns.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(df.columns.tolist())\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53335fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/merged_combined_dedup_final.csv: 96926\n",
      "Found 4491 duplicate rows based on the 'Domain' column:\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/merged_combined_dedup_final.csv\")\n",
    "checkDuplicate(\"./dataset/merged_combined_dedup_final.csv\")\n",
    "load_and_print_all_columns(\"./dataset/merged_combined_dedup_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7d92f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First occurrences saved to: ./dataset/duplicated3.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "input_file = \"./dataset/merged_combined_dedup_final.csv\"\n",
    "output_file = \"./dataset/duplicated3.csv\"\n",
    "\n",
    "# Check if the input file exists\n",
    "if os.path.exists(input_file):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Drop duplicates based on the 'Domain' column, keeping the first occurrence\n",
    "    deduplicated_df = df.drop_duplicates(subset='Domain', keep='first')\n",
    "    \n",
    "    # Save the deduplicated DataFrame to a new file\n",
    "    deduplicated_df.to_csv(output_file, index=False)\n",
    "    print(f\"First occurrences saved to: {output_file}\")\n",
    "else:\n",
    "    print(f\"File not found: {input_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf51f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/duplicated3.csv: 93953\n",
      "No duplicates found based on the 'Domain' column.\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/duplicated3.csv\")\n",
    "checkDuplicate(\"./dataset/duplicated3.csv\")\n",
    "load_and_print_all_columns(\"./dataset/duplicated3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67700f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 77451 occurrences\n",
      "Label 2: 8662 occurrences\n",
      "Label 1: 6618 occurrences\n",
      "Label 3: 1222 occurrences\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = \"./dataset/duplicated3.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count occurrences of each unique value in the 'Label' column\n",
    "    label_counts = df['Label'].value_counts()\n",
    "    \n",
    "    # Print the occurrences\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count} occurrences\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8fbf619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read the original CSV file\n",
    "file_path = \"./dataset/duplicated3.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Split into separate DataFrames for each label\n",
    "label0 = df[df['Label'] == 0]\n",
    "label1 = df[df['Label'] == 1]\n",
    "label2 = df[df['Label'] == 2]\n",
    "label3 = df[df['Label'] == 3]\n",
    "\n",
    "# Save Label1, Label2, Label3 to CSV files\n",
    "label1.to_csv(\"Label1.csv\", index=False)\n",
    "label2.to_csv(\"Label2.csv\", index=False)\n",
    "label3.to_csv(\"Label3.csv\", index=False)\n",
    "\n",
    "# Shuffle Label0 and limit to 9,000 rows\n",
    "label0_shuffled = label0.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "label0_sampled = label0_shuffled.head(9000)\n",
    "\n",
    "# Save the sampled Label0 to CSV\n",
    "label0_sampled.to_csv(\"Label0.csv\", index=False)\n",
    "\n",
    "print(\"Files created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10bfb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/Label0.csv: 9000\n",
      "No duplicates found based on the 'Domain' column.\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/Label0.csv\")\n",
    "checkDuplicate(\"./dataset/Label0.csv\")\n",
    "load_and_print_all_columns(\"./dataset/Label0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccc8142b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/Label1.csv: 6618\n",
      "No duplicates found based on the 'Domain' column.\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/Label1.csv\")\n",
    "checkDuplicate(\"./dataset/Label1.csv\")\n",
    "load_and_print_all_columns(\"./dataset/Label0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59a2fd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/Label2.csv: 8662\n",
      "No duplicates found based on the 'Domain' column.\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/Label2.csv\")\n",
    "checkDuplicate(\"./dataset/Label2.csv\")\n",
    "load_and_print_all_columns(\"./dataset/Label2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "421f8e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/Label3.csv: 1222\n",
      "No duplicates found based on the 'Domain' column.\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/Label3.csv\")\n",
    "checkDuplicate(\"./dataset/Label3.csv\")\n",
    "load_and_print_all_columns(\"./dataset/Label3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc7eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
