{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfb8e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fishmon/.conda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from jsonschema import validate, ValidationError\n",
    "from datasets import Dataset, load_dataset\n",
    "import os\n",
    "\n",
    "def mergeCsv(output_file, *input_files):\n",
    "    \"\"\"\n",
    "    Merges multiple CSV files into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        output_file (str): The name of the output CSV file.\n",
    "        *input_files (str): Paths to the input CSV files to be merged.\n",
    "    \"\"\"\n",
    "    # List to store DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Read each CSV file and append to the list\n",
    "    for file in input_files:\n",
    "        if os.path.exists(file):\n",
    "            df = pd.read_csv(file)\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    if dataframes:\n",
    "        merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "        # Save the merged DataFrame to the output file\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"Merged CSV saved as: {output_file}\")\n",
    "    else:\n",
    "        print(\"No valid files to merge.\")\n",
    "\n",
    "\n",
    "def countRow(input_file):\n",
    "    \"\"\"\n",
    "    Counts the number of rows in a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of rows in the CSV file.\n",
    "    \"\"\"\n",
    "    if os.path.exists(input_file):\n",
    "        df = pd.read_csv(input_file)\n",
    "        row_count = len(df)\n",
    "        print(f\"Number of rows in {input_file}: {row_count}\")\n",
    "    else:\n",
    "        print(f\"File not found: {input_file}\")\n",
    "\n",
    "def checkDuplicate(file_path):\n",
    "    \"\"\"\n",
    "    Checks for duplicate rows in a CSV file based on the 'Domain' column.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check for duplicates based on the 'Domain' column\n",
    "        duplicates = df[df.duplicated(subset='Domain', keep=False)]\n",
    "\n",
    "        # Print the duplicates if any\n",
    "        if not duplicates.empty:\n",
    "            print(f\"Found {len(duplicates)} duplicate rows based on the 'Domain' column:\")\n",
    "        else:\n",
    "            print(\"No duplicates found based on the 'Domain' column.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "def removeDuplicate(file_path, output_file):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows in a CSV file based on the 'Domain' column, keeping the first occurrence.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "        output_file (str): Path to save the deduplicated CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Remove duplicates based on the 'Domain' column, keeping the first occurrence\n",
    "        deduplicated_df = df.drop_duplicates(subset='Domain', keep='first')\n",
    "\n",
    "        # Save the deduplicated DataFrame to a new file\n",
    "        deduplicated_df.to_csv(output_file, index=False)\n",
    "        print(f\"Duplicates removed. Deduplicated file saved as: {output_file}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "\n",
    "def load_and_print_all_columns(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file and print all columns.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(df.columns.tolist())\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53335fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/merged_combined.csv: 96828\n",
      "Found 4480 duplicate rows based on the 'Domain' column:\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/merged_combined.csv\")\n",
    "checkDuplicate(\"./dataset/merged_combined.csv\")\n",
    "load_and_print_all_columns(\"./dataset/merged_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67700f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 79283 occurrences\n",
      "Label 2: 9230 occurrences\n",
      "Label 1: 7076 occurrences\n",
      "Label 3: 1239 occurrences\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = \"./dataset/merged_combined.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count occurrences of each unique value in the 'Label' column\n",
    "    label_counts = df['Label'].value_counts()\n",
    "    \n",
    "    # Print the occurrences\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count} occurrences\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e42f86ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed. Deduplicated file saved as: ./dataset/merged_combined_dedup.csv\n",
      "Number of rows in ./dataset/merged_combined_dedup.csv: 93862\n",
      "No duplicates found based on the 'Domain' column.\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "removeDuplicate(\"./dataset/merged_combined.csv\", \"./dataset/merged_combined_dedup.csv\")\n",
    "countRow(\"./dataset/merged_combined_dedup.csv\")\n",
    "checkDuplicate(\"./dataset/merged_combined_dedup.csv\")\n",
    "load_and_print_all_columns(\"./dataset/merged_combined_dedup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a11e726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 77385 occurrences\n",
      "Label 2: 8654 occurrences\n",
      "Label 1: 6618 occurrences\n",
      "Label 3: 1205 occurrences\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = \"./dataset/merged_combined_dedup.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count occurrences of each unique value in the 'Label' column\n",
    "    label_counts = df['Label'].value_counts()\n",
    "    \n",
    "    # Print the occurrences\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count} occurrences\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8fbf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"./dataset/merged_combined_dedup.csv\")\n",
    "\n",
    "# Configure sample sizes for each label\n",
    "label_sample_sizes = {\n",
    "    0: 2500,  # Benign\n",
    "    1: 1800,   # Gambling (use all)\n",
    "    2: 1800,   # Pornography (use all)\n",
    "    3: 1205    # Harmful (exact sample size or oversample if specified explicitly)\n",
    "}\n",
    "\n",
    "# Process each label\n",
    "dataframes = []\n",
    "for label, sample_size in label_sample_sizes.items():\n",
    "    label_df = df[df[\"Label\"] == label]\n",
    "    if sample_size is None:\n",
    "        # Use all rows for this label\n",
    "        dataframes.append(label_df)\n",
    "    else:\n",
    "        # Undersample or use exact sample size\n",
    "        sampled_df = label_df.sample(n=sample_size, random_state=42)\n",
    "        dataframes.append(sampled_df)\n",
    "\n",
    "# Combine and shuffle\n",
    "balanced_df = pd.concat(dataframes)\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "balanced_df.to_csv(\"./dataset/balanced_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10bfb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/balanced_dataset.csv: 7305\n",
      "No duplicates found based on the 'Domain' column.\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/balanced_dataset.csv\")\n",
    "checkDuplicate(\"./dataset/balanced_dataset.csv\")\n",
    "load_and_print_all_columns(\"./dataset/balanced_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46cc84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 2500 occurrences\n",
      "Label 2: 1800 occurrences\n",
      "Label 1: 1800 occurrences\n",
      "Label 3: 1205 occurrences\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "file_path = \"./dataset/balanced_dataset.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Count occurrences of each unique value in the 'Label' column\n",
    "    label_counts = df['Label'].value_counts()\n",
    "    \n",
    "    # Print the occurrences\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count} occurrences\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2b6d0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset saved as './dataset/netpro_7k_val.csv'\n",
      "Updated dataset saved as './dataset/netpro_7k_train.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the deduplicated dataset\n",
    "file_path = \"./dataset/balanced_dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Configure the number of samples to take from each label\n",
    "label_sample_sizes = {\n",
    "    0: 18,  # Benign\n",
    "    1: 18,  # Gambling\n",
    "    2: 18,  # Pornography\n",
    "    3: 6   # Harmful\n",
    "}\n",
    "\n",
    "# Process each label\n",
    "validation_dataframes = []\n",
    "remaining_dataframes = []\n",
    "\n",
    "for label, sample_size in label_sample_sizes.items():\n",
    "    label_df = df[df[\"Label\"] == label]\n",
    "    \n",
    "    # Take 10 samples for validation\n",
    "    validation_sample = label_df.sample(n=sample_size, random_state=42)\n",
    "    validation_dataframes.append(validation_sample)\n",
    "    \n",
    "    # Remove the sampled rows from the original dataset\n",
    "    remaining_data = label_df.drop(validation_sample.index)\n",
    "    remaining_dataframes.append(remaining_data)\n",
    "\n",
    "# Combine validation samples and save\n",
    "validation_df = pd.concat(validation_dataframes).reset_index(drop=True)\n",
    "validation_df.to_csv(\"./dataset/netpro_7k_val.csv\", index=False)\n",
    "print(\"Validation dataset saved as './dataset/netpro_7k_val.csv'\")\n",
    "\n",
    "# Combine remaining data and save back to the original file\n",
    "remaining_df = pd.concat(remaining_dataframes).reset_index(drop=True)\n",
    "remaining_df.to_csv(\"./dataset/netpro_7k_train.csv\", index=False)\n",
    "print(\"Updated dataset saved as './dataset/netpro_7k_train.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca87cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./dataset/netpro_7k_train.csv: 7245\n",
      "No duplicates found based on the 'Domain' column.\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n",
      "Number of rows in ./dataset/netpro_7k_val.csv: 60\n",
      "No duplicates found based on the 'Domain' column.\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./dataset/netpro_7k_train.csv\")\n",
    "checkDuplicate(\"./dataset/netpro_7k_train.csv\")\n",
    "load_and_print_all_columns(\"./dataset/netpro_7k_train.csv\")\n",
    "countRow(\"./dataset/netpro_7k_val.csv\")\n",
    "checkDuplicate(\"./dataset/netpro_7k_val.csv\")\n",
    "load_and_print_all_columns(\"./dataset/netpro_7k_val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb1416",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3698396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the schema for the classification dictionary (outside the function is cleaner)\n",
    "# classification_schema = {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"answer\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 3},\n",
    "#         \"classification\": {\"type\": \"string\"},\n",
    "#         \"reason\": {\"type\": \"string\"},\n",
    "#         \"confidence\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100}\n",
    "#     },\n",
    "#     \"required\": [\"answer\", \"classification\", \"reason\", \"confidence\"]\n",
    "# }\n",
    "\n",
    "# def to_sharegpt_with_thought(system, input_suffix, dataset) -> Dataset:\n",
    "#     \"\"\"\n",
    "#     Convert website classification dataset to ShareGPT format including reasoning ('thought'),\n",
    "#     with JSON validation and enhanced error handling, specifically checking for unhashable types.\n",
    "#     Returns a Hugging Face Dataset object.\n",
    "    \n",
    "#     Args:\n",
    "#         system (str): System prompt\n",
    "#         input_suffix (str): Suffix to append to the human message (can be empty if not needed)\n",
    "#         dataset (pd.DataFrame): Input DataFrame with columns:\n",
    "#             ['Domain', 'Content', 'Label', 'classification', 'reason', 'confidence', 'thought']\n",
    "            \n",
    "#     Returns:\n",
    "#         datasets.Dataset: Hugging Face Dataset with a 'conversations' column, \n",
    "#                           where each row contains a list representing one conversation.\n",
    "#                           Returns an empty Dataset if input dataset is empty or all rows fail.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(dataset, pd.DataFrame) or dataset.empty:\n",
    "#         print(\"Input is not a valid or non-empty DataFrame. Returning empty Dataset.\")\n",
    "#         # Return an empty Dataset with the expected structure\n",
    "#         return Dataset.from_dict({\"conversations\": []}) \n",
    "        \n",
    "#     # This list will temporarily hold the conversation lists\n",
    "#     conversation_data_list = [] \n",
    "#     error_count = 0\n",
    "#     processed_count = 0\n",
    "\n",
    "#     human_template = f\"{input_suffix}\\nDomain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "#     if not input_suffix:\n",
    "#          human_template = f\"Domain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "\n",
    "#     print(f\"Starting conversion for {len(dataset)} rows...\")\n",
    "\n",
    "#     for idx, row in dataset.iterrows():\n",
    "#         try:\n",
    "#             # --- Data Extraction and Basic Type Check ---\n",
    "#             domain = row.get('Domain', 'N/A') \n",
    "#             content = str(row['Content']) if pd.notna(row['Content']) else \"\" \n",
    "#             thought_text = str(row['thought']).strip() if pd.notna(row['thought']) else \"No thought provided.\"\n",
    "            \n",
    "#             label = row['Label']\n",
    "#             classification = row['classification']\n",
    "#             reason = row['reason']\n",
    "#             confidence = row['confidence']\n",
    "\n",
    "#             if pd.isna(label) or pd.isna(classification) or pd.isna(reason) or pd.isna(confidence):\n",
    "#                  raise ValueError(\"One or more required classification fields are NaN\")\n",
    "\n",
    "#             if isinstance(label, (list, dict)) or \\\n",
    "#                isinstance(classification, (list, dict)) or \\\n",
    "#                isinstance(reason, (list, dict)) or \\\n",
    "#                isinstance(confidence, (list, dict)):\n",
    "#                  raise TypeError(\"One or more classification fields contain unhashable list/dict types\")\n",
    "\n",
    "#             human_value = human_template.format(domain=domain, content=content)\n",
    "            \n",
    "#             # --- Prepare and Validate Classification Dictionary ---\n",
    "#             classification_dict = {\n",
    "#                 \"answer\": int(label), \n",
    "#                 \"classification\": str(classification),\n",
    "#                 \"reason\": str(reason), \n",
    "#                 \"confidence\": int(confidence)\n",
    "#             }\n",
    "#             validate(instance=classification_dict, schema=classification_schema) \n",
    "            \n",
    "#             # --- Serialize and Format Output ---\n",
    "#             final_json_str = json.dumps(classification_dict, ensure_ascii=False, indent=2) \n",
    "#             gpt_value = f\"<think>\\n{thought_text}\\n</think>\\n```json\\n{final_json_str}\\n```\"\n",
    "            \n",
    "#             # This is the list for a single conversation\n",
    "#             conversation = [\n",
    "#                 {\"from\": \"system\", \"value\": system},\n",
    "#                 {\"from\": \"human\", \"value\": human_value},\n",
    "#                 {\"from\": \"gpt\", \"value\": gpt_value} \n",
    "#             ]\n",
    "            \n",
    "#             # Append the conversation list to our temporary list\n",
    "#             conversation_data_list.append(conversation) \n",
    "#             processed_count += 1\n",
    "\n",
    "#         # --- Error Handling ---\n",
    "#         except (ValidationError, ValueError, TypeError) as e: \n",
    "#             error_count += 1\n",
    "#             # Avoid printing excessive errors if many occur\n",
    "#             if error_count < 20 or error_count % 100 == 0: \n",
    "#                  print(f\"Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "#             continue \n",
    "#         except Exception as e: \n",
    "#              error_count += 1\n",
    "#              if error_count < 20 or error_count % 100 == 0:\n",
    "#                  print(f\"UNEXPECTED Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "#              continue\n",
    "\n",
    "#     print(f\"\\nConversion finished.\")\n",
    "#     print(f\"Successfully processed: {processed_count} rows\")\n",
    "#     print(f\"Errors encountered: {error_count} rows\")\n",
    "\n",
    "#     # --- Convert the list of conversation lists to a Hugging Face Dataset ---\n",
    "#     if conversation_data_list:\n",
    "#         # Create the dictionary format expected by from_dict\n",
    "#         hf_dataset_dict = {\"conversations\": conversation_data_list} \n",
    "#         # Create and return the Dataset object\n",
    "#         return Dataset.from_dict(hf_dataset_dict)\n",
    "#     else:\n",
    "#         print(\"No valid data processed. Returning empty Dataset.\")\n",
    "#         # Return an empty Dataset with the expected structure\n",
    "#         return Dataset.from_dict({\"conversations\": []})\n",
    "    \n",
    "\n",
    "# Define the schema for the classification dictionary\n",
    "classification_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"answer\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 3},\n",
    "        \"classification\": {\"type\": \"string\"},\n",
    "        \"reason\": {\"type\": \"string\"},\n",
    "        \"confidence\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 100}\n",
    "    },\n",
    "    \"required\": [\"answer\", \"classification\", \"reason\", \"confidence\"]\n",
    "}\n",
    "\n",
    "def to_sharegpt_with_thought(system, input_suffix, dataset) -> Dataset:\n",
    "    \"\"\"\n",
    "    Convert website classification dataset to ShareGPT format including reasoning ('Thought'),\n",
    "    with JSON validation and enhanced error handling, specifically checking for unhashable types.\n",
    "    Returns a Hugging Face Dataset object.\n",
    "    \n",
    "    Args:\n",
    "        system (str): System prompt\n",
    "        input_suffix (str): Suffix to append to the human message (can be empty if not needed)\n",
    "        dataset (pd.DataFrame): Input DataFrame with columns:\n",
    "            ['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n",
    "            \n",
    "    Returns:\n",
    "        datasets.Dataset: Hugging Face Dataset with a 'conversations' column, \n",
    "                          where each row contains a list representing one conversation\n",
    "    \"\"\"\n",
    "    # Validate input DataFrame\n",
    "    if not isinstance(dataset, pd.DataFrame) or dataset.empty:\n",
    "        print(\"Input is not a valid or non-empty DataFrame. Returning empty Dataset.\")\n",
    "        return Dataset.from_dict({\"conversations\": []}) \n",
    "        \n",
    "    # Validate required columns\n",
    "    required_columns = ['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n",
    "    if not all(col in dataset.columns for col in required_columns):\n",
    "        missing = [col for col in required_columns if col not in dataset.columns]\n",
    "        print(f\"Missing required columns: {missing}. Returning empty Dataset.\")\n",
    "        return Dataset.from_dict({\"conversations\": []})\n",
    "\n",
    "    conversation_data_list = [] \n",
    "    error_count = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Template for human message\n",
    "    human_template = f\"{input_suffix}\\nDomain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "    if not input_suffix:\n",
    "        human_template = f\"Domain: {{domain}}, Content: \\\"{{content}}\\\"\"\n",
    "    \n",
    "    print(f\"Starting conversion for {len(dataset)} rows...\")\n",
    "    \n",
    "    for idx, row in dataset.iterrows():\n",
    "        try:\n",
    "            # Extract data from DataFrame row\n",
    "            domain = str(row['Domain']) if pd.notna(row['Domain']) else \"N/A\"\n",
    "            content = str(row['Content']) if pd.notna(row['Content']) else \"\"\n",
    "            thought = str(row['Thought']) if pd.notna(row['Thought']) else \"No thought provided.\"\n",
    "            \n",
    "            # Validate classification fields\n",
    "            label = row['Label']\n",
    "            classification = row['Classification']\n",
    "            reason = row['Reason']\n",
    "            confidence = row['Confidence']\n",
    "            \n",
    "            if pd.isna(label) or pd.isna(classification) or pd.isna(reason) or pd.isna(confidence):\n",
    "                raise ValueError(\"One or more required classification fields are NaN\")\n",
    "                \n",
    "            if isinstance(label, (list, dict)) or isinstance(classification, (list, dict)) or \\\n",
    "               isinstance(reason, (list, dict)) or isinstance(confidence, (list, dict)):\n",
    "                raise TypeError(\"One or more classification fields contain unhashable list/dict types\")\n",
    "            \n",
    "            # Create human message\n",
    "            human_value = human_template.format(domain=domain, content=content)\n",
    "            \n",
    "            # Prepare and validate classification dictionary\n",
    "            classification_dict = {\n",
    "                \"answer\": int(label),\n",
    "                \"classification\": str(classification),\n",
    "                \"reason\": str(reason),\n",
    "                \"confidence\": int(confidence)\n",
    "            }\n",
    "            validate(instance=classification_dict, schema=classification_schema)\n",
    "            \n",
    "            # Create GPT response with thought and JSON\n",
    "            final_json_str = json.dumps(classification_dict, ensure_ascii=False, indent=2)\n",
    "            gpt_value = f\"<think>\\n{thought}\\n</think>\\n```json\\n{final_json_str}\\n```\"\n",
    "            # Create conversation structure\n",
    "            conversation = [\n",
    "                {\"from\": \"system\", \"value\": system},\n",
    "                {\"from\": \"human\", \"value\": human_value},\n",
    "                {\"from\": \"gpt\", \"value\": gpt_value}\n",
    "            ]\n",
    "            \n",
    "            conversation_data_list.append(conversation)\n",
    "            processed_count += 1\n",
    "            \n",
    "        except (ValidationError, ValueError, TypeError) as e:\n",
    "            error_count += 1\n",
    "            if error_count < 20 or error_count % 100 == 0:\n",
    "                print(f\"Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            if error_count < 20 or error_count % 100 == 0:\n",
    "                print(f\"UNEXPECTED Error processing row {idx} (Domain: {domain}): {type(e).__name__} - {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Print conversion summary\n",
    "    print(f\"\\nConversion finished.\")\n",
    "    print(f\"Successfully processed: {processed_count} rows\")\n",
    "    print(f\"Errors encountered: {error_count} rows\")\n",
    "    \n",
    "    # Return Dataset\n",
    "    if conversation_data_list:\n",
    "        return Dataset.from_dict({\"conversations\": conversation_data_list})\n",
    "    else:\n",
    "        print(\"No valid data processed. Returning empty Dataset.\")\n",
    "        return Dataset.from_dict({\"conversations\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ce0246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversion for 7245 rows...\n",
      "\n",
      "Conversion finished.\n",
      "Successfully processed: 7245 rows\n",
      "Errors encountered: 0 rows\n",
      "Starting conversion for 60 rows...\n",
      "\n",
      "Conversion finished.\n",
      "Successfully processed: 60 rows\n",
      "Errors encountered: 0 rows\n",
      "\n",
      "Output Type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset Info:\n",
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 7245\n",
      "})\n",
      "\n",
      "Output Type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset Info:\n",
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 60\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 7245/7245 [00:00<00:00, 103440.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 60/60 [00:00<00:00, 27881.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "with open('./prompt/labelling_promptv4.txt', 'r', encoding='utf-8') as f:\n",
    "    system_prompt = f.read()\n",
    "dftrain         = pd.read_csv('./dataset/netpro_7k_train.csv')\n",
    "dfvalidation    = pd.read_csv('./dataset/netpro_7k_val.csv')\n",
    "\n",
    "# Convert to ShareGPT format with Unicode preservation\n",
    "train_dataset = to_sharegpt_with_thought(\n",
    "    system=system_prompt,\n",
    "    input_suffix=\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n\",\n",
    "    dataset=dftrain\n",
    ")\n",
    "validation_dataset = to_sharegpt_with_thought(\n",
    "    system=system_prompt,\n",
    "    input_suffix=\"Classify the given URL as 0 (benign), 1 (gambling), 2 (pornography), or 3 (harmful). Output MUST be JSON.\\n\",\n",
    "    dataset=dfvalidation\n",
    ")\n",
    "\n",
    "# Now you can check the type and print the Dataset info\n",
    "print(\"\\nOutput Type:\", type(train_dataset))\n",
    "print(\"Dataset Info:\")\n",
    "print(train_dataset)\n",
    "print(\"\\nOutput Type:\", type(validation_dataset))\n",
    "print(\"Dataset Info:\")\n",
    "print(validation_dataset)\n",
    "# Save the datasets to disk\n",
    "train_dataset.save_to_disk('./dataset/train_dataset')\n",
    "validation_dataset.save_to_disk('./dataset/validation_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bab5966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 7245 conversations to JSONL...\n",
      "Successfully saved data to netpro_7k_sharegpt_thought_train.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving {len(train_dataset)} conversations to JSONL...\")\n",
    "\n",
    "try:\n",
    "    with open('./dataset/netpro_7k_sharegpt_thought_train.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in train_dataset:\n",
    "            json_line = {\"conversations\": item['conversations']}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + '\\n')\n",
    "    print(\"Successfully saved data to netpro_7k_sharegpt_thought_train.jsonl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea62677",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dataset/netpro_7k_sharegpt_thought_train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        try:\n",
    "            json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error in line {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c63fa9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 60 conversations to JSONL...\n",
      "Successfully saved data to netpro_7k_sharegpt_thought_val.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving {len(validation_dataset)} conversations to JSONL...\")\n",
    "\n",
    "try:\n",
    "    with open('./dataset/netpro_7k_sharegpt_thought_val.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in validation_dataset:\n",
    "            json_line = {\"conversations\": item['conversations']}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + '\\n')\n",
    "    print(\"Successfully saved data to netpro_7k_sharegpt_thought_val.jsonl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a43bab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dataset/netpro_7k_sharegpt_thought_val.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        try:\n",
    "            json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error in line {i}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f0450",
   "metadata": {},
   "source": [
    "### Standardize ShareGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82fbc089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 60 examples [00:00, 17145.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from JSONL\n",
    "train_dataset = load_dataset(\"json\", data_files=\"./dataset/netpro_7k_sharegpt_thought_train.jsonl\")\n",
    "validation_dataset = load_dataset(\"json\", data_files=\"./dataset/netpro_7k_sharegpt_thought_val.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7a65817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 60\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbf0ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset[\"train\"] \n",
    "validation_dataset = validation_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b19459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Standardizing formats (num_proc=16): 100%|██████████| 60/60 [00:00<00:00, 444.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "train_dataset = standardize_sharegpt(train_dataset, \"conversations\")\n",
    "validation_dataset = standardize_sharegpt(validation_dataset, \"conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0281ae2",
   "metadata": {},
   "source": [
    "### Save ChatML JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3867391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 7245 standardized conversations to JSONL...\n",
      "Successfully saved standardized data to netpro_7k_chatml_thought_train2.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving {len(train_dataset)} standardized conversations to JSONL...\")\n",
    "# Save standardized dataset in JSONL format\n",
    "try:\n",
    "    with open('./dataset/netpro_7k_chatml_thought_train2.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in train_dataset:\n",
    "            # Wrap each conversation in a dictionary with the key \"conversations\"\n",
    "            json_line = {\"conversations\": item['conversations']}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + '\\n')\n",
    "    print(\"Successfully saved standardized data to netpro_7k_chatml_thought_train2.jsonl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f26e301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 60 standardized conversations to JSONL...\n",
      "Successfully saved standardized data to netpro_7k_chatml_thought_val.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving {len(validation_dataset)} standardized conversations to JSONL...\")\n",
    "# Save standardized dataset in JSONL format\n",
    "try:\n",
    "    with open('./dataset/netpro_7k_chatml_thought_val.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in validation_dataset:\n",
    "            # Wrap each conversation in a dictionary with the key \"conversations\"\n",
    "            json_line = {\"conversations\": item['conversations']}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + '\\n')\n",
    "    print(\"Successfully saved standardized data to netpro_7k_chatml_thought_val.jsonl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving JSONL file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491cce9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e110db04",
   "metadata": {},
   "source": [
    "### Upload To Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93194795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 7245/7245 [00:00<00:00, 82895.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 60/60 [00:00<00:00, 12924.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from huggingface_hub import HfApi\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "# Combine train and validation datasets into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset\n",
    "})\n",
    "dataset_dict.save_to_disk(\"./dataset/chatml_7k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd74039c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 46.78ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 737.40ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully pushed to the Hugging Face Hub with train and validation splits.\n"
     ]
    }
   ],
   "source": [
    "# Push the DatasetDict to the Hugging Face Hub\n",
    "from datasets import load_from_disk\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Loads from .env file automatically\n",
    "\n",
    "if not os.getenv(\"HF_TOKEN\"):\n",
    "    raise ValueError(\"HF_TOKEN not found in .env file\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Reload the dataset to ensure it's in the correct format\n",
    "dataset_dict = load_from_disk(\"./dataset/chatml_thought_7k\")\n",
    "\n",
    "# Push to the Hugging Face Hub\n",
    "dataset_dict.push_to_hub(\"jordinia/netpro-finetune\", config_name=\"chatml_thought_7k\")\n",
    "\n",
    "print(\"Dataset successfully pushed to the Hugging Face Hub with train and validation splits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e24968c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 70.52ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 766.08ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pushed raw-7k config with train and validation splits.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Step 1: Load environment variables and token\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HF_TOKEN not found in .env file\")\n",
    "\n",
    "# Step 2: Load the raw CSVs into datasets\n",
    "train_dataset = Dataset.from_csv(\"./dataset/netpro_7k_train.csv\")\n",
    "val_dataset = Dataset.from_csv(\"./dataset/netpro_7k_val.csv\")\n",
    "\n",
    "# Step 3: Create a DatasetDict with train and validation splits\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "})\n",
    "\n",
    "# Step 4: Push to the Hub with config name \"raw-7k\"\n",
    "dataset_dict.push_to_hub(\"jordinia/netpro-finetune\", config_name=\"raw_7k\", token=hf_token)\n",
    "\n",
    "print(\"Successfully pushed raw-7k config with train and validation splits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25480c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "netpro_chatml_thought.jsonl: 100%|██████████| 710M/710M [01:22<00:00, 8.62MB/s]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jordinia/netpro-finetune/commit/e5a321ee51600a2778b55d5d33d8059d33dad9fb', commit_message='Initial dataset upload', commit_description='', oid='e5a321ee51600a2778b55d5d33d8059d33dad9fb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/jordinia/netpro-finetune', endpoint='https://huggingface.co', repo_type='dataset', repo_id='jordinia/netpro-finetune'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "# 1. Load environment variables\n",
    "load_dotenv()  # Loads from .env file automatically\n",
    "\n",
    "# 2. Verify token loading\n",
    "if not os.getenv(\"HF_TOKEN\"):\n",
    "    raise ValueError(\"HF_TOKEN not found in .env file\")\n",
    "\n",
    "# 3. Initialize and upload\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"./dataset/netpro_chatml_thought.jsonl\",\n",
    "    path_in_repo=\"data/netpro_chatml_thought.jsonl\",\n",
    "    repo_id=\"jordinia/netpro-finetune\",\n",
    "    repo_type=\"dataset\",\n",
    "    commit_message=\"Initial dataset upload\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "776e2553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 33262/33262 [00:01<00:00, 19689.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# dataset = load_dataset(\"json\", data_files=\"./dataset/netpro_chatml_thought.jsonl\")\n",
    "\n",
    "dataset = load_dataset(\"jordinia/netpro-finetune\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "823d045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|██████████| 33262/33262 [00:00<00:00, 75387.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"./dataset/chatml_thought_33k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4163842a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fishmon/.conda/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"./dataset/chatml_thought_33k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b942c610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 33262\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d332c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = dataset.train_test_split(test_size=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6e3d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 33128\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 134\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ef3be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 17/17 [00:00<00:00, 31.86ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 17/17 [00:00<00:00, 34.30ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:30<00:00, 15.36s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 265.26ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pushed raw-7k config with train and validation splits.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Loads from .env file automatically\n",
    "\n",
    "if not os.getenv(\"HF_TOKEN\"):\n",
    "    raise ValueError(\"HF_TOKEN not found in .env file\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# Step 4: Push to the Hub with config name \"raw-7k\"\n",
    "dataset_dict.push_to_hub(\"jordinia/netpro-finetune\", config_name=\"chatml_thought_33k\", token=hf_token)\n",
    "\n",
    "print(\"Successfully pushed raw-7k config with train and validation splits.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
