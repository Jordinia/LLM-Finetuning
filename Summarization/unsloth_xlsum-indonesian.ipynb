{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9918641-17e2-43b4-b553-4570713d7fcf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f9c177-2f0b-4ff8-bbd8-9f8f33783f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordinia/miniconda3/envs/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from datasets import load_dataset, Dataset\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import gc, torch, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24e7146-9ccb-40cd-b060-302a3b8c52e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_templates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_chat_template\n",
      "File \u001b[0;32m~/miniconda3/envs/3.10/lib/python3.10/site-packages/unsloth/__init__.py:93\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# First check if CUDA is available ie a NVIDIA GPU is seen\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Fix Xformers performance issues since 0.0.25\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecea084-1a1d-4b46-b4c8-d82816723188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e7da4-ba85-4b21-82ae-24ce5db00d0c",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de1d57-6d9f-455f-bbbe-6ee9b7b6003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 30000 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "# fourbit_models = [\n",
    "#     \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "#     \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "#     \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "#     \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "#     \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "#     \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "#     \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "#     \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "#     \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "#     \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "#     \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "#     \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "#     \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "#     \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "# ] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3ec40-f8bb-4610-aaff-31fafaf4d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining,\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e7af51-eb16-4ab1-9a97-d7330647dc3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd43ba-0cec-4892-823e-5e081c624e24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e9a332-dc80-4142-82b4-e200413715af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_mem():\n",
    "  for _ in range(10):\n",
    "        gc.collect()\n",
    "        with torch.no_grad():\n",
    "          torch.cuda.empty_cache()\n",
    "        time.sleep(0.1)\n",
    "      \n",
    "def load_jsonl_dataset(file_path: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Load a JSONL file into a Dataset object with 'conversations' feature.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSONL file\n",
    "        \n",
    "    Returns:\n",
    "        Dataset object with 'conversations' feature\n",
    "    \"\"\"\n",
    "    # Read the JSONL file\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Parse each line as JSON and wrap in conversations format\n",
    "            conversations = json.loads(line.strip())\n",
    "            data.append({'conversations': conversations})\n",
    "    \n",
    "    # Convert to Dataset\n",
    "    dataset = Dataset.from_list(data)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1918d2-f334-40db-bcf7-b3969ae74322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlsum_convert_to_sharegpt(example):\n",
    "    \"\"\"\n",
    "    Convert a single XLSum example to ShareGPT format with a detailed prefix.\n",
    "    \"\"\"\n",
    "    human_message = {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": (\n",
    "            \"You are an expert summarization assistant trained to generate concise, clear, and accurate summaries of \"\n",
    "            \"given texts. Your goal is to understand the core ideas of the provided article and create a summary and related keywords.\\n\\n\"\n",
    "            f\"Provide a summary for the following article in its original language: \\n\"\n",
    "            f\"Title: {example['title']} \\n\"\n",
    "            f\"Content: {example['text']} \\n\"\n",
    "            \"Summary:\"\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    gpt_message = {\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": example['summary']\n",
    "    }\n",
    "    \n",
    "    return [human_message, gpt_message]\n",
    "\n",
    "\n",
    "def create_extended_conversation(examples: List[Dict], conversation_extension: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create an extended conversation by combining multiple examples.\n",
    "    \n",
    "    Args:\n",
    "        examples: List of dataset examples to sample from\n",
    "        conversation_extension: Number of examples to combine into one conversation\n",
    "    \n",
    "    Returns:\n",
    "        List of messages forming a single extended conversation\n",
    "    \"\"\"\n",
    "    # Randomly sample the specified number of examples\n",
    "    selected_examples = random.sample(examples, min(conversation_extension, len(examples)))\n",
    "    \n",
    "    # Convert each example and combine their messages\n",
    "    extended_conversation = []\n",
    "    for example in selected_examples:\n",
    "        conversation_pair = convert_to_sharegpt_format(example)\n",
    "        extended_conversation.extend(conversation_pair)\n",
    "    \n",
    "    return extended_conversation\n",
    "\n",
    "def process_dataset(dataset, conversation_extension: int = 1):\n",
    "    \"\"\"\n",
    "    Process the dataset and convert to ShareGPT format with optional conversation extension.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The input dataset\n",
    "        conversation_extension: Number of examples to combine into one conversation\n",
    "    \n",
    "    Returns:\n",
    "        Dataset object with conversations column\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    # Process each split in the dataset\n",
    "    for split in dataset.keys():\n",
    "        print(f\"Processing {split} split...\")\n",
    "        examples = list(dataset[split])\n",
    "        \n",
    "        # Calculate number of conversations needed\n",
    "        num_conversations = len(examples) // conversation_extension\n",
    "        \n",
    "        # Create extended conversations\n",
    "        for i in range(num_conversations):\n",
    "            start_idx = i * conversation_extension\n",
    "            end_idx = start_idx + conversation_extension\n",
    "            conversation_examples = examples[start_idx:end_idx]\n",
    "            \n",
    "            # Create extended conversation\n",
    "            if conversation_extension > 1:\n",
    "                conversation = create_extended_conversation(conversation_examples, conversation_extension)\n",
    "            else:\n",
    "                conversation = xlsum_convert_to_sharegpt(conversation_examples[0])\n",
    "            \n",
    "            processed_data.append({\"conversations\": conversation})\n",
    "    \n",
    "    # Convert to Dataset object\n",
    "    return Dataset.from_list(processed_data)\n",
    "\n",
    "def save_sharegpt_format(dataset, output_path):\n",
    "    \"\"\"\n",
    "    Save the converted data in JSONL format.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset:\n",
    "            f.write(json.dumps(item['conversations'], ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5320040-ad9c-4b98-831a-37b89af73236",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30df25b-05b4-4a2c-9e17-d890443d1213",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Download Data - wikipedia-10k-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5492ef-6061-4800-934e-ed68fe2b896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and subset\n",
    "dataset_name = \"indonesian-nlp/wikipedia-10k\"\n",
    "output_dir = Path(\"dataset/wikipedia-10k\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, \"wikipedia-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea8a9b-a41e-4d3e-ab83-4bd6fdf84f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataset.column_names)\n",
    "print(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef490730-139e-4ac5-8702-cacb6008b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['id', 'url', 'title', 'text']\n",
    "standalone_dataset = dataset['test']\n",
    "standalone_dataset = standalone_dataset.select_columns(columns_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3414038-3374-4e9c-af82-f6dc5b12cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(standalone_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be4660-7ef8-4b7e-b54c-6b3b3af215f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import to_sharegpt\n",
    "dataset = to_sharegpt(\n",
    "    standalone_dataset,\n",
    "    merged_prompt = \"Anda adalah sebuah asisten wikipedia. Jelaskan mengenai topik berikut: {title}\",\n",
    "    output_column_name = \"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c67a64-a191-45d2-b57a-1da8d0981dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286889a-8396-4233-bf8e-a8bb4671e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"{output_dir}/sharegpt_wikipedia-10k.jsonl\"\n",
    "save_sharegpt_format(dataset, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993fd5d3-c515-4966-90e2-5ea91470e7b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Download Data - alpaca-cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693072f-f798-4583-a296-1a12de101f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and subset\n",
    "dataset_name = \"yahma/alpaca-cleaned\"\n",
    "output_dir = Path(\"dataset/alpaca-cleaned\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4949e2f-2e80-46ae-9cda-3ffd6c43bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.column_names)\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f9f08-e09b-433f-bcf7-cd8f1a96bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf786ade-92d7-4668-9b78-3bb5c5219f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import to_sharegpt\n",
    "dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt = \"{instruction}[[\\nYour input is:\\n{input}]]\",\n",
    "    output_column_name = \"output\",\n",
    "    conversation_extension = 3, # Select more to handle longer conversations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea89dc94-b53a-4d13-9b3c-c91bba9ac481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9612ca-a868-4c13-b0d5-5261026fe565",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"{output_dir}/sharegpt_alpaca-cleaned.jsonl\"\n",
    "save_sharegpt_format(dataset, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999e2d1-3b07-4362-b7ef-5afd52c8776a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Download Data - alpaca-gpt4-indonesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4255dfb-a095-447d-a0b1-9138d8ee7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and subset\n",
    "dataset_name = \"FreedomIntelligence/alpaca-gpt4-indonesian\"\n",
    "output_dir = Path(\"dataset/alpaca-gpt4-indonesian\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d5da0f-37cb-4bc4-a555-94217af418ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.column_names)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a7d7f-8428-4c5e-92b5-15db0962a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"{output_dir}/sharegpt_alpaca-gpt4-indonesian.jsonl\"\n",
    "save_sharegpt_format(dataset, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63932fef-aa2b-471f-9f80-ad431bf66ad7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Download Data - finetome100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fe034-5dee-49eb-8511-ca741a4c04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and subset\n",
    "dataset_name = \"mlabonne/FineTome-100k\"\n",
    "output_dir = Path(\"dataset/FineTome-100k\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddcb7f0-819a-4a4e-ac28-60f4ad542ce0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataset.column_names)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4722d7-5f90-4477-b64f-cf7cf3fff08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b92ce-978c-41a2-a4fd-6136919e43f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"{output_dir}/sharegpt_FineTome-100k.jsonl\"\n",
    "save_sharegpt_format(dataset, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4c39c-4f4d-4ca0-973f-0abbc1026ddf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Download Data - xlsum-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6db207-5e97-43ef-b8bd-2ab487aaa934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and subset\n",
    "dataset_name = \"csebuetnlp/xlsum\"\n",
    "language_subset = \"english\"\n",
    "output_dir = Path(\"dataset/xlsum-english\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, language_subset, split = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8741b7-cf04-4f07-90f4-1ca68ab9d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset\n",
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62f156-83c5-42a8-ba51-c776fb1967a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b65bdb-ca77-4d26-9e2f-975f3318513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlsum_en_convert_to_sharegpt(dataset):\n",
    "    \"\"\"\n",
    "    Convert the dataset into ShareGPT format.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (Dataset): The dataset to be converted.\n",
    "\n",
    "    Returns:\n",
    "    Dataset: A new dataset in ShareGPT format.\n",
    "    \"\"\"\n",
    "    sharegpt_data = []\n",
    "\n",
    "    for example in dataset:\n",
    "        # Format the input message\n",
    "        human_message = (\n",
    "            f\"Provide a summary for the following article in its original language: \\nTitle: {example['title']} \\nContent: {example['text']} \\nSummary:\"\n",
    "        )\n",
    "        \n",
    "        # Format the output message\n",
    "        gpt_message = (\n",
    "            f\"{example['summary']}\"\n",
    "        )\n",
    "        \n",
    "        # Create the ShareGPT format\n",
    "        conversation = [\n",
    "            {\"from\": \"human\", \"value\": human_message},\n",
    "            {\"from\": \"gpt\", \"value\": gpt_message}\n",
    "        ]\n",
    "        sharegpt_data.append(conversation)\n",
    "    \n",
    "    # Convert to Dataset format\n",
    "    return sharegpt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b210683-410b-4378-bd0b-86f579ffd246",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = xlsum_en_convert_to_sharegpt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14bf86f-d39c-4b7e-b841-682d54d1ff97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\"conversations\": dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95876ff4-0633-4169-85c5-1cd936bb5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ea886-5319-4622-adda-ce6da95c88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"{output_dir}/sharegpt_xlsum_english.jsonl\"\n",
    "save_sharegpt_format(dataset, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b9f739-cdf1-41d2-86d5-bd817258221b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Download Data - xlsum-indonesian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3eb97-8b34-4c95-bb83-b98af4c9efd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54d58d-d560-42b3-9eac-b1fc5cd5f6c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the dataset and subset\n",
    "dataset_name = \"csebuetnlp/xlsum\"\n",
    "language_subset = \"indonesian\"\n",
    "output_dir = Path(\"dataset/xlsum-indonesian\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, language_subset, split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed70743-af5e-4678-abd9-a62e7fd9a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ba3b5-9b96-4b65-b694-01113affa579",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7345d-7bc1-437f-aa7d-01a741256526",
   "metadata": {},
   "source": [
    "Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d6030-3c0c-424f-8ece-f16cde185fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"test\", \"validation\"]:\n",
    "    df = pd.DataFrame(dataset[split])  # Convert to Pandas DataFrame\n",
    "    output_file = output_dir / f\"xlsum_indonesian_{split}.csv\"\n",
    "    df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved {split} split to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e268b3-2e0f-4c44-bcc7-850dda52e9d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb634c4b-feb1-4eb4-b457-e08a47652692",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"dataset/xlsum-indonesian/xlsum_indonesian_train.csv\"\n",
    "test_path = \"dataset/xlsum-indonesian/xlsum_indonesian_test.csv\"\n",
    "validation_path = \"dataset/xlsum-indonesian/xlsum_indonesian_validation.csv\"\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={\n",
    "    \"train\": train_path,\n",
    "    \"test\": test_path,\n",
    "    \"validation\": validation_path\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b255f57-0fb2-41e4-b62d-871f5c57cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f5a30-eee3-4aef-9207-a644aa2ae92e",
   "metadata": {},
   "source": [
    "##### Convert to sharegpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe511b63-779f-4926-a67d-2c202c473d9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Unsloth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea2c4a-9df4-4162-8148-c47bf23cbd33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##this code doesnt work, out of index bug\n",
    "\n",
    "from unsloth import to_sharegpt\n",
    "train_dataset = to_sharegpt(\n",
    "    dataset['validation'],\n",
    "    merged_prompt= \\\n",
    "        \"[[Provide a summary for the following article in its original language:]]\"\\\n",
    "        \"[[\\nTitle: {title}]]\"\\\n",
    "        \"[[\\nContent: {text}]]\"\\\n",
    "        \"[[\\nSummarize below:]]\",\n",
    "    conversation_extension=2,  # Randomly combines conversations \n",
    "    output_column_name=\"summary\",  # Use the \"summary\" column as the target\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e69ee8-fda0-475c-8692-18c40ef81fcc",
   "metadata": {},
   "source": [
    "###### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc7751-7b9d-4407-8e72-ab4ab137581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set conversation extension (e.g., 3 for combining 3 examples into one conversation)\n",
    "conversation_extension = 1\n",
    "\n",
    "# Convert to ShareGPT format with conversation extension\n",
    "dataset = process_dataset(dataset, conversation_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1bd4d-3ce7-4ed5-a785-57a1ca0357f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a sample conversation\n",
    "print(\"\\nSample conversation:\")\n",
    "print(json.dumps(dataset[0]['conversations'], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7fe7f-875f-4d0d-9851-8cd4344ff142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset info\n",
    "print(\"\\nDataset info:\")\n",
    "print(f\"Number of conversations: {len(dataset)}\")\n",
    "print(f\"Features: {dataset.features}\")\n",
    "print(f\"Datatype: {type(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3679c-fea0-47b2-8f46-cdffd1916571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the converted data\n",
    "output_path = \"dataset/xlsum-indonesian/sharegpt_xlsum_indonesian.jsonl\"\n",
    "save_sharegpt_format(dataset, output_path)\n",
    "print(f\"Converted data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ade67-50bb-4e65-81e9-79739f9dfa06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Download Data - scientific_lay_summarisation-plos-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa8885-4ed1-4c5d-bf80-0f56e77bcb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and subset\n",
    "dataset_name = \"pszemraj/scientific_lay_summarisation-plos-norm\"\n",
    "output_dir = Path(\"dataset/scientific_lay_summarisation-plos-norm\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1608da-2fb8-4208-b8d1-fcee226d3bae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6be55-b836-4133-8b79-e324aedd60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scientific_convert_to_sharegpt(dataset):\n",
    "    \"\"\"\n",
    "    Convert the dataset into ShareGPT format.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (Dataset): The dataset to be converted.\n",
    "\n",
    "    Returns:\n",
    "    Dataset: A new dataset in ShareGPT format.\n",
    "    \"\"\"\n",
    "    sharegpt_data = []\n",
    "\n",
    "    for example in dataset:\n",
    "        # Format the input message\n",
    "        human_message = (\n",
    "            \"You are an expert summarization assistant trained to generate concise, clear, and accurate summaries of \"\n",
    "            \"given texts. Your goal is to understand the core ideas of the provided article and create a summary and related keywords.\\n\\n\"\n",
    "            f\"Title: {example['title']}\\n\\n\"\n",
    "            f\"Content: {example['article']}\\n\\n\"\n",
    "            \"Summary:\\n\"\n",
    "        )\n",
    "        \n",
    "        # Format the output message\n",
    "        gpt_message = (\n",
    "            f\"{example['summary']}\\n\\n\"\n",
    "            f\"keywords: {example['keywords']}\"\n",
    "        )\n",
    "        \n",
    "        # Create the ShareGPT format\n",
    "        conversation = [\n",
    "            {\"from\": \"human\", \"value\": human_message},\n",
    "            {\"from\": \"gpt\", \"value\": gpt_message}\n",
    "        ]\n",
    "        sharegpt_data.append(conversation)\n",
    "    \n",
    "    # Convert to Dataset format\n",
    "    return sharegpt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fecafd-e7ac-41c0-b754-bf01432aa675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sharegpt_dataset = scientific_convert_to_sharegpt(dataset)\n",
    "print(sharegpt_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f771cd6-1405-413a-a06c-02a124734a26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\"conversations\": sharegpt_dataset})\n",
    "\n",
    "# Check the first example\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3fd84-24d9-4c86-a204-7d1449cfa2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee436fe2-3ef5-4e93-91da-b36ebff3d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"{output_dir}/sharegpt_scientific_lay_summarisation-plos-norm.jsonl\"\n",
    "save_sharegpt_format(dataset, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6794c636-6970-4d94-9d92-4b38a6e4aea5",
   "metadata": {},
   "source": [
    "#### Download Data - govreport-summarization-8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36558ea0-5cc0-4e24-9eb3-84ea3e1ed734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and subset\n",
    "dataset_name = \"pszemraj/govreport-summarization-8192\"\n",
    "output_dir = Path(\"dataset/govreport-summarization-8192\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_name, split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de0264f-5e2d-41c8-a31c-f14cfaa22f02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataset.column_names)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c189ce-864b-4e61-9cf9-090bb6e30c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import to_sharegpt\n",
    "dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt = \"You are an expert summarization assistant trained to generate concise, clear, and accurate summaries of given texts. Your goal is to understand the core ideas of the provided article and create a summary. [[\\nHere is the article:\\n{report}]]\",\n",
    "    output_column_name = \"summary\",\n",
    "    # conversation_extension = 3, # Select more to handle longer conversations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66f115-4e2f-4178-b4bc-82e83e4ae1f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataset.column_names)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b295fce-dc7c-43e9-9679-4e277a80df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"{output_dir}/sharegpt_govreport-summarization-8192.jsonl\"\n",
    "save_sharegpt_format(dataset, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79bfce-65bf-4af1-a3b2-d2fa11401de9",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a60d3b-77c9-471b-9560-f1e6c61b1638",
   "metadata": {},
   "source": [
    "### Combine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4542aa-2a3a-4f0b-9091-516f703d77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects a given number of rows from a dataset\n",
    "# Will be off due to rounding errors***\n",
    "def subset(dataset : Dataset, count : int) -> Dataset:\n",
    "  divisor = int(len(dataset) / count)\n",
    "  new_dataset = dataset[::divisor]\n",
    "  while len(new_dataset['conversations']) > count:\n",
    "    new_dataset['conversations'].pop()\n",
    "  return Dataset.from_dict(new_dataset)\n",
    "\n",
    "# Way slower but more precise\n",
    "def subset_slow_exact(dataset : Dataset, count : int) -> Dataset:\n",
    "  divisor = len(dataset) / count\n",
    "  new_dataset = {'conversations':[]}\n",
    "  i = 0\n",
    "  while i < len(dataset) - 1:\n",
    "    new_dataset['conversations'].append(dataset['conversations'][int(i)])\n",
    "    i += divisor\n",
    "  return Dataset.from_dict(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d732600c-83c7-4c54-9733-2ad3b84b07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd7501-1c21-466e-b515-3ac51bb922af",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_dataset = data_path / 'sharegpt_finetune.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e071a6c-1f8d-4753-8997-136941f7fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_dataset = load_jsonl_dataset(str(finetune_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6f261-cae6-4711-a9f6-e9af5e94a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpaca_cleaned = data_path / 'alpaca-cleaned/sharegpt_alpaca-cleaned.jsonl'\n",
    "# alpaca_id = data_path / 'alpaca-gpt4-indonesian/sharegpt_alpaca-gpt4-indonesian.jsonl'\n",
    "# finetome_100k = data_path / 'FineTome-100k/sharegpt_FineTome-100k.jsonl'\n",
    "# wikipedia_10k = data_path / 'wikipedia-10k/sharegpt_wikipedia-10k.jsonl'\n",
    "xlsum_en = data_path / 'xlsum-english/sharegpt_xlsum_english.jsonl'\n",
    "xlsum_id = data_path / 'xlsum-indonesian/sharegpt_xlsum_indonesian.jsonl'\n",
    "govreport = data_path / 'govreport-summarization-8192/sharegpt_govreport-summarization-8192.jsonl'\n",
    "scientific_sum = data_path / 'scientific_lay_summarisation-plos-norm/sharegpt_scientific_3k.jsonl'\n",
    "        \n",
    "# Load the dataset for each path\n",
    "# alpaca_cleaned = load_jsonl_dataset(str(alpaca_cleaned))\n",
    "# alpaca_id = load_jsonl_dataset(str(alpaca_id))\n",
    "# finetome_100k = load_jsonl_dataset(str(finetome_100k))\n",
    "# wikipedia_10k = load_jsonl_dataset(str(wikipedia_10k))\n",
    "xlsum_en = load_jsonl_dataset(str(xlsum_en))\n",
    "xlsum_id = load_jsonl_dataset(str(xlsum_id))\n",
    "govreport = load_jsonl_dataset(str(govreport))\n",
    "scientific_sum = load_jsonl_dataset(str(scientific_sum))\n",
    "\n",
    "# Print the number of rows in each dataset as a quick verification\n",
    "# print(f\"Alpaca Cleaned: {len(alpaca_cleaned)} examples\")\n",
    "# print(f\"Alpaca Indonesian: {len(alpaca_id)} examples\")\n",
    "# print(f\"FineTome 100k: {len(finetome_100k)} examples\")\n",
    "# print(f\"Wikipedia 10k: {len(wikipedia_10k)} examples\")\n",
    "print(f\"XLSum English: {len(xlsum_en)} examples\")\n",
    "print(f\"XLSum Indonesian: {len(xlsum_id)} examples\")\n",
    "print(f\"GovReport: {len(govreport)} examples\")\n",
    "print(f\"Scientific Summary: {len(scientific_sum)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da569df4-7fb3-4411-aee3-609ec97fc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Combine datasets together\n",
    "# pretrain_dataset = concatenate_datasets([alpaca_cleaned, alpaca_id, finetome_100k, wikipedia_10k])\n",
    "finetune_dataset = concatenate_datasets([xlsum_id, xlsum_en, govreport, scientific_sum])\n",
    "# finetune_dataset = concatenate_datasets([xlsum_id, govreport, scientific_sum])\n",
    "\n",
    "# Shuffle dataset (optional)\n",
    "# pretrain_dataset = pretrain_dataset.shuffle(seed=0)\n",
    "# finetune_dataset = finetune_dataset.shuffle(seed=0)\n",
    "# scientific_sum = scientific_sum.shuffle(seed=0)\n",
    "# scientific_sum = subset_slow_exact(scientific_sum, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e0f82-17f7-4d45-ab2f-87f768d67fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a13ff-8331-448e-8a54-08ac0d9d33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40269b74-71f8-4071-8adc-f7fb6ad351d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens_in_conversation(conversation, tokenizer):\n",
    "    \"\"\"\n",
    "    Count tokens in a single conversation.\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Concatenate all messages in the conversation\n",
    "    full_text = \"\"\n",
    "    for message in conversation:\n",
    "        # Add message format\n",
    "        full_text += f\"{message['from']}: {message['value']}\\n\"\n",
    "    \n",
    "    # Count tokens\n",
    "    tokens = tokenizer(full_text, return_tensors=\"pt\", truncation=False)\n",
    "    return len(tokens.input_ids[0])\n",
    "\n",
    "def analyze_dataset_tokens(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Analyze token counts across the entire dataset.\n",
    "    \"\"\"\n",
    "    token_counts = []\n",
    "    \n",
    "    # Process each conversation\n",
    "    for idx, item in enumerate(dataset):\n",
    "        if idx % 1000 == 0:  # Progress indicator\n",
    "            print(f\"Processing conversation {idx}/{len(dataset)}\")\n",
    "            \n",
    "        tokens = count_tokens_in_conversation(item['conversations'], tokenizer)\n",
    "        token_counts.append(tokens)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_tokens = sum(token_counts)\n",
    "    avg_tokens = total_tokens / len(token_counts)\n",
    "    max_tokens = max(token_counts)\n",
    "    min_tokens = min(token_counts)\n",
    "    \n",
    "    # Find conversations exceeding token limit\n",
    "    over_limit = sum(1 for count in token_counts if count > max_seq_length)\n",
    "    \n",
    "    return {\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"average_tokens\": avg_tokens,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"min_tokens\": min_tokens,\n",
    "        \"total_conversations\": len(token_counts),\n",
    "        \"conversations_over_limit\": over_limit,\n",
    "        \"percent_over_limit\": (over_limit / len(token_counts)) * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6506d08a-7bfb-4a46-b948-ebe022cc40b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Use the functions\n",
    "# stats = analyze_dataset_tokens(pretrain_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54642d-ec3e-4686-ab0f-be8865b8e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print results\n",
    "# print(\"\\nPretrain Dataset Token Statistics:\")\n",
    "# print(f\"Total Tokens: {stats['total_tokens']:,}\")\n",
    "# print(f\"Average Tokens per Conversation: {stats['average_tokens']:.2f}\")\n",
    "# print(f\"Max Tokens in a Conversation: {stats['max_tokens']:,}\")\n",
    "# print(f\"Min Tokens in a Conversation: {stats['min_tokens']:,}\")\n",
    "# print(f\"Total Conversations: {stats['total_conversations']:,}\")\n",
    "# print(f\"Conversations Over {max_seq_length:,} tokens: {stats['conversations_over_limit']:,} ({stats['percent_over_limit']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10518e63-3e6c-4ca4-b10e-37d1bc6b5f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = analyze_dataset_tokens(scientific_sum, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ebdbf-1147-4018-b110-adca8f1bdfef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"\\nFinetune Dataset Token Statistics:\")\n",
    "print(f\"Total Tokens: {stats['total_tokens']:,}\")\n",
    "print(f\"Average Tokens per Conversation: {stats['average_tokens']:.2f}\")\n",
    "print(f\"Max Tokens in a Conversation: {stats['max_tokens']:,}\")\n",
    "print(f\"Min Tokens in a Conversation: {stats['min_tokens']:,}\")\n",
    "print(f\"Total Conversations: {stats['total_conversations']:,}\")\n",
    "print(f\"Conversations Over {max_seq_length:,} tokens: {stats['conversations_over_limit']:,} ({stats['percent_over_limit']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296a8fd-b4fe-437c-992e-e218e47683fe",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f33bab-7b8e-4bd8-b34b-1bd95e3c957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import standardize_sharegpt\n",
    "# pretrain_dataset = standardize_sharegpt(pretrain_dataset)\n",
    "finetune_dataset = standardize_sharegpt(finetune_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e47c0-a879-4aa7-9aa6-c711b4a8c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free the variables\n",
    "del xlsum_id\n",
    "del govreport\n",
    "del scientific_sum\n",
    "\n",
    "# Optionally, force garbage collection\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b149143-e4f8-4fce-a99e-9bdca877db26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"\\nSample conversation (pretrain):\")\n",
    "# print(json.dumps(pretrain_dataset[1]['conversations'], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107d9a0-9323-4c61-8a36-5fc2b9e74c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nSample conversation (finetune):\")\n",
    "print(json.dumps(finetune_dataset[1]['conversations'], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119e7fb-b189-48a8-bd1c-7b5c97c254fc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7927b-c23f-46f9-9aa8-6d8b1bf2ca43",
   "metadata": {},
   "source": [
    "### Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5539b-74b6-4e38-a6fe-aef70c1bceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "# def formatting_prompts_func(examples):\n",
    "#     convos = examples[\"conversations\"]\n",
    "#     texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "#     return { \"text\" : texts, }\n",
    "# pass\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    \n",
    "    for convo in convos:\n",
    "        try:\n",
    "            # Skip invalid conversations\n",
    "            if not convo or not isinstance(convo, list):\n",
    "                texts.append(\"\")\n",
    "                continue\n",
    "                \n",
    "            # Validate and clean conversation messages\n",
    "            valid_convo = [\n",
    "                {\n",
    "                    \"role\": msg[\"role\"],\n",
    "                    \"content\": msg[\"content\"] or \"\"  # Convert None to empty string\n",
    "                }\n",
    "                for msg in convo\n",
    "                if isinstance(msg, dict) \n",
    "                and msg.get(\"role\") \n",
    "                and msg.get(\"content\") is not None\n",
    "            ]\n",
    "            \n",
    "            if not valid_convo:\n",
    "                texts.append(\"\")\n",
    "                continue\n",
    "                \n",
    "            text = tokenizer.apply_chat_template(\n",
    "                valid_convo,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing conversation: {e}\")\n",
    "            texts.append(\"\")\n",
    "            \n",
    "    return {\"text\": texts}\n",
    "\n",
    "def clean_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Clean the dataset by removing conversations with None values\n",
    "    and validating conversation structure\n",
    "    \"\"\"\n",
    "    def is_valid_conversation(conv):\n",
    "        if not conv or not isinstance(conv, list):\n",
    "            return False\n",
    "        return all(\n",
    "            isinstance(msg, dict) \n",
    "            and isinstance(msg.get('role'), str) \n",
    "            and isinstance(msg.get('content'), str)\n",
    "            for msg in conv\n",
    "        )\n",
    "    \n",
    "    # Filter valid conversations\n",
    "    filtered_dataset = dataset.filter(\n",
    "        lambda x: is_valid_conversation(x['conversations']),\n",
    "        num_proc=4  # Adjust based on your CPU cores\n",
    "    )\n",
    "    \n",
    "    print(f\"Original dataset size: {len(dataset)}\")\n",
    "    print(f\"Cleaned dataset size: {len(filtered_dataset)}\")\n",
    "    \n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2553460-fce4-46cb-99a6-7731a65c8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain_dataset = pretrain_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eafce0-dee2-4e13-9485-22d2c4d49de0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finetune_dataset = finetune_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd4678-01d0-437b-85f0-072d54fc38bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pretrain_dataset[5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f033064a-5e70-413d-8182-4e0330c565be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pretrain_dataset[5][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db9a4a-46e8-4603-a05c-8bdcb9d40d4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finetune_dataset[37269][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac369573-32c3-4d72-a671-549e0aa2f8dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finetune_dataset[37269][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d33e0-c6fa-4447-ad29-39c0b31b1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataset before training\n",
    "finetune_dataset = clean_dataset(finetune_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c1863-b632-475d-adf2-ab86ad7f1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"dataset/sharegpt_finetune.jsonl\"\n",
    "save_sharegpt_format(finetune_dataset, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fbf9db-fe9a-460a-b2b0-bd67db386c33",
   "metadata": {},
   "source": [
    "### Continued Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75bf569-76e0-4262-bbd5-6cd905c217ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = finetune_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        # warmup_steps = 10,\n",
    "        warmup_ratio = 0.01,\n",
    "        num_train_epochs = 2, # Set this for 1 full training run.\n",
    "        # max_steps =None,\n",
    "        # learning_rate = 2e-4,\n",
    "        learning_rate = 2e-4,\n",
    "        # embedding_learning_rate = 1e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 100,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c6dc2-ca94-4e46-b1ad-032a4e1f23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trl import SFTTrainer\n",
    "# from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "# from unsloth import is_bfloat16_supported\n",
    "# from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "# trainer = UnslothTrainer(\n",
    "#     model = model,\n",
    "#     tokenizer = tokenizer,\n",
    "#     # train_dataset = pretrain_dataset,\n",
    "#     train_dataset = finetune_dataset,\n",
    "#     dataset_text_field = \"text\",\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "#     dataset_num_proc = 2,\n",
    "#     packing = False, # Can make training 5x faster for short sequences.\n",
    "#     args = UnslothTrainingArguments(\n",
    "#         per_device_train_batch_size = 2,\n",
    "#         gradient_accumulation_steps = 8,\n",
    "#         # warmup_steps = 10,\n",
    "#         warmup_ratio = 0.05,\n",
    "#         num_train_epochs = 2, # Set this for 1 full training run.\n",
    "#         # max_steps =None,\n",
    "#         # learning_rate = 2e-4,\n",
    "#         learning_rate = 5e-5,\n",
    "#         embedding_learning_rate = 1e-5,\n",
    "#         fp16 = not is_bfloat16_supported(),\n",
    "#         bf16 = is_bfloat16_supported(),\n",
    "#         logging_steps = 1,\n",
    "#         optim = \"adamw_8bit\",\n",
    "#         weight_decay = 0.01,\n",
    "#         lr_scheduler_type = \"linear\",\n",
    "#         seed = 3407,\n",
    "#         output_dir = \"outputs\",\n",
    "#         save_strategy = \"steps\",\n",
    "#         save_steps = 100,\n",
    "#         report_to = \"none\", # Use this for WandB etc\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bccf9fb-fd59-4660-a1ff-793c7dec4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1eda55-5c4a-4a18-9ed7-d8f5a97aa019",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f9a9f-3703-4a26-bc4a-ac3bb9ab7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2fec6-1184-4922-9945-2258b8ee0beb",
   "metadata": {},
   "source": [
    "#### Show memory stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb1c53-5b4a-4c4c-87a6-09dad9f52d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "free_mem()\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa91cae",
   "metadata": {},
   "outputs": [],
   "source": [
    " model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512ca5e-718c-444a-a09e-1741e8e6bd1d",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f8212-8b6a-4c71-ba5a-745b06132ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to train from scratch\n",
    "# trainer_stats = trainer.train()\n",
    "\n",
    "# use this to train from checkpoint\n",
    "trainer_stats = trainer.train(resume_from_checkpoint = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd16d52-43b3-4d53-9623-f4400e64e629",
   "metadata": {},
   "source": [
    "#### Final Memory anf Time Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23fa717-8925-49fb-97f3-d7475db1de86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
